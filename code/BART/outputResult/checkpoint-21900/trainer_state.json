{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 21900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00045662100456621003,
      "grad_norm": Infinity,
      "learning_rate": 2.999178082191781e-05,
      "loss": 13.7014,
      "step": 10
    },
    {
      "epoch": 0.0009132420091324201,
      "grad_norm": 41.092926025390625,
      "learning_rate": 2.997945205479452e-05,
      "loss": 10.3406,
      "step": 20
    },
    {
      "epoch": 0.0013698630136986301,
      "grad_norm": 52.132843017578125,
      "learning_rate": 2.9965753424657534e-05,
      "loss": 6.9153,
      "step": 30
    },
    {
      "epoch": 0.0018264840182648401,
      "grad_norm": 51.59584426879883,
      "learning_rate": 2.9952054794520552e-05,
      "loss": 4.9997,
      "step": 40
    },
    {
      "epoch": 0.00228310502283105,
      "grad_norm": 51.040672302246094,
      "learning_rate": 2.9938356164383563e-05,
      "loss": 4.0292,
      "step": 50
    },
    {
      "epoch": 0.0027397260273972603,
      "grad_norm": 49.18275833129883,
      "learning_rate": 2.9924657534246578e-05,
      "loss": 3.2548,
      "step": 60
    },
    {
      "epoch": 0.0031963470319634705,
      "grad_norm": 47.42967224121094,
      "learning_rate": 2.991095890410959e-05,
      "loss": 2.5543,
      "step": 70
    },
    {
      "epoch": 0.0036529680365296802,
      "grad_norm": 41.771324157714844,
      "learning_rate": 2.9897260273972603e-05,
      "loss": 1.901,
      "step": 80
    },
    {
      "epoch": 0.00410958904109589,
      "grad_norm": 35.2108154296875,
      "learning_rate": 2.9883561643835618e-05,
      "loss": 1.3432,
      "step": 90
    },
    {
      "epoch": 0.0045662100456621,
      "grad_norm": 24.798107147216797,
      "learning_rate": 2.986986301369863e-05,
      "loss": 0.8687,
      "step": 100
    },
    {
      "epoch": 0.00502283105022831,
      "grad_norm": 15.441203117370605,
      "learning_rate": 2.9856164383561647e-05,
      "loss": 0.5619,
      "step": 110
    },
    {
      "epoch": 0.005479452054794521,
      "grad_norm": 8.943325996398926,
      "learning_rate": 2.9842465753424658e-05,
      "loss": 0.3255,
      "step": 120
    },
    {
      "epoch": 0.005936073059360731,
      "grad_norm": 4.9674482345581055,
      "learning_rate": 2.9828767123287673e-05,
      "loss": 0.1826,
      "step": 130
    },
    {
      "epoch": 0.006392694063926941,
      "grad_norm": 3.1611714363098145,
      "learning_rate": 2.9815068493150684e-05,
      "loss": 0.1448,
      "step": 140
    },
    {
      "epoch": 0.00684931506849315,
      "grad_norm": 1.6789770126342773,
      "learning_rate": 2.98013698630137e-05,
      "loss": 0.1141,
      "step": 150
    },
    {
      "epoch": 0.0073059360730593605,
      "grad_norm": 1.4241091012954712,
      "learning_rate": 2.9787671232876713e-05,
      "loss": 0.1105,
      "step": 160
    },
    {
      "epoch": 0.007762557077625571,
      "grad_norm": 1.4745845794677734,
      "learning_rate": 2.9773972602739724e-05,
      "loss": 0.0853,
      "step": 170
    },
    {
      "epoch": 0.00821917808219178,
      "grad_norm": 1.239727258682251,
      "learning_rate": 2.9760273972602742e-05,
      "loss": 0.0735,
      "step": 180
    },
    {
      "epoch": 0.008675799086757991,
      "grad_norm": 1.2842113971710205,
      "learning_rate": 2.9746575342465753e-05,
      "loss": 0.0651,
      "step": 190
    },
    {
      "epoch": 0.0091324200913242,
      "grad_norm": 1.1059021949768066,
      "learning_rate": 2.9732876712328768e-05,
      "loss": 0.0647,
      "step": 200
    },
    {
      "epoch": 0.009589041095890411,
      "grad_norm": 1.3825576305389404,
      "learning_rate": 2.9719178082191783e-05,
      "loss": 0.0648,
      "step": 210
    },
    {
      "epoch": 0.01004566210045662,
      "grad_norm": 0.6627464890480042,
      "learning_rate": 2.9705479452054794e-05,
      "loss": 0.0598,
      "step": 220
    },
    {
      "epoch": 0.010502283105022832,
      "grad_norm": 1.32079017162323,
      "learning_rate": 2.969178082191781e-05,
      "loss": 0.0773,
      "step": 230
    },
    {
      "epoch": 0.010958904109589041,
      "grad_norm": 1.493644118309021,
      "learning_rate": 2.9678082191780823e-05,
      "loss": 0.0598,
      "step": 240
    },
    {
      "epoch": 0.01141552511415525,
      "grad_norm": 0.9875349402427673,
      "learning_rate": 2.9664383561643837e-05,
      "loss": 0.0604,
      "step": 250
    },
    {
      "epoch": 0.011872146118721462,
      "grad_norm": 1.4185668230056763,
      "learning_rate": 2.9650684931506852e-05,
      "loss": 0.0596,
      "step": 260
    },
    {
      "epoch": 0.012328767123287671,
      "grad_norm": 2.1021506786346436,
      "learning_rate": 2.9636986301369863e-05,
      "loss": 0.0594,
      "step": 270
    },
    {
      "epoch": 0.012785388127853882,
      "grad_norm": 1.766182541847229,
      "learning_rate": 2.9623287671232878e-05,
      "loss": 0.0505,
      "step": 280
    },
    {
      "epoch": 0.013242009132420091,
      "grad_norm": 0.8123580813407898,
      "learning_rate": 2.960958904109589e-05,
      "loss": 0.0506,
      "step": 290
    },
    {
      "epoch": 0.0136986301369863,
      "grad_norm": 2.490922212600708,
      "learning_rate": 2.9595890410958904e-05,
      "loss": 0.0543,
      "step": 300
    },
    {
      "epoch": 0.014155251141552512,
      "grad_norm": 1.7576977014541626,
      "learning_rate": 2.958219178082192e-05,
      "loss": 0.0526,
      "step": 310
    },
    {
      "epoch": 0.014611872146118721,
      "grad_norm": 1.6325037479400635,
      "learning_rate": 2.9568493150684933e-05,
      "loss": 0.0512,
      "step": 320
    },
    {
      "epoch": 0.015068493150684932,
      "grad_norm": 1.6842668056488037,
      "learning_rate": 2.9554794520547947e-05,
      "loss": 0.05,
      "step": 330
    },
    {
      "epoch": 0.015525114155251141,
      "grad_norm": 0.6946329474449158,
      "learning_rate": 2.954109589041096e-05,
      "loss": 0.0463,
      "step": 340
    },
    {
      "epoch": 0.01598173515981735,
      "grad_norm": 1.2819886207580566,
      "learning_rate": 2.9527397260273973e-05,
      "loss": 0.0498,
      "step": 350
    },
    {
      "epoch": 0.01643835616438356,
      "grad_norm": 1.1564936637878418,
      "learning_rate": 2.9513698630136988e-05,
      "loss": 0.0425,
      "step": 360
    },
    {
      "epoch": 0.016894977168949773,
      "grad_norm": 2.932863235473633,
      "learning_rate": 2.95e-05,
      "loss": 0.0482,
      "step": 370
    },
    {
      "epoch": 0.017351598173515982,
      "grad_norm": 1.150285005569458,
      "learning_rate": 2.9486301369863017e-05,
      "loss": 0.052,
      "step": 380
    },
    {
      "epoch": 0.01780821917808219,
      "grad_norm": 0.5775150060653687,
      "learning_rate": 2.9472602739726028e-05,
      "loss": 0.0491,
      "step": 390
    },
    {
      "epoch": 0.0182648401826484,
      "grad_norm": 0.8579307794570923,
      "learning_rate": 2.9458904109589042e-05,
      "loss": 0.0534,
      "step": 400
    },
    {
      "epoch": 0.01872146118721461,
      "grad_norm": 2.8332784175872803,
      "learning_rate": 2.9445205479452054e-05,
      "loss": 0.0461,
      "step": 410
    },
    {
      "epoch": 0.019178082191780823,
      "grad_norm": 1.313560128211975,
      "learning_rate": 2.9431506849315068e-05,
      "loss": 0.0459,
      "step": 420
    },
    {
      "epoch": 0.019634703196347032,
      "grad_norm": 1.208168625831604,
      "learning_rate": 2.9417808219178083e-05,
      "loss": 0.0473,
      "step": 430
    },
    {
      "epoch": 0.02009132420091324,
      "grad_norm": 1.7538193464279175,
      "learning_rate": 2.9404109589041097e-05,
      "loss": 0.0422,
      "step": 440
    },
    {
      "epoch": 0.02054794520547945,
      "grad_norm": 2.475050449371338,
      "learning_rate": 2.9390410958904112e-05,
      "loss": 0.0582,
      "step": 450
    },
    {
      "epoch": 0.021004566210045664,
      "grad_norm": 3.1475491523742676,
      "learning_rate": 2.9376712328767123e-05,
      "loss": 0.0523,
      "step": 460
    },
    {
      "epoch": 0.021461187214611873,
      "grad_norm": 1.3038179874420166,
      "learning_rate": 2.9363013698630138e-05,
      "loss": 0.0513,
      "step": 470
    },
    {
      "epoch": 0.021917808219178082,
      "grad_norm": 1.1751430034637451,
      "learning_rate": 2.9349315068493152e-05,
      "loss": 0.0433,
      "step": 480
    },
    {
      "epoch": 0.02237442922374429,
      "grad_norm": 1.6390329599380493,
      "learning_rate": 2.9335616438356163e-05,
      "loss": 0.0476,
      "step": 490
    },
    {
      "epoch": 0.0228310502283105,
      "grad_norm": 0.6694724559783936,
      "learning_rate": 2.9321917808219178e-05,
      "loss": 0.043,
      "step": 500
    },
    {
      "epoch": 0.023287671232876714,
      "grad_norm": 1.2831276655197144,
      "learning_rate": 2.9308219178082193e-05,
      "loss": 0.0387,
      "step": 510
    },
    {
      "epoch": 0.023744292237442923,
      "grad_norm": 2.0965654850006104,
      "learning_rate": 2.9294520547945207e-05,
      "loss": 0.0405,
      "step": 520
    },
    {
      "epoch": 0.024200913242009132,
      "grad_norm": 1.5182349681854248,
      "learning_rate": 2.9280821917808222e-05,
      "loss": 0.0638,
      "step": 530
    },
    {
      "epoch": 0.024657534246575342,
      "grad_norm": 0.9026920795440674,
      "learning_rate": 2.9267123287671233e-05,
      "loss": 0.0505,
      "step": 540
    },
    {
      "epoch": 0.02511415525114155,
      "grad_norm": 0.9641650915145874,
      "learning_rate": 2.9253424657534248e-05,
      "loss": 0.0347,
      "step": 550
    },
    {
      "epoch": 0.025570776255707764,
      "grad_norm": 1.3138294219970703,
      "learning_rate": 2.923972602739726e-05,
      "loss": 0.0636,
      "step": 560
    },
    {
      "epoch": 0.026027397260273973,
      "grad_norm": 1.3943803310394287,
      "learning_rate": 2.9226027397260273e-05,
      "loss": 0.0498,
      "step": 570
    },
    {
      "epoch": 0.026484018264840183,
      "grad_norm": 0.9236640334129333,
      "learning_rate": 2.921232876712329e-05,
      "loss": 0.0456,
      "step": 580
    },
    {
      "epoch": 0.026940639269406392,
      "grad_norm": 0.7704586386680603,
      "learning_rate": 2.9198630136986302e-05,
      "loss": 0.0569,
      "step": 590
    },
    {
      "epoch": 0.0273972602739726,
      "grad_norm": 1.1557788848876953,
      "learning_rate": 2.9184931506849317e-05,
      "loss": 0.0398,
      "step": 600
    },
    {
      "epoch": 0.027853881278538814,
      "grad_norm": 1.2871992588043213,
      "learning_rate": 2.9171232876712328e-05,
      "loss": 0.0431,
      "step": 610
    },
    {
      "epoch": 0.028310502283105023,
      "grad_norm": 1.139630913734436,
      "learning_rate": 2.9157534246575343e-05,
      "loss": 0.0473,
      "step": 620
    },
    {
      "epoch": 0.028767123287671233,
      "grad_norm": 0.6328549981117249,
      "learning_rate": 2.9143835616438357e-05,
      "loss": 0.0668,
      "step": 630
    },
    {
      "epoch": 0.029223744292237442,
      "grad_norm": 1.6260937452316284,
      "learning_rate": 2.9130136986301372e-05,
      "loss": 0.0517,
      "step": 640
    },
    {
      "epoch": 0.02968036529680365,
      "grad_norm": 1.7985517978668213,
      "learning_rate": 2.9116438356164386e-05,
      "loss": 0.0492,
      "step": 650
    },
    {
      "epoch": 0.030136986301369864,
      "grad_norm": 1.1402415037155151,
      "learning_rate": 2.9102739726027398e-05,
      "loss": 0.0377,
      "step": 660
    },
    {
      "epoch": 0.030593607305936073,
      "grad_norm": 1.1162745952606201,
      "learning_rate": 2.9089041095890412e-05,
      "loss": 0.0513,
      "step": 670
    },
    {
      "epoch": 0.031050228310502283,
      "grad_norm": 1.3536741733551025,
      "learning_rate": 2.9075342465753423e-05,
      "loss": 0.0445,
      "step": 680
    },
    {
      "epoch": 0.031506849315068496,
      "grad_norm": 0.8192769885063171,
      "learning_rate": 2.9061643835616438e-05,
      "loss": 0.0353,
      "step": 690
    },
    {
      "epoch": 0.0319634703196347,
      "grad_norm": 1.26319420337677,
      "learning_rate": 2.9047945205479453e-05,
      "loss": 0.0383,
      "step": 700
    },
    {
      "epoch": 0.032420091324200914,
      "grad_norm": 1.0122944116592407,
      "learning_rate": 2.9034246575342467e-05,
      "loss": 0.0439,
      "step": 710
    },
    {
      "epoch": 0.03287671232876712,
      "grad_norm": 1.1895229816436768,
      "learning_rate": 2.9020547945205482e-05,
      "loss": 0.0479,
      "step": 720
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.5996974110603333,
      "learning_rate": 2.9006849315068493e-05,
      "loss": 0.0459,
      "step": 730
    },
    {
      "epoch": 0.033789954337899546,
      "grad_norm": 2.181468963623047,
      "learning_rate": 2.8993150684931507e-05,
      "loss": 0.0487,
      "step": 740
    },
    {
      "epoch": 0.03424657534246575,
      "grad_norm": 3.858323812484741,
      "learning_rate": 2.8979452054794522e-05,
      "loss": 0.0426,
      "step": 750
    },
    {
      "epoch": 0.034703196347031964,
      "grad_norm": 0.7009260058403015,
      "learning_rate": 2.8965753424657533e-05,
      "loss": 0.048,
      "step": 760
    },
    {
      "epoch": 0.03515981735159817,
      "grad_norm": 1.6838433742523193,
      "learning_rate": 2.8952054794520548e-05,
      "loss": 0.0387,
      "step": 770
    },
    {
      "epoch": 0.03561643835616438,
      "grad_norm": 1.228136658668518,
      "learning_rate": 2.8938356164383562e-05,
      "loss": 0.0387,
      "step": 780
    },
    {
      "epoch": 0.036073059360730596,
      "grad_norm": 1.1616355180740356,
      "learning_rate": 2.8924657534246577e-05,
      "loss": 0.0496,
      "step": 790
    },
    {
      "epoch": 0.0365296803652968,
      "grad_norm": 1.6799992322921753,
      "learning_rate": 2.891095890410959e-05,
      "loss": 0.0557,
      "step": 800
    },
    {
      "epoch": 0.036986301369863014,
      "grad_norm": 1.1291719675064087,
      "learning_rate": 2.8897260273972603e-05,
      "loss": 0.0388,
      "step": 810
    },
    {
      "epoch": 0.03744292237442922,
      "grad_norm": 0.9585476517677307,
      "learning_rate": 2.8883561643835617e-05,
      "loss": 0.0391,
      "step": 820
    },
    {
      "epoch": 0.03789954337899543,
      "grad_norm": 1.076064109802246,
      "learning_rate": 2.886986301369863e-05,
      "loss": 0.0388,
      "step": 830
    },
    {
      "epoch": 0.038356164383561646,
      "grad_norm": 0.9548675417900085,
      "learning_rate": 2.8856164383561646e-05,
      "loss": 0.0474,
      "step": 840
    },
    {
      "epoch": 0.03881278538812785,
      "grad_norm": 0.6091785430908203,
      "learning_rate": 2.884246575342466e-05,
      "loss": 0.0405,
      "step": 850
    },
    {
      "epoch": 0.039269406392694065,
      "grad_norm": 0.5964647531509399,
      "learning_rate": 2.8828767123287672e-05,
      "loss": 0.0453,
      "step": 860
    },
    {
      "epoch": 0.03972602739726028,
      "grad_norm": 1.064834713935852,
      "learning_rate": 2.8815068493150687e-05,
      "loss": 0.0402,
      "step": 870
    },
    {
      "epoch": 0.04018264840182648,
      "grad_norm": 0.7378220558166504,
      "learning_rate": 2.8801369863013698e-05,
      "loss": 0.0365,
      "step": 880
    },
    {
      "epoch": 0.040639269406392696,
      "grad_norm": 1.285792589187622,
      "learning_rate": 2.8787671232876712e-05,
      "loss": 0.0447,
      "step": 890
    },
    {
      "epoch": 0.0410958904109589,
      "grad_norm": 1.264236330986023,
      "learning_rate": 2.8773972602739727e-05,
      "loss": 0.0418,
      "step": 900
    },
    {
      "epoch": 0.041552511415525115,
      "grad_norm": 0.45370355248451233,
      "learning_rate": 2.876027397260274e-05,
      "loss": 0.0339,
      "step": 910
    },
    {
      "epoch": 0.04200913242009133,
      "grad_norm": 0.9969292283058167,
      "learning_rate": 2.8746575342465756e-05,
      "loss": 0.0498,
      "step": 920
    },
    {
      "epoch": 0.04246575342465753,
      "grad_norm": 3.414316177368164,
      "learning_rate": 2.8732876712328767e-05,
      "loss": 0.0398,
      "step": 930
    },
    {
      "epoch": 0.042922374429223746,
      "grad_norm": 1.2120004892349243,
      "learning_rate": 2.8719178082191782e-05,
      "loss": 0.0408,
      "step": 940
    },
    {
      "epoch": 0.04337899543378995,
      "grad_norm": 1.29774010181427,
      "learning_rate": 2.8705479452054793e-05,
      "loss": 0.0407,
      "step": 950
    },
    {
      "epoch": 0.043835616438356165,
      "grad_norm": 1.2249648571014404,
      "learning_rate": 2.8691780821917808e-05,
      "loss": 0.0425,
      "step": 960
    },
    {
      "epoch": 0.04429223744292238,
      "grad_norm": 1.2296459674835205,
      "learning_rate": 2.8678082191780822e-05,
      "loss": 0.0529,
      "step": 970
    },
    {
      "epoch": 0.04474885844748858,
      "grad_norm": 0.666769802570343,
      "learning_rate": 2.8664383561643837e-05,
      "loss": 0.0404,
      "step": 980
    },
    {
      "epoch": 0.045205479452054796,
      "grad_norm": 0.8574283719062805,
      "learning_rate": 2.865068493150685e-05,
      "loss": 0.0597,
      "step": 990
    },
    {
      "epoch": 0.045662100456621,
      "grad_norm": 0.4717479646205902,
      "learning_rate": 2.8636986301369863e-05,
      "loss": 0.0423,
      "step": 1000
    },
    {
      "epoch": 0.046118721461187215,
      "grad_norm": 1.0207767486572266,
      "learning_rate": 2.8623287671232877e-05,
      "loss": 0.0545,
      "step": 1010
    },
    {
      "epoch": 0.04657534246575343,
      "grad_norm": 1.2297786474227905,
      "learning_rate": 2.8609589041095892e-05,
      "loss": 0.0545,
      "step": 1020
    },
    {
      "epoch": 0.047031963470319633,
      "grad_norm": 0.9264557957649231,
      "learning_rate": 2.8595890410958903e-05,
      "loss": 0.0429,
      "step": 1030
    },
    {
      "epoch": 0.047488584474885846,
      "grad_norm": 0.6089246869087219,
      "learning_rate": 2.858219178082192e-05,
      "loss": 0.0294,
      "step": 1040
    },
    {
      "epoch": 0.04794520547945205,
      "grad_norm": 1.430214762687683,
      "learning_rate": 2.8568493150684932e-05,
      "loss": 0.0449,
      "step": 1050
    },
    {
      "epoch": 0.048401826484018265,
      "grad_norm": 1.3296153545379639,
      "learning_rate": 2.8554794520547947e-05,
      "loss": 0.0406,
      "step": 1060
    },
    {
      "epoch": 0.04885844748858448,
      "grad_norm": 1.884820818901062,
      "learning_rate": 2.854109589041096e-05,
      "loss": 0.0408,
      "step": 1070
    },
    {
      "epoch": 0.049315068493150684,
      "grad_norm": 0.9767186641693115,
      "learning_rate": 2.8527397260273972e-05,
      "loss": 0.0441,
      "step": 1080
    },
    {
      "epoch": 0.049771689497716896,
      "grad_norm": 1.0916759967803955,
      "learning_rate": 2.8513698630136987e-05,
      "loss": 0.0384,
      "step": 1090
    },
    {
      "epoch": 0.0502283105022831,
      "grad_norm": 1.4782378673553467,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.0372,
      "step": 1100
    },
    {
      "epoch": 0.050684931506849315,
      "grad_norm": 2.341148614883423,
      "learning_rate": 2.8486301369863016e-05,
      "loss": 0.0378,
      "step": 1110
    },
    {
      "epoch": 0.05114155251141553,
      "grad_norm": 1.8975260257720947,
      "learning_rate": 2.847260273972603e-05,
      "loss": 0.043,
      "step": 1120
    },
    {
      "epoch": 0.051598173515981734,
      "grad_norm": 0.6771071553230286,
      "learning_rate": 2.8458904109589042e-05,
      "loss": 0.0469,
      "step": 1130
    },
    {
      "epoch": 0.052054794520547946,
      "grad_norm": 0.4252431392669678,
      "learning_rate": 2.8445205479452056e-05,
      "loss": 0.0425,
      "step": 1140
    },
    {
      "epoch": 0.05251141552511415,
      "grad_norm": 0.9727302193641663,
      "learning_rate": 2.8431506849315068e-05,
      "loss": 0.039,
      "step": 1150
    },
    {
      "epoch": 0.052968036529680365,
      "grad_norm": 1.290204405784607,
      "learning_rate": 2.8417808219178082e-05,
      "loss": 0.0361,
      "step": 1160
    },
    {
      "epoch": 0.05342465753424658,
      "grad_norm": 1.2226874828338623,
      "learning_rate": 2.8404109589041097e-05,
      "loss": 0.0449,
      "step": 1170
    },
    {
      "epoch": 0.053881278538812784,
      "grad_norm": 1.1163618564605713,
      "learning_rate": 2.839041095890411e-05,
      "loss": 0.064,
      "step": 1180
    },
    {
      "epoch": 0.054337899543379,
      "grad_norm": 0.8527857065200806,
      "learning_rate": 2.8376712328767126e-05,
      "loss": 0.048,
      "step": 1190
    },
    {
      "epoch": 0.0547945205479452,
      "grad_norm": 0.5308504700660706,
      "learning_rate": 2.8363013698630137e-05,
      "loss": 0.0338,
      "step": 1200
    },
    {
      "epoch": 0.055251141552511415,
      "grad_norm": 1.4965542554855347,
      "learning_rate": 2.8349315068493152e-05,
      "loss": 0.0371,
      "step": 1210
    },
    {
      "epoch": 0.05570776255707763,
      "grad_norm": 2.30342698097229,
      "learning_rate": 2.8335616438356163e-05,
      "loss": 0.0375,
      "step": 1220
    },
    {
      "epoch": 0.056164383561643834,
      "grad_norm": 0.6390032768249512,
      "learning_rate": 2.8321917808219177e-05,
      "loss": 0.0431,
      "step": 1230
    },
    {
      "epoch": 0.05662100456621005,
      "grad_norm": 0.9324535131454468,
      "learning_rate": 2.8308219178082192e-05,
      "loss": 0.0349,
      "step": 1240
    },
    {
      "epoch": 0.05707762557077625,
      "grad_norm": 1.8866300582885742,
      "learning_rate": 2.8294520547945207e-05,
      "loss": 0.0397,
      "step": 1250
    },
    {
      "epoch": 0.057534246575342465,
      "grad_norm": 1.105863332748413,
      "learning_rate": 2.828082191780822e-05,
      "loss": 0.0449,
      "step": 1260
    },
    {
      "epoch": 0.05799086757990868,
      "grad_norm": 0.866005003452301,
      "learning_rate": 2.8267123287671232e-05,
      "loss": 0.043,
      "step": 1270
    },
    {
      "epoch": 0.058447488584474884,
      "grad_norm": 0.9182422161102295,
      "learning_rate": 2.8253424657534247e-05,
      "loss": 0.0421,
      "step": 1280
    },
    {
      "epoch": 0.0589041095890411,
      "grad_norm": 0.6776736378669739,
      "learning_rate": 2.823972602739726e-05,
      "loss": 0.0283,
      "step": 1290
    },
    {
      "epoch": 0.0593607305936073,
      "grad_norm": 1.7614723443984985,
      "learning_rate": 2.8226027397260273e-05,
      "loss": 0.0601,
      "step": 1300
    },
    {
      "epoch": 0.059817351598173515,
      "grad_norm": 0.6317474246025085,
      "learning_rate": 2.821232876712329e-05,
      "loss": 0.0488,
      "step": 1310
    },
    {
      "epoch": 0.06027397260273973,
      "grad_norm": 1.3099145889282227,
      "learning_rate": 2.8198630136986302e-05,
      "loss": 0.0407,
      "step": 1320
    },
    {
      "epoch": 0.060730593607305934,
      "grad_norm": 1.4781827926635742,
      "learning_rate": 2.8184931506849316e-05,
      "loss": 0.0343,
      "step": 1330
    },
    {
      "epoch": 0.06118721461187215,
      "grad_norm": 1.099867582321167,
      "learning_rate": 2.817123287671233e-05,
      "loss": 0.0301,
      "step": 1340
    },
    {
      "epoch": 0.06164383561643835,
      "grad_norm": 1.3782895803451538,
      "learning_rate": 2.8157534246575342e-05,
      "loss": 0.0432,
      "step": 1350
    },
    {
      "epoch": 0.062100456621004566,
      "grad_norm": 2.5311737060546875,
      "learning_rate": 2.8143835616438357e-05,
      "loss": 0.0511,
      "step": 1360
    },
    {
      "epoch": 0.06255707762557078,
      "grad_norm": 0.9255328178405762,
      "learning_rate": 2.8130136986301368e-05,
      "loss": 0.039,
      "step": 1370
    },
    {
      "epoch": 0.06301369863013699,
      "grad_norm": 0.7610028386116028,
      "learning_rate": 2.8116438356164386e-05,
      "loss": 0.0484,
      "step": 1380
    },
    {
      "epoch": 0.06347031963470319,
      "grad_norm": 0.3721295893192291,
      "learning_rate": 2.81027397260274e-05,
      "loss": 0.0225,
      "step": 1390
    },
    {
      "epoch": 0.0639269406392694,
      "grad_norm": 2.2980213165283203,
      "learning_rate": 2.808904109589041e-05,
      "loss": 0.0413,
      "step": 1400
    },
    {
      "epoch": 0.06438356164383562,
      "grad_norm": 0.7403649091720581,
      "learning_rate": 2.8075342465753426e-05,
      "loss": 0.0407,
      "step": 1410
    },
    {
      "epoch": 0.06484018264840183,
      "grad_norm": 0.8498013615608215,
      "learning_rate": 2.8061643835616437e-05,
      "loss": 0.0577,
      "step": 1420
    },
    {
      "epoch": 0.06529680365296804,
      "grad_norm": 0.9889243841171265,
      "learning_rate": 2.8047945205479452e-05,
      "loss": 0.0561,
      "step": 1430
    },
    {
      "epoch": 0.06575342465753424,
      "grad_norm": 1.448830485343933,
      "learning_rate": 2.8034246575342463e-05,
      "loss": 0.0426,
      "step": 1440
    },
    {
      "epoch": 0.06621004566210045,
      "grad_norm": 1.565636396408081,
      "learning_rate": 2.802054794520548e-05,
      "loss": 0.0494,
      "step": 1450
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 1.9864698648452759,
      "learning_rate": 2.8006849315068496e-05,
      "loss": 0.0472,
      "step": 1460
    },
    {
      "epoch": 0.06712328767123288,
      "grad_norm": 0.8028621673583984,
      "learning_rate": 2.7993150684931507e-05,
      "loss": 0.0541,
      "step": 1470
    },
    {
      "epoch": 0.06757990867579909,
      "grad_norm": 0.8390869498252869,
      "learning_rate": 2.797945205479452e-05,
      "loss": 0.043,
      "step": 1480
    },
    {
      "epoch": 0.06803652968036529,
      "grad_norm": 1.5572235584259033,
      "learning_rate": 2.7965753424657533e-05,
      "loss": 0.0481,
      "step": 1490
    },
    {
      "epoch": 0.0684931506849315,
      "grad_norm": 1.193710207939148,
      "learning_rate": 2.7952054794520547e-05,
      "loss": 0.0395,
      "step": 1500
    },
    {
      "epoch": 0.06894977168949772,
      "grad_norm": 1.240532398223877,
      "learning_rate": 2.7938356164383565e-05,
      "loss": 0.0382,
      "step": 1510
    },
    {
      "epoch": 0.06940639269406393,
      "grad_norm": 1.077344298362732,
      "learning_rate": 2.7924657534246576e-05,
      "loss": 0.0406,
      "step": 1520
    },
    {
      "epoch": 0.06986301369863014,
      "grad_norm": 1.1514923572540283,
      "learning_rate": 2.791095890410959e-05,
      "loss": 0.0337,
      "step": 1530
    },
    {
      "epoch": 0.07031963470319634,
      "grad_norm": 3.2549779415130615,
      "learning_rate": 2.7897260273972602e-05,
      "loss": 0.0559,
      "step": 1540
    },
    {
      "epoch": 0.07077625570776255,
      "grad_norm": 1.4238077402114868,
      "learning_rate": 2.7883561643835617e-05,
      "loss": 0.04,
      "step": 1550
    },
    {
      "epoch": 0.07123287671232877,
      "grad_norm": 0.4229277968406677,
      "learning_rate": 2.786986301369863e-05,
      "loss": 0.0427,
      "step": 1560
    },
    {
      "epoch": 0.07168949771689498,
      "grad_norm": 0.5540062189102173,
      "learning_rate": 2.7856164383561642e-05,
      "loss": 0.0296,
      "step": 1570
    },
    {
      "epoch": 0.07214611872146119,
      "grad_norm": 0.877108633518219,
      "learning_rate": 2.784246575342466e-05,
      "loss": 0.035,
      "step": 1580
    },
    {
      "epoch": 0.07260273972602739,
      "grad_norm": 0.6161752343177795,
      "learning_rate": 2.782876712328767e-05,
      "loss": 0.031,
      "step": 1590
    },
    {
      "epoch": 0.0730593607305936,
      "grad_norm": 2.3038833141326904,
      "learning_rate": 2.7815068493150686e-05,
      "loss": 0.0442,
      "step": 1600
    },
    {
      "epoch": 0.07351598173515982,
      "grad_norm": 0.6095194220542908,
      "learning_rate": 2.78013698630137e-05,
      "loss": 0.0559,
      "step": 1610
    },
    {
      "epoch": 0.07397260273972603,
      "grad_norm": 0.9951409101486206,
      "learning_rate": 2.7787671232876712e-05,
      "loss": 0.0467,
      "step": 1620
    },
    {
      "epoch": 0.07442922374429224,
      "grad_norm": 0.9944403171539307,
      "learning_rate": 2.7773972602739726e-05,
      "loss": 0.0421,
      "step": 1630
    },
    {
      "epoch": 0.07488584474885844,
      "grad_norm": 4.688938617706299,
      "learning_rate": 2.7760273972602738e-05,
      "loss": 0.0304,
      "step": 1640
    },
    {
      "epoch": 0.07534246575342465,
      "grad_norm": 0.5791969895362854,
      "learning_rate": 2.7746575342465756e-05,
      "loss": 0.0335,
      "step": 1650
    },
    {
      "epoch": 0.07579908675799087,
      "grad_norm": 0.3593866527080536,
      "learning_rate": 2.773287671232877e-05,
      "loss": 0.0373,
      "step": 1660
    },
    {
      "epoch": 0.07625570776255708,
      "grad_norm": 1.6440216302871704,
      "learning_rate": 2.771917808219178e-05,
      "loss": 0.0416,
      "step": 1670
    },
    {
      "epoch": 0.07671232876712329,
      "grad_norm": 1.0060617923736572,
      "learning_rate": 2.7705479452054796e-05,
      "loss": 0.0393,
      "step": 1680
    },
    {
      "epoch": 0.0771689497716895,
      "grad_norm": 0.8483237028121948,
      "learning_rate": 2.7691780821917807e-05,
      "loss": 0.0294,
      "step": 1690
    },
    {
      "epoch": 0.0776255707762557,
      "grad_norm": 0.8405387997627258,
      "learning_rate": 2.7678082191780822e-05,
      "loss": 0.0454,
      "step": 1700
    },
    {
      "epoch": 0.07808219178082192,
      "grad_norm": 0.32569146156311035,
      "learning_rate": 2.7664383561643836e-05,
      "loss": 0.0397,
      "step": 1710
    },
    {
      "epoch": 0.07853881278538813,
      "grad_norm": 1.5286155939102173,
      "learning_rate": 2.765068493150685e-05,
      "loss": 0.0567,
      "step": 1720
    },
    {
      "epoch": 0.07899543378995434,
      "grad_norm": 1.735736608505249,
      "learning_rate": 2.7636986301369865e-05,
      "loss": 0.0357,
      "step": 1730
    },
    {
      "epoch": 0.07945205479452055,
      "grad_norm": 1.068263292312622,
      "learning_rate": 2.7623287671232877e-05,
      "loss": 0.0455,
      "step": 1740
    },
    {
      "epoch": 0.07990867579908675,
      "grad_norm": 0.9032262563705444,
      "learning_rate": 2.760958904109589e-05,
      "loss": 0.0345,
      "step": 1750
    },
    {
      "epoch": 0.08036529680365297,
      "grad_norm": 0.788111686706543,
      "learning_rate": 2.7595890410958902e-05,
      "loss": 0.0282,
      "step": 1760
    },
    {
      "epoch": 0.08082191780821918,
      "grad_norm": 1.2580991983413696,
      "learning_rate": 2.7582191780821917e-05,
      "loss": 0.0373,
      "step": 1770
    },
    {
      "epoch": 0.08127853881278539,
      "grad_norm": 0.6422262191772461,
      "learning_rate": 2.7568493150684935e-05,
      "loss": 0.0322,
      "step": 1780
    },
    {
      "epoch": 0.0817351598173516,
      "grad_norm": 0.6150151491165161,
      "learning_rate": 2.7554794520547946e-05,
      "loss": 0.032,
      "step": 1790
    },
    {
      "epoch": 0.0821917808219178,
      "grad_norm": 1.6399866342544556,
      "learning_rate": 2.754109589041096e-05,
      "loss": 0.1081,
      "step": 1800
    },
    {
      "epoch": 0.08264840182648402,
      "grad_norm": 2.979229211807251,
      "learning_rate": 2.7527397260273972e-05,
      "loss": 0.0342,
      "step": 1810
    },
    {
      "epoch": 0.08310502283105023,
      "grad_norm": 0.9237160682678223,
      "learning_rate": 2.7513698630136986e-05,
      "loss": 0.0364,
      "step": 1820
    },
    {
      "epoch": 0.08356164383561644,
      "grad_norm": 1.2659441232681274,
      "learning_rate": 2.75e-05,
      "loss": 0.0382,
      "step": 1830
    },
    {
      "epoch": 0.08401826484018265,
      "grad_norm": 1.1232197284698486,
      "learning_rate": 2.7486301369863012e-05,
      "loss": 0.0468,
      "step": 1840
    },
    {
      "epoch": 0.08447488584474885,
      "grad_norm": 1.0266398191452026,
      "learning_rate": 2.747260273972603e-05,
      "loss": 0.0371,
      "step": 1850
    },
    {
      "epoch": 0.08493150684931507,
      "grad_norm": 1.3367249965667725,
      "learning_rate": 2.745890410958904e-05,
      "loss": 0.0449,
      "step": 1860
    },
    {
      "epoch": 0.08538812785388128,
      "grad_norm": 0.6205911040306091,
      "learning_rate": 2.7445205479452056e-05,
      "loss": 0.044,
      "step": 1870
    },
    {
      "epoch": 0.08584474885844749,
      "grad_norm": 0.33799949288368225,
      "learning_rate": 2.743150684931507e-05,
      "loss": 0.0359,
      "step": 1880
    },
    {
      "epoch": 0.0863013698630137,
      "grad_norm": 1.2436721324920654,
      "learning_rate": 2.741780821917808e-05,
      "loss": 0.0534,
      "step": 1890
    },
    {
      "epoch": 0.0867579908675799,
      "grad_norm": 0.6591087579727173,
      "learning_rate": 2.7404109589041096e-05,
      "loss": 0.0314,
      "step": 1900
    },
    {
      "epoch": 0.08721461187214612,
      "grad_norm": 1.5062060356140137,
      "learning_rate": 2.739041095890411e-05,
      "loss": 0.0401,
      "step": 1910
    },
    {
      "epoch": 0.08767123287671233,
      "grad_norm": 0.9526385068893433,
      "learning_rate": 2.7376712328767125e-05,
      "loss": 0.0323,
      "step": 1920
    },
    {
      "epoch": 0.08812785388127854,
      "grad_norm": 2.616457939147949,
      "learning_rate": 2.736301369863014e-05,
      "loss": 0.0367,
      "step": 1930
    },
    {
      "epoch": 0.08858447488584476,
      "grad_norm": 0.9116426706314087,
      "learning_rate": 2.734931506849315e-05,
      "loss": 0.0425,
      "step": 1940
    },
    {
      "epoch": 0.08904109589041095,
      "grad_norm": 1.0095996856689453,
      "learning_rate": 2.7335616438356166e-05,
      "loss": 0.0416,
      "step": 1950
    },
    {
      "epoch": 0.08949771689497717,
      "grad_norm": 0.967067301273346,
      "learning_rate": 2.7321917808219177e-05,
      "loss": 0.0346,
      "step": 1960
    },
    {
      "epoch": 0.08995433789954338,
      "grad_norm": 0.6189596056938171,
      "learning_rate": 2.730821917808219e-05,
      "loss": 0.0286,
      "step": 1970
    },
    {
      "epoch": 0.09041095890410959,
      "grad_norm": 4.6462507247924805,
      "learning_rate": 2.7294520547945206e-05,
      "loss": 0.0489,
      "step": 1980
    },
    {
      "epoch": 0.0908675799086758,
      "grad_norm": 0.5206642150878906,
      "learning_rate": 2.728082191780822e-05,
      "loss": 0.0335,
      "step": 1990
    },
    {
      "epoch": 0.091324200913242,
      "grad_norm": 0.38972291350364685,
      "learning_rate": 2.7267123287671235e-05,
      "loss": 0.0337,
      "step": 2000
    },
    {
      "epoch": 0.09178082191780822,
      "grad_norm": 2.510084629058838,
      "learning_rate": 2.7253424657534246e-05,
      "loss": 0.0417,
      "step": 2010
    },
    {
      "epoch": 0.09223744292237443,
      "grad_norm": 1.3994091749191284,
      "learning_rate": 2.723972602739726e-05,
      "loss": 0.0373,
      "step": 2020
    },
    {
      "epoch": 0.09269406392694064,
      "grad_norm": 0.5405195951461792,
      "learning_rate": 2.7226027397260272e-05,
      "loss": 0.0354,
      "step": 2030
    },
    {
      "epoch": 0.09315068493150686,
      "grad_norm": 0.9638215899467468,
      "learning_rate": 2.7212328767123287e-05,
      "loss": 0.0318,
      "step": 2040
    },
    {
      "epoch": 0.09360730593607305,
      "grad_norm": 1.0959839820861816,
      "learning_rate": 2.7198630136986305e-05,
      "loss": 0.0511,
      "step": 2050
    },
    {
      "epoch": 0.09406392694063927,
      "grad_norm": 2.2613625526428223,
      "learning_rate": 2.7184931506849316e-05,
      "loss": 0.0307,
      "step": 2060
    },
    {
      "epoch": 0.09452054794520548,
      "grad_norm": 1.1474581956863403,
      "learning_rate": 2.717123287671233e-05,
      "loss": 0.0379,
      "step": 2070
    },
    {
      "epoch": 0.09497716894977169,
      "grad_norm": 0.8748509883880615,
      "learning_rate": 2.715753424657534e-05,
      "loss": 0.0258,
      "step": 2080
    },
    {
      "epoch": 0.0954337899543379,
      "grad_norm": 1.104946255683899,
      "learning_rate": 2.7143835616438356e-05,
      "loss": 0.0288,
      "step": 2090
    },
    {
      "epoch": 0.0958904109589041,
      "grad_norm": 0.8884478807449341,
      "learning_rate": 2.713013698630137e-05,
      "loss": 0.0318,
      "step": 2100
    },
    {
      "epoch": 0.09634703196347032,
      "grad_norm": 0.835564911365509,
      "learning_rate": 2.7116438356164385e-05,
      "loss": 0.0364,
      "step": 2110
    },
    {
      "epoch": 0.09680365296803653,
      "grad_norm": 0.7173551917076111,
      "learning_rate": 2.71027397260274e-05,
      "loss": 0.0371,
      "step": 2120
    },
    {
      "epoch": 0.09726027397260274,
      "grad_norm": 1.1907943487167358,
      "learning_rate": 2.708904109589041e-05,
      "loss": 0.0506,
      "step": 2130
    },
    {
      "epoch": 0.09771689497716896,
      "grad_norm": 0.7205183506011963,
      "learning_rate": 2.7075342465753426e-05,
      "loss": 0.0253,
      "step": 2140
    },
    {
      "epoch": 0.09817351598173515,
      "grad_norm": 0.265344500541687,
      "learning_rate": 2.706164383561644e-05,
      "loss": 0.0282,
      "step": 2150
    },
    {
      "epoch": 0.09863013698630137,
      "grad_norm": 1.4085792303085327,
      "learning_rate": 2.704794520547945e-05,
      "loss": 0.0369,
      "step": 2160
    },
    {
      "epoch": 0.09908675799086758,
      "grad_norm": 0.4795975387096405,
      "learning_rate": 2.7034246575342466e-05,
      "loss": 0.0425,
      "step": 2170
    },
    {
      "epoch": 0.09954337899543379,
      "grad_norm": 0.5738417506217957,
      "learning_rate": 2.702054794520548e-05,
      "loss": 0.0313,
      "step": 2180
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.0581001043319702,
      "learning_rate": 2.7006849315068495e-05,
      "loss": 0.0298,
      "step": 2190
    },
    {
      "epoch": 0.1004566210045662,
      "grad_norm": 0.47591888904571533,
      "learning_rate": 2.6993150684931506e-05,
      "loss": 0.0413,
      "step": 2200
    },
    {
      "epoch": 0.10091324200913242,
      "grad_norm": 1.3278571367263794,
      "learning_rate": 2.697945205479452e-05,
      "loss": 0.0287,
      "step": 2210
    },
    {
      "epoch": 0.10136986301369863,
      "grad_norm": 1.1878671646118164,
      "learning_rate": 2.6965753424657535e-05,
      "loss": 0.0378,
      "step": 2220
    },
    {
      "epoch": 0.10182648401826484,
      "grad_norm": 1.177527666091919,
      "learning_rate": 2.6952054794520547e-05,
      "loss": 0.0392,
      "step": 2230
    },
    {
      "epoch": 0.10228310502283106,
      "grad_norm": 0.841049075126648,
      "learning_rate": 2.693835616438356e-05,
      "loss": 0.0301,
      "step": 2240
    },
    {
      "epoch": 0.10273972602739725,
      "grad_norm": 0.9294116497039795,
      "learning_rate": 2.6924657534246576e-05,
      "loss": 0.037,
      "step": 2250
    },
    {
      "epoch": 0.10319634703196347,
      "grad_norm": 1.2012773752212524,
      "learning_rate": 2.691095890410959e-05,
      "loss": 0.0418,
      "step": 2260
    },
    {
      "epoch": 0.10365296803652968,
      "grad_norm": 1.7437412738800049,
      "learning_rate": 2.6897260273972605e-05,
      "loss": 0.0359,
      "step": 2270
    },
    {
      "epoch": 0.10410958904109589,
      "grad_norm": 1.148818850517273,
      "learning_rate": 2.6883561643835616e-05,
      "loss": 0.0413,
      "step": 2280
    },
    {
      "epoch": 0.1045662100456621,
      "grad_norm": 1.2093998193740845,
      "learning_rate": 2.686986301369863e-05,
      "loss": 0.0455,
      "step": 2290
    },
    {
      "epoch": 0.1050228310502283,
      "grad_norm": 0.4796343445777893,
      "learning_rate": 2.6856164383561642e-05,
      "loss": 0.0279,
      "step": 2300
    },
    {
      "epoch": 0.10547945205479452,
      "grad_norm": 1.0520384311676025,
      "learning_rate": 2.684246575342466e-05,
      "loss": 0.0313,
      "step": 2310
    },
    {
      "epoch": 0.10593607305936073,
      "grad_norm": 0.7231737375259399,
      "learning_rate": 2.6828767123287674e-05,
      "loss": 0.0351,
      "step": 2320
    },
    {
      "epoch": 0.10639269406392694,
      "grad_norm": 1.2579313516616821,
      "learning_rate": 2.6815068493150686e-05,
      "loss": 0.0405,
      "step": 2330
    },
    {
      "epoch": 0.10684931506849316,
      "grad_norm": 1.1662908792495728,
      "learning_rate": 2.68013698630137e-05,
      "loss": 0.0475,
      "step": 2340
    },
    {
      "epoch": 0.10730593607305935,
      "grad_norm": 0.6919782161712646,
      "learning_rate": 2.678767123287671e-05,
      "loss": 0.0383,
      "step": 2350
    },
    {
      "epoch": 0.10776255707762557,
      "grad_norm": 0.5501591563224792,
      "learning_rate": 2.6773972602739726e-05,
      "loss": 0.0327,
      "step": 2360
    },
    {
      "epoch": 0.10821917808219178,
      "grad_norm": 0.40390923619270325,
      "learning_rate": 2.676027397260274e-05,
      "loss": 0.03,
      "step": 2370
    },
    {
      "epoch": 0.108675799086758,
      "grad_norm": 1.4023408889770508,
      "learning_rate": 2.6746575342465755e-05,
      "loss": 0.0342,
      "step": 2380
    },
    {
      "epoch": 0.1091324200913242,
      "grad_norm": 1.0047833919525146,
      "learning_rate": 2.673287671232877e-05,
      "loss": 0.047,
      "step": 2390
    },
    {
      "epoch": 0.1095890410958904,
      "grad_norm": 0.6397455334663391,
      "learning_rate": 2.671917808219178e-05,
      "loss": 0.0459,
      "step": 2400
    },
    {
      "epoch": 0.11004566210045662,
      "grad_norm": 1.0789158344268799,
      "learning_rate": 2.6705479452054795e-05,
      "loss": 0.0448,
      "step": 2410
    },
    {
      "epoch": 0.11050228310502283,
      "grad_norm": 0.9887068867683411,
      "learning_rate": 2.669178082191781e-05,
      "loss": 0.0406,
      "step": 2420
    },
    {
      "epoch": 0.11095890410958904,
      "grad_norm": 1.478820562362671,
      "learning_rate": 2.667808219178082e-05,
      "loss": 0.045,
      "step": 2430
    },
    {
      "epoch": 0.11141552511415526,
      "grad_norm": 0.6679337024688721,
      "learning_rate": 2.6664383561643836e-05,
      "loss": 0.0357,
      "step": 2440
    },
    {
      "epoch": 0.11187214611872145,
      "grad_norm": 1.428187608718872,
      "learning_rate": 2.665068493150685e-05,
      "loss": 0.0373,
      "step": 2450
    },
    {
      "epoch": 0.11232876712328767,
      "grad_norm": 1.0210236310958862,
      "learning_rate": 2.6636986301369865e-05,
      "loss": 0.0317,
      "step": 2460
    },
    {
      "epoch": 0.11278538812785388,
      "grad_norm": 1.5512975454330444,
      "learning_rate": 2.6623287671232876e-05,
      "loss": 0.0346,
      "step": 2470
    },
    {
      "epoch": 0.1132420091324201,
      "grad_norm": 0.6838537454605103,
      "learning_rate": 2.660958904109589e-05,
      "loss": 0.0372,
      "step": 2480
    },
    {
      "epoch": 0.1136986301369863,
      "grad_norm": 1.0126862525939941,
      "learning_rate": 2.6595890410958905e-05,
      "loss": 0.0365,
      "step": 2490
    },
    {
      "epoch": 0.1141552511415525,
      "grad_norm": 1.164422631263733,
      "learning_rate": 2.6582191780821916e-05,
      "loss": 0.0419,
      "step": 2500
    },
    {
      "epoch": 0.11461187214611872,
      "grad_norm": 0.7878551483154297,
      "learning_rate": 2.6568493150684934e-05,
      "loss": 0.0375,
      "step": 2510
    },
    {
      "epoch": 0.11506849315068493,
      "grad_norm": 2.025148868560791,
      "learning_rate": 2.6554794520547945e-05,
      "loss": 0.0389,
      "step": 2520
    },
    {
      "epoch": 0.11552511415525114,
      "grad_norm": 0.4877706468105316,
      "learning_rate": 2.654109589041096e-05,
      "loss": 0.0367,
      "step": 2530
    },
    {
      "epoch": 0.11598173515981736,
      "grad_norm": 1.7378244400024414,
      "learning_rate": 2.6527397260273975e-05,
      "loss": 0.0465,
      "step": 2540
    },
    {
      "epoch": 0.11643835616438356,
      "grad_norm": 1.045359492301941,
      "learning_rate": 2.6513698630136986e-05,
      "loss": 0.0407,
      "step": 2550
    },
    {
      "epoch": 0.11689497716894977,
      "grad_norm": 0.6131485104560852,
      "learning_rate": 2.65e-05,
      "loss": 0.0322,
      "step": 2560
    },
    {
      "epoch": 0.11735159817351598,
      "grad_norm": 0.5608521699905396,
      "learning_rate": 2.648630136986301e-05,
      "loss": 0.034,
      "step": 2570
    },
    {
      "epoch": 0.1178082191780822,
      "grad_norm": 0.39495232701301575,
      "learning_rate": 2.647260273972603e-05,
      "loss": 0.0355,
      "step": 2580
    },
    {
      "epoch": 0.1182648401826484,
      "grad_norm": 0.7737281322479248,
      "learning_rate": 2.6458904109589044e-05,
      "loss": 0.0298,
      "step": 2590
    },
    {
      "epoch": 0.1187214611872146,
      "grad_norm": 0.5156254768371582,
      "learning_rate": 2.6445205479452055e-05,
      "loss": 0.0409,
      "step": 2600
    },
    {
      "epoch": 0.11917808219178082,
      "grad_norm": 0.8682867884635925,
      "learning_rate": 2.643150684931507e-05,
      "loss": 0.0462,
      "step": 2610
    },
    {
      "epoch": 0.11963470319634703,
      "grad_norm": 0.07007398456335068,
      "learning_rate": 2.641780821917808e-05,
      "loss": 0.0425,
      "step": 2620
    },
    {
      "epoch": 0.12009132420091324,
      "grad_norm": 0.7072452306747437,
      "learning_rate": 2.6404109589041096e-05,
      "loss": 0.0498,
      "step": 2630
    },
    {
      "epoch": 0.12054794520547946,
      "grad_norm": 1.6805567741394043,
      "learning_rate": 2.639041095890411e-05,
      "loss": 0.0348,
      "step": 2640
    },
    {
      "epoch": 0.12100456621004566,
      "grad_norm": 0.7701615691184998,
      "learning_rate": 2.6376712328767125e-05,
      "loss": 0.0411,
      "step": 2650
    },
    {
      "epoch": 0.12146118721461187,
      "grad_norm": 0.6806913018226624,
      "learning_rate": 2.636301369863014e-05,
      "loss": 0.0291,
      "step": 2660
    },
    {
      "epoch": 0.12191780821917808,
      "grad_norm": 0.9069223999977112,
      "learning_rate": 2.634931506849315e-05,
      "loss": 0.0409,
      "step": 2670
    },
    {
      "epoch": 0.1223744292237443,
      "grad_norm": 0.8629015684127808,
      "learning_rate": 2.6335616438356165e-05,
      "loss": 0.0328,
      "step": 2680
    },
    {
      "epoch": 0.1228310502283105,
      "grad_norm": 0.6618692874908447,
      "learning_rate": 2.632191780821918e-05,
      "loss": 0.0399,
      "step": 2690
    },
    {
      "epoch": 0.1232876712328767,
      "grad_norm": 0.5925524830818176,
      "learning_rate": 2.630821917808219e-05,
      "loss": 0.0379,
      "step": 2700
    },
    {
      "epoch": 0.12374429223744292,
      "grad_norm": 1.6678128242492676,
      "learning_rate": 2.629452054794521e-05,
      "loss": 0.0395,
      "step": 2710
    },
    {
      "epoch": 0.12420091324200913,
      "grad_norm": 0.5157683491706848,
      "learning_rate": 2.628082191780822e-05,
      "loss": 0.0283,
      "step": 2720
    },
    {
      "epoch": 0.12465753424657534,
      "grad_norm": 2.0046699047088623,
      "learning_rate": 2.6267123287671235e-05,
      "loss": 0.0254,
      "step": 2730
    },
    {
      "epoch": 0.12511415525114156,
      "grad_norm": 0.6528434753417969,
      "learning_rate": 2.6253424657534246e-05,
      "loss": 0.0375,
      "step": 2740
    },
    {
      "epoch": 0.12557077625570776,
      "grad_norm": 0.727563738822937,
      "learning_rate": 2.623972602739726e-05,
      "loss": 0.0394,
      "step": 2750
    },
    {
      "epoch": 0.12602739726027398,
      "grad_norm": 1.3001600503921509,
      "learning_rate": 2.6226027397260275e-05,
      "loss": 0.046,
      "step": 2760
    },
    {
      "epoch": 0.12648401826484018,
      "grad_norm": 0.7084218859672546,
      "learning_rate": 2.6212328767123286e-05,
      "loss": 0.0524,
      "step": 2770
    },
    {
      "epoch": 0.12694063926940638,
      "grad_norm": 0.6635661125183105,
      "learning_rate": 2.6198630136986304e-05,
      "loss": 0.0387,
      "step": 2780
    },
    {
      "epoch": 0.1273972602739726,
      "grad_norm": 0.6178509593009949,
      "learning_rate": 2.6184931506849315e-05,
      "loss": 0.0328,
      "step": 2790
    },
    {
      "epoch": 0.1278538812785388,
      "grad_norm": 0.683761477470398,
      "learning_rate": 2.617123287671233e-05,
      "loss": 0.036,
      "step": 2800
    },
    {
      "epoch": 0.12831050228310503,
      "grad_norm": 2.709719181060791,
      "learning_rate": 2.6157534246575344e-05,
      "loss": 0.0427,
      "step": 2810
    },
    {
      "epoch": 0.12876712328767123,
      "grad_norm": 0.8574289679527283,
      "learning_rate": 2.6143835616438356e-05,
      "loss": 0.0354,
      "step": 2820
    },
    {
      "epoch": 0.12922374429223743,
      "grad_norm": 0.5652356743812561,
      "learning_rate": 2.613013698630137e-05,
      "loss": 0.0293,
      "step": 2830
    },
    {
      "epoch": 0.12968036529680366,
      "grad_norm": 1.536374568939209,
      "learning_rate": 2.611643835616438e-05,
      "loss": 0.0369,
      "step": 2840
    },
    {
      "epoch": 0.13013698630136986,
      "grad_norm": 0.7868465781211853,
      "learning_rate": 2.61027397260274e-05,
      "loss": 0.0368,
      "step": 2850
    },
    {
      "epoch": 0.13059360730593608,
      "grad_norm": 0.6681937575340271,
      "learning_rate": 2.6089041095890414e-05,
      "loss": 0.0403,
      "step": 2860
    },
    {
      "epoch": 0.13105022831050228,
      "grad_norm": 2.0433013439178467,
      "learning_rate": 2.6075342465753425e-05,
      "loss": 0.0471,
      "step": 2870
    },
    {
      "epoch": 0.13150684931506848,
      "grad_norm": 0.7855214476585388,
      "learning_rate": 2.606164383561644e-05,
      "loss": 0.027,
      "step": 2880
    },
    {
      "epoch": 0.1319634703196347,
      "grad_norm": 0.7758203744888306,
      "learning_rate": 2.604794520547945e-05,
      "loss": 0.0385,
      "step": 2890
    },
    {
      "epoch": 0.1324200913242009,
      "grad_norm": 0.9331187605857849,
      "learning_rate": 2.6034246575342465e-05,
      "loss": 0.0419,
      "step": 2900
    },
    {
      "epoch": 0.13287671232876713,
      "grad_norm": 0.804885745048523,
      "learning_rate": 2.6020547945205483e-05,
      "loss": 0.0459,
      "step": 2910
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.32243913412094116,
      "learning_rate": 2.6006849315068495e-05,
      "loss": 0.0318,
      "step": 2920
    },
    {
      "epoch": 0.13378995433789953,
      "grad_norm": 1.6494699716567993,
      "learning_rate": 2.599315068493151e-05,
      "loss": 0.0372,
      "step": 2930
    },
    {
      "epoch": 0.13424657534246576,
      "grad_norm": 0.6068289279937744,
      "learning_rate": 2.597945205479452e-05,
      "loss": 0.0469,
      "step": 2940
    },
    {
      "epoch": 0.13470319634703196,
      "grad_norm": 1.548466444015503,
      "learning_rate": 2.5965753424657535e-05,
      "loss": 0.0445,
      "step": 2950
    },
    {
      "epoch": 0.13515981735159818,
      "grad_norm": 0.7188389897346497,
      "learning_rate": 2.595205479452055e-05,
      "loss": 0.0348,
      "step": 2960
    },
    {
      "epoch": 0.13561643835616438,
      "grad_norm": 1.212679386138916,
      "learning_rate": 2.593835616438356e-05,
      "loss": 0.0301,
      "step": 2970
    },
    {
      "epoch": 0.13607305936073058,
      "grad_norm": 0.840132474899292,
      "learning_rate": 2.592465753424658e-05,
      "loss": 0.0314,
      "step": 2980
    },
    {
      "epoch": 0.1365296803652968,
      "grad_norm": 0.7354640364646912,
      "learning_rate": 2.591095890410959e-05,
      "loss": 0.0302,
      "step": 2990
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 0.9977197051048279,
      "learning_rate": 2.5897260273972604e-05,
      "loss": 0.0369,
      "step": 3000
    },
    {
      "epoch": 0.13744292237442923,
      "grad_norm": 1.734840750694275,
      "learning_rate": 2.5883561643835615e-05,
      "loss": 0.0492,
      "step": 3010
    },
    {
      "epoch": 0.13789954337899543,
      "grad_norm": 2.4627490043640137,
      "learning_rate": 2.586986301369863e-05,
      "loss": 0.0467,
      "step": 3020
    },
    {
      "epoch": 0.13835616438356163,
      "grad_norm": 0.9187779426574707,
      "learning_rate": 2.5856164383561645e-05,
      "loss": 0.0327,
      "step": 3030
    },
    {
      "epoch": 0.13881278538812786,
      "grad_norm": 0.6763878464698792,
      "learning_rate": 2.5842465753424656e-05,
      "loss": 0.035,
      "step": 3040
    },
    {
      "epoch": 0.13926940639269406,
      "grad_norm": 0.9398276805877686,
      "learning_rate": 2.5828767123287674e-05,
      "loss": 0.0303,
      "step": 3050
    },
    {
      "epoch": 0.13972602739726028,
      "grad_norm": 0.9258968830108643,
      "learning_rate": 2.5815068493150685e-05,
      "loss": 0.0389,
      "step": 3060
    },
    {
      "epoch": 0.14018264840182648,
      "grad_norm": 1.7063499689102173,
      "learning_rate": 2.58013698630137e-05,
      "loss": 0.0301,
      "step": 3070
    },
    {
      "epoch": 0.14063926940639268,
      "grad_norm": 0.29099148511886597,
      "learning_rate": 2.5787671232876714e-05,
      "loss": 0.0253,
      "step": 3080
    },
    {
      "epoch": 0.1410958904109589,
      "grad_norm": 0.5839685201644897,
      "learning_rate": 2.5773972602739725e-05,
      "loss": 0.0415,
      "step": 3090
    },
    {
      "epoch": 0.1415525114155251,
      "grad_norm": 0.5823724269866943,
      "learning_rate": 2.576027397260274e-05,
      "loss": 0.0299,
      "step": 3100
    },
    {
      "epoch": 0.14200913242009133,
      "grad_norm": 0.9835200905799866,
      "learning_rate": 2.5746575342465754e-05,
      "loss": 0.0357,
      "step": 3110
    },
    {
      "epoch": 0.14246575342465753,
      "grad_norm": 0.7373268604278564,
      "learning_rate": 2.573287671232877e-05,
      "loss": 0.0304,
      "step": 3120
    },
    {
      "epoch": 0.14292237442922373,
      "grad_norm": 0.6148927807807922,
      "learning_rate": 2.5719178082191784e-05,
      "loss": 0.0291,
      "step": 3130
    },
    {
      "epoch": 0.14337899543378996,
      "grad_norm": 0.6603077054023743,
      "learning_rate": 2.5705479452054795e-05,
      "loss": 0.032,
      "step": 3140
    },
    {
      "epoch": 0.14383561643835616,
      "grad_norm": 3.2882161140441895,
      "learning_rate": 2.569178082191781e-05,
      "loss": 0.0426,
      "step": 3150
    },
    {
      "epoch": 0.14429223744292238,
      "grad_norm": 1.5468722581863403,
      "learning_rate": 2.567808219178082e-05,
      "loss": 0.0398,
      "step": 3160
    },
    {
      "epoch": 0.14474885844748858,
      "grad_norm": 0.2740078270435333,
      "learning_rate": 2.5664383561643835e-05,
      "loss": 0.0394,
      "step": 3170
    },
    {
      "epoch": 0.14520547945205478,
      "grad_norm": 1.1589728593826294,
      "learning_rate": 2.5650684931506853e-05,
      "loss": 0.0236,
      "step": 3180
    },
    {
      "epoch": 0.145662100456621,
      "grad_norm": 0.4817255437374115,
      "learning_rate": 2.5636986301369864e-05,
      "loss": 0.0248,
      "step": 3190
    },
    {
      "epoch": 0.1461187214611872,
      "grad_norm": 0.5358665585517883,
      "learning_rate": 2.562328767123288e-05,
      "loss": 0.0246,
      "step": 3200
    },
    {
      "epoch": 0.14657534246575343,
      "grad_norm": 0.7714756727218628,
      "learning_rate": 2.560958904109589e-05,
      "loss": 0.0214,
      "step": 3210
    },
    {
      "epoch": 0.14703196347031963,
      "grad_norm": 0.39716076850891113,
      "learning_rate": 2.5595890410958905e-05,
      "loss": 0.0426,
      "step": 3220
    },
    {
      "epoch": 0.14748858447488583,
      "grad_norm": 0.6289660334587097,
      "learning_rate": 2.558219178082192e-05,
      "loss": 0.0353,
      "step": 3230
    },
    {
      "epoch": 0.14794520547945206,
      "grad_norm": 0.4862566590309143,
      "learning_rate": 2.556849315068493e-05,
      "loss": 0.0364,
      "step": 3240
    },
    {
      "epoch": 0.14840182648401826,
      "grad_norm": 0.7449494004249573,
      "learning_rate": 2.5554794520547948e-05,
      "loss": 0.0418,
      "step": 3250
    },
    {
      "epoch": 0.14885844748858448,
      "grad_norm": 0.7357194423675537,
      "learning_rate": 2.554109589041096e-05,
      "loss": 0.0304,
      "step": 3260
    },
    {
      "epoch": 0.14931506849315068,
      "grad_norm": 0.5407043099403381,
      "learning_rate": 2.5527397260273974e-05,
      "loss": 0.0313,
      "step": 3270
    },
    {
      "epoch": 0.14977168949771688,
      "grad_norm": 0.8308456540107727,
      "learning_rate": 2.5513698630136985e-05,
      "loss": 0.0362,
      "step": 3280
    },
    {
      "epoch": 0.1502283105022831,
      "grad_norm": 1.1206610202789307,
      "learning_rate": 2.55e-05,
      "loss": 0.0401,
      "step": 3290
    },
    {
      "epoch": 0.1506849315068493,
      "grad_norm": 1.0275245904922485,
      "learning_rate": 2.5486301369863014e-05,
      "loss": 0.0348,
      "step": 3300
    },
    {
      "epoch": 0.15114155251141553,
      "grad_norm": 1.0422905683517456,
      "learning_rate": 2.5472602739726026e-05,
      "loss": 0.0438,
      "step": 3310
    },
    {
      "epoch": 0.15159817351598173,
      "grad_norm": 1.1091190576553345,
      "learning_rate": 2.5458904109589044e-05,
      "loss": 0.0429,
      "step": 3320
    },
    {
      "epoch": 0.15205479452054796,
      "grad_norm": 0.49956318736076355,
      "learning_rate": 2.5445205479452055e-05,
      "loss": 0.0415,
      "step": 3330
    },
    {
      "epoch": 0.15251141552511416,
      "grad_norm": 0.9954918622970581,
      "learning_rate": 2.543150684931507e-05,
      "loss": 0.0421,
      "step": 3340
    },
    {
      "epoch": 0.15296803652968036,
      "grad_norm": 0.5215722322463989,
      "learning_rate": 2.5417808219178084e-05,
      "loss": 0.0361,
      "step": 3350
    },
    {
      "epoch": 0.15342465753424658,
      "grad_norm": 0.9962165951728821,
      "learning_rate": 2.5404109589041095e-05,
      "loss": 0.039,
      "step": 3360
    },
    {
      "epoch": 0.15388127853881278,
      "grad_norm": 1.048030138015747,
      "learning_rate": 2.539041095890411e-05,
      "loss": 0.042,
      "step": 3370
    },
    {
      "epoch": 0.154337899543379,
      "grad_norm": 0.5336843132972717,
      "learning_rate": 2.5376712328767124e-05,
      "loss": 0.03,
      "step": 3380
    },
    {
      "epoch": 0.1547945205479452,
      "grad_norm": 0.6755729913711548,
      "learning_rate": 2.536301369863014e-05,
      "loss": 0.025,
      "step": 3390
    },
    {
      "epoch": 0.1552511415525114,
      "grad_norm": 0.4896405339241028,
      "learning_rate": 2.5349315068493153e-05,
      "loss": 0.0243,
      "step": 3400
    },
    {
      "epoch": 0.15570776255707763,
      "grad_norm": 1.5788159370422363,
      "learning_rate": 2.5335616438356165e-05,
      "loss": 0.0335,
      "step": 3410
    },
    {
      "epoch": 0.15616438356164383,
      "grad_norm": 1.1182299852371216,
      "learning_rate": 2.532191780821918e-05,
      "loss": 0.034,
      "step": 3420
    },
    {
      "epoch": 0.15662100456621006,
      "grad_norm": 0.6921901702880859,
      "learning_rate": 2.530821917808219e-05,
      "loss": 0.0315,
      "step": 3430
    },
    {
      "epoch": 0.15707762557077626,
      "grad_norm": 0.7248780727386475,
      "learning_rate": 2.5294520547945205e-05,
      "loss": 0.0266,
      "step": 3440
    },
    {
      "epoch": 0.15753424657534246,
      "grad_norm": 1.0219860076904297,
      "learning_rate": 2.5280821917808223e-05,
      "loss": 0.0298,
      "step": 3450
    },
    {
      "epoch": 0.15799086757990868,
      "grad_norm": 0.5631651282310486,
      "learning_rate": 2.5267123287671234e-05,
      "loss": 0.0519,
      "step": 3460
    },
    {
      "epoch": 0.15844748858447488,
      "grad_norm": 0.7053313851356506,
      "learning_rate": 2.525342465753425e-05,
      "loss": 0.0361,
      "step": 3470
    },
    {
      "epoch": 0.1589041095890411,
      "grad_norm": 0.900187075138092,
      "learning_rate": 2.523972602739726e-05,
      "loss": 0.0298,
      "step": 3480
    },
    {
      "epoch": 0.1593607305936073,
      "grad_norm": 1.9382541179656982,
      "learning_rate": 2.5226027397260274e-05,
      "loss": 0.0246,
      "step": 3490
    },
    {
      "epoch": 0.1598173515981735,
      "grad_norm": 0.5351411700248718,
      "learning_rate": 2.5212328767123285e-05,
      "loss": 0.0358,
      "step": 3500
    },
    {
      "epoch": 0.16027397260273973,
      "grad_norm": 1.6781986951828003,
      "learning_rate": 2.51986301369863e-05,
      "loss": 0.0229,
      "step": 3510
    },
    {
      "epoch": 0.16073059360730593,
      "grad_norm": 1.5784335136413574,
      "learning_rate": 2.5184931506849318e-05,
      "loss": 0.0236,
      "step": 3520
    },
    {
      "epoch": 0.16118721461187216,
      "grad_norm": 0.7822949290275574,
      "learning_rate": 2.517123287671233e-05,
      "loss": 0.0344,
      "step": 3530
    },
    {
      "epoch": 0.16164383561643836,
      "grad_norm": 0.40546321868896484,
      "learning_rate": 2.5157534246575344e-05,
      "loss": 0.0315,
      "step": 3540
    },
    {
      "epoch": 0.16210045662100456,
      "grad_norm": 1.8234347105026245,
      "learning_rate": 2.5143835616438355e-05,
      "loss": 0.025,
      "step": 3550
    },
    {
      "epoch": 0.16255707762557078,
      "grad_norm": 0.9864658117294312,
      "learning_rate": 2.513013698630137e-05,
      "loss": 0.0374,
      "step": 3560
    },
    {
      "epoch": 0.16301369863013698,
      "grad_norm": 1.6709333658218384,
      "learning_rate": 2.5116438356164384e-05,
      "loss": 0.0364,
      "step": 3570
    },
    {
      "epoch": 0.1634703196347032,
      "grad_norm": 2.079585313796997,
      "learning_rate": 2.51027397260274e-05,
      "loss": 0.0277,
      "step": 3580
    },
    {
      "epoch": 0.1639269406392694,
      "grad_norm": 0.5558716654777527,
      "learning_rate": 2.5089041095890413e-05,
      "loss": 0.0375,
      "step": 3590
    },
    {
      "epoch": 0.1643835616438356,
      "grad_norm": 1.378700852394104,
      "learning_rate": 2.5075342465753424e-05,
      "loss": 0.0308,
      "step": 3600
    },
    {
      "epoch": 0.16484018264840183,
      "grad_norm": 0.9850267171859741,
      "learning_rate": 2.506164383561644e-05,
      "loss": 0.0294,
      "step": 3610
    },
    {
      "epoch": 0.16529680365296803,
      "grad_norm": 3.5690667629241943,
      "learning_rate": 2.5047945205479454e-05,
      "loss": 0.0354,
      "step": 3620
    },
    {
      "epoch": 0.16575342465753426,
      "grad_norm": 0.158338725566864,
      "learning_rate": 2.5034246575342465e-05,
      "loss": 0.0368,
      "step": 3630
    },
    {
      "epoch": 0.16621004566210046,
      "grad_norm": 0.9240610003471375,
      "learning_rate": 2.502054794520548e-05,
      "loss": 0.044,
      "step": 3640
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.2497165203094482,
      "learning_rate": 2.5006849315068494e-05,
      "loss": 0.0377,
      "step": 3650
    },
    {
      "epoch": 0.16712328767123288,
      "grad_norm": 2.5253376960754395,
      "learning_rate": 2.499315068493151e-05,
      "loss": 0.0432,
      "step": 3660
    },
    {
      "epoch": 0.16757990867579908,
      "grad_norm": 0.7514452338218689,
      "learning_rate": 2.4979452054794523e-05,
      "loss": 0.0255,
      "step": 3670
    },
    {
      "epoch": 0.1680365296803653,
      "grad_norm": 1.1064980030059814,
      "learning_rate": 2.4965753424657534e-05,
      "loss": 0.0363,
      "step": 3680
    },
    {
      "epoch": 0.1684931506849315,
      "grad_norm": 0.5057860016822815,
      "learning_rate": 2.495205479452055e-05,
      "loss": 0.0346,
      "step": 3690
    },
    {
      "epoch": 0.1689497716894977,
      "grad_norm": 0.6181474328041077,
      "learning_rate": 2.493835616438356e-05,
      "loss": 0.0313,
      "step": 3700
    },
    {
      "epoch": 0.16940639269406393,
      "grad_norm": 0.3887766897678375,
      "learning_rate": 2.4924657534246575e-05,
      "loss": 0.0308,
      "step": 3710
    },
    {
      "epoch": 0.16986301369863013,
      "grad_norm": 1.4041775465011597,
      "learning_rate": 2.4910958904109593e-05,
      "loss": 0.0276,
      "step": 3720
    },
    {
      "epoch": 0.17031963470319636,
      "grad_norm": 1.4702893495559692,
      "learning_rate": 2.4897260273972604e-05,
      "loss": 0.0378,
      "step": 3730
    },
    {
      "epoch": 0.17077625570776256,
      "grad_norm": 0.7868024110794067,
      "learning_rate": 2.4883561643835618e-05,
      "loss": 0.0299,
      "step": 3740
    },
    {
      "epoch": 0.17123287671232876,
      "grad_norm": 1.238337516784668,
      "learning_rate": 2.486986301369863e-05,
      "loss": 0.0329,
      "step": 3750
    },
    {
      "epoch": 0.17168949771689498,
      "grad_norm": 0.3783014714717865,
      "learning_rate": 2.4856164383561644e-05,
      "loss": 0.0376,
      "step": 3760
    },
    {
      "epoch": 0.17214611872146118,
      "grad_norm": 0.8823246955871582,
      "learning_rate": 2.4842465753424655e-05,
      "loss": 0.0406,
      "step": 3770
    },
    {
      "epoch": 0.1726027397260274,
      "grad_norm": 1.600422739982605,
      "learning_rate": 2.4828767123287673e-05,
      "loss": 0.0359,
      "step": 3780
    },
    {
      "epoch": 0.1730593607305936,
      "grad_norm": 1.0039427280426025,
      "learning_rate": 2.4815068493150688e-05,
      "loss": 0.0423,
      "step": 3790
    },
    {
      "epoch": 0.1735159817351598,
      "grad_norm": 0.874082624912262,
      "learning_rate": 2.48013698630137e-05,
      "loss": 0.0364,
      "step": 3800
    },
    {
      "epoch": 0.17397260273972603,
      "grad_norm": 0.7351372838020325,
      "learning_rate": 2.4787671232876714e-05,
      "loss": 0.0299,
      "step": 3810
    },
    {
      "epoch": 0.17442922374429223,
      "grad_norm": 1.4962894916534424,
      "learning_rate": 2.4773972602739725e-05,
      "loss": 0.0405,
      "step": 3820
    },
    {
      "epoch": 0.17488584474885846,
      "grad_norm": 0.38110095262527466,
      "learning_rate": 2.476027397260274e-05,
      "loss": 0.0434,
      "step": 3830
    },
    {
      "epoch": 0.17534246575342466,
      "grad_norm": 0.543721616268158,
      "learning_rate": 2.4746575342465754e-05,
      "loss": 0.0367,
      "step": 3840
    },
    {
      "epoch": 0.17579908675799086,
      "grad_norm": 0.8847785592079163,
      "learning_rate": 2.473287671232877e-05,
      "loss": 0.0217,
      "step": 3850
    },
    {
      "epoch": 0.17625570776255708,
      "grad_norm": 1.284273386001587,
      "learning_rate": 2.4719178082191783e-05,
      "loss": 0.0561,
      "step": 3860
    },
    {
      "epoch": 0.17671232876712328,
      "grad_norm": 0.5512844324111938,
      "learning_rate": 2.4705479452054794e-05,
      "loss": 0.0482,
      "step": 3870
    },
    {
      "epoch": 0.1771689497716895,
      "grad_norm": 1.6880401372909546,
      "learning_rate": 2.469178082191781e-05,
      "loss": 0.0359,
      "step": 3880
    },
    {
      "epoch": 0.1776255707762557,
      "grad_norm": 0.5542016625404358,
      "learning_rate": 2.4678082191780823e-05,
      "loss": 0.0316,
      "step": 3890
    },
    {
      "epoch": 0.1780821917808219,
      "grad_norm": 0.972967267036438,
      "learning_rate": 2.4664383561643835e-05,
      "loss": 0.0413,
      "step": 3900
    },
    {
      "epoch": 0.17853881278538813,
      "grad_norm": 0.5548372268676758,
      "learning_rate": 2.465068493150685e-05,
      "loss": 0.0317,
      "step": 3910
    },
    {
      "epoch": 0.17899543378995433,
      "grad_norm": 0.44635170698165894,
      "learning_rate": 2.4636986301369864e-05,
      "loss": 0.0353,
      "step": 3920
    },
    {
      "epoch": 0.17945205479452056,
      "grad_norm": 0.3428777754306793,
      "learning_rate": 2.4623287671232878e-05,
      "loss": 0.0228,
      "step": 3930
    },
    {
      "epoch": 0.17990867579908676,
      "grad_norm": 0.41042977571487427,
      "learning_rate": 2.4609589041095893e-05,
      "loss": 0.0257,
      "step": 3940
    },
    {
      "epoch": 0.18036529680365296,
      "grad_norm": 0.0881611630320549,
      "learning_rate": 2.4595890410958904e-05,
      "loss": 0.0241,
      "step": 3950
    },
    {
      "epoch": 0.18082191780821918,
      "grad_norm": 0.7508656978607178,
      "learning_rate": 2.458219178082192e-05,
      "loss": 0.0415,
      "step": 3960
    },
    {
      "epoch": 0.18127853881278538,
      "grad_norm": 1.0478227138519287,
      "learning_rate": 2.456849315068493e-05,
      "loss": 0.0371,
      "step": 3970
    },
    {
      "epoch": 0.1817351598173516,
      "grad_norm": 1.4118889570236206,
      "learning_rate": 2.4554794520547948e-05,
      "loss": 0.0454,
      "step": 3980
    },
    {
      "epoch": 0.1821917808219178,
      "grad_norm": 1.2829434871673584,
      "learning_rate": 2.4541095890410962e-05,
      "loss": 0.033,
      "step": 3990
    },
    {
      "epoch": 0.182648401826484,
      "grad_norm": 1.6807552576065063,
      "learning_rate": 2.4527397260273973e-05,
      "loss": 0.0326,
      "step": 4000
    },
    {
      "epoch": 0.18310502283105023,
      "grad_norm": 0.8111643195152283,
      "learning_rate": 2.4513698630136988e-05,
      "loss": 0.0333,
      "step": 4010
    },
    {
      "epoch": 0.18356164383561643,
      "grad_norm": 0.31033799052238464,
      "learning_rate": 2.45e-05,
      "loss": 0.026,
      "step": 4020
    },
    {
      "epoch": 0.18401826484018266,
      "grad_norm": 0.6299190521240234,
      "learning_rate": 2.4486301369863014e-05,
      "loss": 0.0453,
      "step": 4030
    },
    {
      "epoch": 0.18447488584474886,
      "grad_norm": 0.4172183871269226,
      "learning_rate": 2.4472602739726025e-05,
      "loss": 0.0341,
      "step": 4040
    },
    {
      "epoch": 0.18493150684931506,
      "grad_norm": 0.553379476070404,
      "learning_rate": 2.4458904109589043e-05,
      "loss": 0.0244,
      "step": 4050
    },
    {
      "epoch": 0.18538812785388128,
      "grad_norm": 0.6357022523880005,
      "learning_rate": 2.4445205479452057e-05,
      "loss": 0.0348,
      "step": 4060
    },
    {
      "epoch": 0.18584474885844748,
      "grad_norm": 0.7202559113502502,
      "learning_rate": 2.443150684931507e-05,
      "loss": 0.0401,
      "step": 4070
    },
    {
      "epoch": 0.1863013698630137,
      "grad_norm": 0.5657584071159363,
      "learning_rate": 2.4417808219178083e-05,
      "loss": 0.0283,
      "step": 4080
    },
    {
      "epoch": 0.1867579908675799,
      "grad_norm": 0.8743647336959839,
      "learning_rate": 2.4404109589041094e-05,
      "loss": 0.0331,
      "step": 4090
    },
    {
      "epoch": 0.1872146118721461,
      "grad_norm": 0.6969423294067383,
      "learning_rate": 2.439041095890411e-05,
      "loss": 0.0326,
      "step": 4100
    },
    {
      "epoch": 0.18767123287671234,
      "grad_norm": 1.1670994758605957,
      "learning_rate": 2.4376712328767124e-05,
      "loss": 0.0326,
      "step": 4110
    },
    {
      "epoch": 0.18812785388127853,
      "grad_norm": 1.1891875267028809,
      "learning_rate": 2.4363013698630138e-05,
      "loss": 0.0322,
      "step": 4120
    },
    {
      "epoch": 0.18858447488584476,
      "grad_norm": 0.6526486277580261,
      "learning_rate": 2.4349315068493153e-05,
      "loss": 0.0457,
      "step": 4130
    },
    {
      "epoch": 0.18904109589041096,
      "grad_norm": 1.3464165925979614,
      "learning_rate": 2.4335616438356164e-05,
      "loss": 0.0318,
      "step": 4140
    },
    {
      "epoch": 0.18949771689497716,
      "grad_norm": 1.407142996788025,
      "learning_rate": 2.432191780821918e-05,
      "loss": 0.0313,
      "step": 4150
    },
    {
      "epoch": 0.18995433789954339,
      "grad_norm": 0.7667511701583862,
      "learning_rate": 2.4308219178082193e-05,
      "loss": 0.0313,
      "step": 4160
    },
    {
      "epoch": 0.19041095890410958,
      "grad_norm": 0.6610613465309143,
      "learning_rate": 2.4294520547945204e-05,
      "loss": 0.0319,
      "step": 4170
    },
    {
      "epoch": 0.1908675799086758,
      "grad_norm": 1.620647668838501,
      "learning_rate": 2.4280821917808222e-05,
      "loss": 0.0392,
      "step": 4180
    },
    {
      "epoch": 0.191324200913242,
      "grad_norm": 1.339729905128479,
      "learning_rate": 2.4267123287671233e-05,
      "loss": 0.0372,
      "step": 4190
    },
    {
      "epoch": 0.1917808219178082,
      "grad_norm": 0.829640805721283,
      "learning_rate": 2.4253424657534248e-05,
      "loss": 0.0276,
      "step": 4200
    },
    {
      "epoch": 0.19223744292237444,
      "grad_norm": 0.9213610887527466,
      "learning_rate": 2.4239726027397263e-05,
      "loss": 0.0308,
      "step": 4210
    },
    {
      "epoch": 0.19269406392694063,
      "grad_norm": 0.8894879221916199,
      "learning_rate": 2.4226027397260274e-05,
      "loss": 0.0404,
      "step": 4220
    },
    {
      "epoch": 0.19315068493150686,
      "grad_norm": 0.6180270314216614,
      "learning_rate": 2.4212328767123288e-05,
      "loss": 0.0282,
      "step": 4230
    },
    {
      "epoch": 0.19360730593607306,
      "grad_norm": 1.404144048690796,
      "learning_rate": 2.41986301369863e-05,
      "loss": 0.0432,
      "step": 4240
    },
    {
      "epoch": 0.19406392694063926,
      "grad_norm": 9.327730178833008,
      "learning_rate": 2.4184931506849317e-05,
      "loss": 0.0275,
      "step": 4250
    },
    {
      "epoch": 0.19452054794520549,
      "grad_norm": 1.3551610708236694,
      "learning_rate": 2.417123287671233e-05,
      "loss": 0.0319,
      "step": 4260
    },
    {
      "epoch": 0.19497716894977168,
      "grad_norm": 0.6162335276603699,
      "learning_rate": 2.4157534246575343e-05,
      "loss": 0.0415,
      "step": 4270
    },
    {
      "epoch": 0.1954337899543379,
      "grad_norm": 0.7766938209533691,
      "learning_rate": 2.4143835616438358e-05,
      "loss": 0.0261,
      "step": 4280
    },
    {
      "epoch": 0.1958904109589041,
      "grad_norm": 0.3981949985027313,
      "learning_rate": 2.413013698630137e-05,
      "loss": 0.0307,
      "step": 4290
    },
    {
      "epoch": 0.1963470319634703,
      "grad_norm": 0.747062623500824,
      "learning_rate": 2.4116438356164384e-05,
      "loss": 0.0323,
      "step": 4300
    },
    {
      "epoch": 0.19680365296803654,
      "grad_norm": 0.5963582396507263,
      "learning_rate": 2.4102739726027395e-05,
      "loss": 0.0389,
      "step": 4310
    },
    {
      "epoch": 0.19726027397260273,
      "grad_norm": 0.6629471182823181,
      "learning_rate": 2.4089041095890413e-05,
      "loss": 0.0312,
      "step": 4320
    },
    {
      "epoch": 0.19771689497716896,
      "grad_norm": 0.9451082944869995,
      "learning_rate": 2.4075342465753427e-05,
      "loss": 0.0375,
      "step": 4330
    },
    {
      "epoch": 0.19817351598173516,
      "grad_norm": 0.633306086063385,
      "learning_rate": 2.406164383561644e-05,
      "loss": 0.0299,
      "step": 4340
    },
    {
      "epoch": 0.19863013698630136,
      "grad_norm": 0.13957560062408447,
      "learning_rate": 2.4047945205479453e-05,
      "loss": 0.03,
      "step": 4350
    },
    {
      "epoch": 0.19908675799086759,
      "grad_norm": 0.750643789768219,
      "learning_rate": 2.4034246575342464e-05,
      "loss": 0.0341,
      "step": 4360
    },
    {
      "epoch": 0.19954337899543378,
      "grad_norm": 1.3527355194091797,
      "learning_rate": 2.402054794520548e-05,
      "loss": 0.023,
      "step": 4370
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.6667296886444092,
      "learning_rate": 2.4006849315068497e-05,
      "loss": 0.029,
      "step": 4380
    },
    {
      "epoch": 0.2004566210045662,
      "grad_norm": 0.9308931231498718,
      "learning_rate": 2.3993150684931508e-05,
      "loss": 0.0451,
      "step": 4390
    },
    {
      "epoch": 0.2009132420091324,
      "grad_norm": 0.8331783413887024,
      "learning_rate": 2.3979452054794522e-05,
      "loss": 0.0242,
      "step": 4400
    },
    {
      "epoch": 0.20136986301369864,
      "grad_norm": 1.0441666841506958,
      "learning_rate": 2.3965753424657534e-05,
      "loss": 0.0305,
      "step": 4410
    },
    {
      "epoch": 0.20182648401826483,
      "grad_norm": 0.675937831401825,
      "learning_rate": 2.3952054794520548e-05,
      "loss": 0.0322,
      "step": 4420
    },
    {
      "epoch": 0.20228310502283106,
      "grad_norm": 0.4967113435268402,
      "learning_rate": 2.3938356164383563e-05,
      "loss": 0.0345,
      "step": 4430
    },
    {
      "epoch": 0.20273972602739726,
      "grad_norm": 0.7737391591072083,
      "learning_rate": 2.3924657534246574e-05,
      "loss": 0.0358,
      "step": 4440
    },
    {
      "epoch": 0.20319634703196346,
      "grad_norm": 0.6537953615188599,
      "learning_rate": 2.3910958904109592e-05,
      "loss": 0.0329,
      "step": 4450
    },
    {
      "epoch": 0.20365296803652969,
      "grad_norm": 1.6412070989608765,
      "learning_rate": 2.3897260273972603e-05,
      "loss": 0.0326,
      "step": 4460
    },
    {
      "epoch": 0.20410958904109588,
      "grad_norm": 0.46569547057151794,
      "learning_rate": 2.3883561643835618e-05,
      "loss": 0.029,
      "step": 4470
    },
    {
      "epoch": 0.2045662100456621,
      "grad_norm": 0.548104465007782,
      "learning_rate": 2.3869863013698632e-05,
      "loss": 0.0315,
      "step": 4480
    },
    {
      "epoch": 0.2050228310502283,
      "grad_norm": 0.5413289070129395,
      "learning_rate": 2.3856164383561643e-05,
      "loss": 0.0307,
      "step": 4490
    },
    {
      "epoch": 0.2054794520547945,
      "grad_norm": 0.4647771418094635,
      "learning_rate": 2.3842465753424658e-05,
      "loss": 0.0198,
      "step": 4500
    },
    {
      "epoch": 0.20593607305936074,
      "grad_norm": 0.9299076795578003,
      "learning_rate": 2.382876712328767e-05,
      "loss": 0.0394,
      "step": 4510
    },
    {
      "epoch": 0.20639269406392693,
      "grad_norm": 0.6678827404975891,
      "learning_rate": 2.3815068493150687e-05,
      "loss": 0.052,
      "step": 4520
    },
    {
      "epoch": 0.20684931506849316,
      "grad_norm": 0.6573423743247986,
      "learning_rate": 2.38013698630137e-05,
      "loss": 0.0352,
      "step": 4530
    },
    {
      "epoch": 0.20730593607305936,
      "grad_norm": 0.6077752709388733,
      "learning_rate": 2.3787671232876713e-05,
      "loss": 0.0249,
      "step": 4540
    },
    {
      "epoch": 0.20776255707762556,
      "grad_norm": 2.0902795791625977,
      "learning_rate": 2.3773972602739727e-05,
      "loss": 0.0325,
      "step": 4550
    },
    {
      "epoch": 0.20821917808219179,
      "grad_norm": 0.7285657525062561,
      "learning_rate": 2.376027397260274e-05,
      "loss": 0.0309,
      "step": 4560
    },
    {
      "epoch": 0.20867579908675798,
      "grad_norm": 0.5840703845024109,
      "learning_rate": 2.3746575342465753e-05,
      "loss": 0.0307,
      "step": 4570
    },
    {
      "epoch": 0.2091324200913242,
      "grad_norm": 1.0853625535964966,
      "learning_rate": 2.3732876712328768e-05,
      "loss": 0.0422,
      "step": 4580
    },
    {
      "epoch": 0.2095890410958904,
      "grad_norm": 0.5355898141860962,
      "learning_rate": 2.3719178082191782e-05,
      "loss": 0.0329,
      "step": 4590
    },
    {
      "epoch": 0.2100456621004566,
      "grad_norm": 0.7025665640830994,
      "learning_rate": 2.3705479452054797e-05,
      "loss": 0.028,
      "step": 4600
    },
    {
      "epoch": 0.21050228310502284,
      "grad_norm": 0.8943238854408264,
      "learning_rate": 2.3691780821917808e-05,
      "loss": 0.0375,
      "step": 4610
    },
    {
      "epoch": 0.21095890410958903,
      "grad_norm": 0.7940705418586731,
      "learning_rate": 2.3678082191780823e-05,
      "loss": 0.029,
      "step": 4620
    },
    {
      "epoch": 0.21141552511415526,
      "grad_norm": 1.006443977355957,
      "learning_rate": 2.3664383561643834e-05,
      "loss": 0.0376,
      "step": 4630
    },
    {
      "epoch": 0.21187214611872146,
      "grad_norm": 0.9073486328125,
      "learning_rate": 2.365068493150685e-05,
      "loss": 0.0343,
      "step": 4640
    },
    {
      "epoch": 0.21232876712328766,
      "grad_norm": 0.4870679974555969,
      "learning_rate": 2.3636986301369866e-05,
      "loss": 0.0385,
      "step": 4650
    },
    {
      "epoch": 0.21278538812785389,
      "grad_norm": 0.8583700060844421,
      "learning_rate": 2.3623287671232878e-05,
      "loss": 0.0279,
      "step": 4660
    },
    {
      "epoch": 0.21324200913242009,
      "grad_norm": 0.7296199798583984,
      "learning_rate": 2.3609589041095892e-05,
      "loss": 0.0196,
      "step": 4670
    },
    {
      "epoch": 0.2136986301369863,
      "grad_norm": 0.7856406569480896,
      "learning_rate": 2.3595890410958903e-05,
      "loss": 0.0278,
      "step": 4680
    },
    {
      "epoch": 0.2141552511415525,
      "grad_norm": 0.743751585483551,
      "learning_rate": 2.3582191780821918e-05,
      "loss": 0.0286,
      "step": 4690
    },
    {
      "epoch": 0.2146118721461187,
      "grad_norm": 1.2692010402679443,
      "learning_rate": 2.3568493150684933e-05,
      "loss": 0.0315,
      "step": 4700
    },
    {
      "epoch": 0.21506849315068494,
      "grad_norm": 0.9077544808387756,
      "learning_rate": 2.3554794520547944e-05,
      "loss": 0.0326,
      "step": 4710
    },
    {
      "epoch": 0.21552511415525114,
      "grad_norm": 1.116683840751648,
      "learning_rate": 2.354109589041096e-05,
      "loss": 0.0343,
      "step": 4720
    },
    {
      "epoch": 0.21598173515981736,
      "grad_norm": 0.8127245306968689,
      "learning_rate": 2.3527397260273973e-05,
      "loss": 0.0371,
      "step": 4730
    },
    {
      "epoch": 0.21643835616438356,
      "grad_norm": 1.5977134704589844,
      "learning_rate": 2.3513698630136987e-05,
      "loss": 0.0387,
      "step": 4740
    },
    {
      "epoch": 0.21689497716894976,
      "grad_norm": 1.2522629499435425,
      "learning_rate": 2.3500000000000002e-05,
      "loss": 0.025,
      "step": 4750
    },
    {
      "epoch": 0.217351598173516,
      "grad_norm": 3.0642802715301514,
      "learning_rate": 2.3486301369863013e-05,
      "loss": 0.0285,
      "step": 4760
    },
    {
      "epoch": 0.21780821917808219,
      "grad_norm": 0.9734972715377808,
      "learning_rate": 2.3472602739726028e-05,
      "loss": 0.034,
      "step": 4770
    },
    {
      "epoch": 0.2182648401826484,
      "grad_norm": 0.933704137802124,
      "learning_rate": 2.3458904109589042e-05,
      "loss": 0.0297,
      "step": 4780
    },
    {
      "epoch": 0.2187214611872146,
      "grad_norm": 0.711082398891449,
      "learning_rate": 2.3445205479452057e-05,
      "loss": 0.0319,
      "step": 4790
    },
    {
      "epoch": 0.2191780821917808,
      "grad_norm": 0.8723405003547668,
      "learning_rate": 2.3431506849315068e-05,
      "loss": 0.0505,
      "step": 4800
    },
    {
      "epoch": 0.21963470319634704,
      "grad_norm": 1.3877079486846924,
      "learning_rate": 2.3417808219178083e-05,
      "loss": 0.0357,
      "step": 4810
    },
    {
      "epoch": 0.22009132420091324,
      "grad_norm": 1.2378497123718262,
      "learning_rate": 2.3404109589041097e-05,
      "loss": 0.0406,
      "step": 4820
    },
    {
      "epoch": 0.22054794520547946,
      "grad_norm": 0.7561886310577393,
      "learning_rate": 2.339041095890411e-05,
      "loss": 0.0338,
      "step": 4830
    },
    {
      "epoch": 0.22100456621004566,
      "grad_norm": 0.48369550704956055,
      "learning_rate": 2.3376712328767123e-05,
      "loss": 0.0365,
      "step": 4840
    },
    {
      "epoch": 0.22146118721461186,
      "grad_norm": 0.3895626366138458,
      "learning_rate": 2.3363013698630138e-05,
      "loss": 0.0252,
      "step": 4850
    },
    {
      "epoch": 0.2219178082191781,
      "grad_norm": 0.9270831942558289,
      "learning_rate": 2.3349315068493152e-05,
      "loss": 0.0414,
      "step": 4860
    },
    {
      "epoch": 0.22237442922374429,
      "grad_norm": 0.4061135947704315,
      "learning_rate": 2.3335616438356167e-05,
      "loss": 0.0295,
      "step": 4870
    },
    {
      "epoch": 0.2228310502283105,
      "grad_norm": 0.7196735739707947,
      "learning_rate": 2.3321917808219178e-05,
      "loss": 0.0328,
      "step": 4880
    },
    {
      "epoch": 0.2232876712328767,
      "grad_norm": 1.2763373851776123,
      "learning_rate": 2.3308219178082192e-05,
      "loss": 0.0395,
      "step": 4890
    },
    {
      "epoch": 0.2237442922374429,
      "grad_norm": 0.7025047540664673,
      "learning_rate": 2.3294520547945204e-05,
      "loss": 0.0425,
      "step": 4900
    },
    {
      "epoch": 0.22420091324200914,
      "grad_norm": 0.36277273297309875,
      "learning_rate": 2.3280821917808218e-05,
      "loss": 0.0343,
      "step": 4910
    },
    {
      "epoch": 0.22465753424657534,
      "grad_norm": 0.8022110462188721,
      "learning_rate": 2.3267123287671236e-05,
      "loss": 0.0343,
      "step": 4920
    },
    {
      "epoch": 0.22511415525114156,
      "grad_norm": 2.106454610824585,
      "learning_rate": 2.3253424657534247e-05,
      "loss": 0.0422,
      "step": 4930
    },
    {
      "epoch": 0.22557077625570776,
      "grad_norm": 1.4652860164642334,
      "learning_rate": 2.3239726027397262e-05,
      "loss": 0.0352,
      "step": 4940
    },
    {
      "epoch": 0.22602739726027396,
      "grad_norm": 0.49777740240097046,
      "learning_rate": 2.3226027397260273e-05,
      "loss": 0.0268,
      "step": 4950
    },
    {
      "epoch": 0.2264840182648402,
      "grad_norm": 0.9699192643165588,
      "learning_rate": 2.3212328767123288e-05,
      "loss": 0.0599,
      "step": 4960
    },
    {
      "epoch": 0.22694063926940639,
      "grad_norm": 0.8094980716705322,
      "learning_rate": 2.3198630136986302e-05,
      "loss": 0.035,
      "step": 4970
    },
    {
      "epoch": 0.2273972602739726,
      "grad_norm": 0.9030733108520508,
      "learning_rate": 2.3184931506849317e-05,
      "loss": 0.0312,
      "step": 4980
    },
    {
      "epoch": 0.2278538812785388,
      "grad_norm": 1.0398287773132324,
      "learning_rate": 2.317123287671233e-05,
      "loss": 0.0419,
      "step": 4990
    },
    {
      "epoch": 0.228310502283105,
      "grad_norm": 1.006195306777954,
      "learning_rate": 2.3157534246575343e-05,
      "loss": 0.0328,
      "step": 5000
    },
    {
      "epoch": 0.22876712328767124,
      "grad_norm": 1.0912418365478516,
      "learning_rate": 2.3143835616438357e-05,
      "loss": 0.0561,
      "step": 5010
    },
    {
      "epoch": 0.22922374429223744,
      "grad_norm": 1.2443197965621948,
      "learning_rate": 2.3130136986301372e-05,
      "loss": 0.0493,
      "step": 5020
    },
    {
      "epoch": 0.22968036529680366,
      "grad_norm": 2.255955219268799,
      "learning_rate": 2.3116438356164383e-05,
      "loss": 0.0333,
      "step": 5030
    },
    {
      "epoch": 0.23013698630136986,
      "grad_norm": 0.6366379857063293,
      "learning_rate": 2.3102739726027398e-05,
      "loss": 0.0342,
      "step": 5040
    },
    {
      "epoch": 0.23059360730593606,
      "grad_norm": 1.8765954971313477,
      "learning_rate": 2.3089041095890412e-05,
      "loss": 0.0247,
      "step": 5050
    },
    {
      "epoch": 0.2310502283105023,
      "grad_norm": 0.4895287752151489,
      "learning_rate": 2.3075342465753427e-05,
      "loss": 0.0376,
      "step": 5060
    },
    {
      "epoch": 0.23150684931506849,
      "grad_norm": 0.5934706330299377,
      "learning_rate": 2.3061643835616438e-05,
      "loss": 0.0426,
      "step": 5070
    },
    {
      "epoch": 0.2319634703196347,
      "grad_norm": 0.7761096954345703,
      "learning_rate": 2.3047945205479452e-05,
      "loss": 0.0339,
      "step": 5080
    },
    {
      "epoch": 0.2324200913242009,
      "grad_norm": 0.46630436182022095,
      "learning_rate": 2.3034246575342467e-05,
      "loss": 0.0321,
      "step": 5090
    },
    {
      "epoch": 0.2328767123287671,
      "grad_norm": 0.6808513402938843,
      "learning_rate": 2.3020547945205478e-05,
      "loss": 0.0291,
      "step": 5100
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.2052695751190186,
      "learning_rate": 2.3006849315068493e-05,
      "loss": 0.0243,
      "step": 5110
    },
    {
      "epoch": 0.23378995433789954,
      "grad_norm": 2.536501407623291,
      "learning_rate": 2.2993150684931507e-05,
      "loss": 0.0453,
      "step": 5120
    },
    {
      "epoch": 0.23424657534246576,
      "grad_norm": 0.692805290222168,
      "learning_rate": 2.2979452054794522e-05,
      "loss": 0.0298,
      "step": 5130
    },
    {
      "epoch": 0.23470319634703196,
      "grad_norm": 0.8807207345962524,
      "learning_rate": 2.2965753424657536e-05,
      "loss": 0.0366,
      "step": 5140
    },
    {
      "epoch": 0.23515981735159816,
      "grad_norm": 1.7488822937011719,
      "learning_rate": 2.2952054794520548e-05,
      "loss": 0.0304,
      "step": 5150
    },
    {
      "epoch": 0.2356164383561644,
      "grad_norm": 0.6319167017936707,
      "learning_rate": 2.2938356164383562e-05,
      "loss": 0.0428,
      "step": 5160
    },
    {
      "epoch": 0.23607305936073059,
      "grad_norm": 0.37893661856651306,
      "learning_rate": 2.2924657534246573e-05,
      "loss": 0.0334,
      "step": 5170
    },
    {
      "epoch": 0.2365296803652968,
      "grad_norm": 0.8866802453994751,
      "learning_rate": 2.291095890410959e-05,
      "loss": 0.0323,
      "step": 5180
    },
    {
      "epoch": 0.236986301369863,
      "grad_norm": 1.1005014181137085,
      "learning_rate": 2.2897260273972606e-05,
      "loss": 0.0259,
      "step": 5190
    },
    {
      "epoch": 0.2374429223744292,
      "grad_norm": 0.8161753416061401,
      "learning_rate": 2.2883561643835617e-05,
      "loss": 0.0339,
      "step": 5200
    },
    {
      "epoch": 0.23789954337899544,
      "grad_norm": 0.9592489004135132,
      "learning_rate": 2.286986301369863e-05,
      "loss": 0.0448,
      "step": 5210
    },
    {
      "epoch": 0.23835616438356164,
      "grad_norm": 0.8419524431228638,
      "learning_rate": 2.2856164383561643e-05,
      "loss": 0.0502,
      "step": 5220
    },
    {
      "epoch": 0.23881278538812786,
      "grad_norm": 0.5605854392051697,
      "learning_rate": 2.2842465753424657e-05,
      "loss": 0.022,
      "step": 5230
    },
    {
      "epoch": 0.23926940639269406,
      "grad_norm": 0.6857159733772278,
      "learning_rate": 2.2828767123287672e-05,
      "loss": 0.0248,
      "step": 5240
    },
    {
      "epoch": 0.23972602739726026,
      "grad_norm": 0.40650442242622375,
      "learning_rate": 2.2815068493150687e-05,
      "loss": 0.0448,
      "step": 5250
    },
    {
      "epoch": 0.2401826484018265,
      "grad_norm": 1.0896962881088257,
      "learning_rate": 2.28013698630137e-05,
      "loss": 0.0435,
      "step": 5260
    },
    {
      "epoch": 0.2406392694063927,
      "grad_norm": 1.3710277080535889,
      "learning_rate": 2.2787671232876712e-05,
      "loss": 0.0446,
      "step": 5270
    },
    {
      "epoch": 0.2410958904109589,
      "grad_norm": 0.7591186761856079,
      "learning_rate": 2.2773972602739727e-05,
      "loss": 0.0261,
      "step": 5280
    },
    {
      "epoch": 0.2415525114155251,
      "grad_norm": 0.604155421257019,
      "learning_rate": 2.276027397260274e-05,
      "loss": 0.0349,
      "step": 5290
    },
    {
      "epoch": 0.2420091324200913,
      "grad_norm": 1.952311396598816,
      "learning_rate": 2.2746575342465753e-05,
      "loss": 0.037,
      "step": 5300
    },
    {
      "epoch": 0.24246575342465754,
      "grad_norm": 1.0769234895706177,
      "learning_rate": 2.2732876712328767e-05,
      "loss": 0.0314,
      "step": 5310
    },
    {
      "epoch": 0.24292237442922374,
      "grad_norm": 1.2512078285217285,
      "learning_rate": 2.2719178082191782e-05,
      "loss": 0.0341,
      "step": 5320
    },
    {
      "epoch": 0.24337899543378996,
      "grad_norm": 0.7077733874320984,
      "learning_rate": 2.2705479452054796e-05,
      "loss": 0.0411,
      "step": 5330
    },
    {
      "epoch": 0.24383561643835616,
      "grad_norm": 0.6260880827903748,
      "learning_rate": 2.2691780821917808e-05,
      "loss": 0.0434,
      "step": 5340
    },
    {
      "epoch": 0.24429223744292236,
      "grad_norm": 0.3324491083621979,
      "learning_rate": 2.2678082191780822e-05,
      "loss": 0.0311,
      "step": 5350
    },
    {
      "epoch": 0.2447488584474886,
      "grad_norm": 0.5384647250175476,
      "learning_rate": 2.2664383561643837e-05,
      "loss": 0.0312,
      "step": 5360
    },
    {
      "epoch": 0.2452054794520548,
      "grad_norm": 0.540949285030365,
      "learning_rate": 2.2650684931506848e-05,
      "loss": 0.0249,
      "step": 5370
    },
    {
      "epoch": 0.245662100456621,
      "grad_norm": 0.7683880925178528,
      "learning_rate": 2.2636986301369866e-05,
      "loss": 0.0375,
      "step": 5380
    },
    {
      "epoch": 0.2461187214611872,
      "grad_norm": 0.613694965839386,
      "learning_rate": 2.2623287671232877e-05,
      "loss": 0.0292,
      "step": 5390
    },
    {
      "epoch": 0.2465753424657534,
      "grad_norm": 0.7399060726165771,
      "learning_rate": 2.260958904109589e-05,
      "loss": 0.0332,
      "step": 5400
    },
    {
      "epoch": 0.24703196347031964,
      "grad_norm": 0.46178171038627625,
      "learning_rate": 2.2595890410958906e-05,
      "loss": 0.0233,
      "step": 5410
    },
    {
      "epoch": 0.24748858447488584,
      "grad_norm": 0.7484195232391357,
      "learning_rate": 2.2582191780821917e-05,
      "loss": 0.04,
      "step": 5420
    },
    {
      "epoch": 0.24794520547945206,
      "grad_norm": 0.4022121727466583,
      "learning_rate": 2.2568493150684932e-05,
      "loss": 0.0397,
      "step": 5430
    },
    {
      "epoch": 0.24840182648401826,
      "grad_norm": 0.8818802833557129,
      "learning_rate": 2.2554794520547943e-05,
      "loss": 0.0361,
      "step": 5440
    },
    {
      "epoch": 0.24885844748858446,
      "grad_norm": 0.674987256526947,
      "learning_rate": 2.254109589041096e-05,
      "loss": 0.0353,
      "step": 5450
    },
    {
      "epoch": 0.2493150684931507,
      "grad_norm": 0.24012957513332367,
      "learning_rate": 2.2527397260273976e-05,
      "loss": 0.0204,
      "step": 5460
    },
    {
      "epoch": 0.2497716894977169,
      "grad_norm": 0.6013044118881226,
      "learning_rate": 2.2513698630136987e-05,
      "loss": 0.0278,
      "step": 5470
    },
    {
      "epoch": 0.2502283105022831,
      "grad_norm": 0.6452556252479553,
      "learning_rate": 2.25e-05,
      "loss": 0.0276,
      "step": 5480
    },
    {
      "epoch": 0.25068493150684934,
      "grad_norm": 0.9082561731338501,
      "learning_rate": 2.2486301369863013e-05,
      "loss": 0.0235,
      "step": 5490
    },
    {
      "epoch": 0.2511415525114155,
      "grad_norm": 1.9690730571746826,
      "learning_rate": 2.2472602739726027e-05,
      "loss": 0.0288,
      "step": 5500
    },
    {
      "epoch": 0.25159817351598174,
      "grad_norm": 0.7619344592094421,
      "learning_rate": 2.2458904109589042e-05,
      "loss": 0.0309,
      "step": 5510
    },
    {
      "epoch": 0.25205479452054796,
      "grad_norm": 1.4584274291992188,
      "learning_rate": 2.2445205479452056e-05,
      "loss": 0.0295,
      "step": 5520
    },
    {
      "epoch": 0.25251141552511414,
      "grad_norm": 0.9649802446365356,
      "learning_rate": 2.243150684931507e-05,
      "loss": 0.0337,
      "step": 5530
    },
    {
      "epoch": 0.25296803652968036,
      "grad_norm": 0.5481763482093811,
      "learning_rate": 2.2417808219178082e-05,
      "loss": 0.0244,
      "step": 5540
    },
    {
      "epoch": 0.2534246575342466,
      "grad_norm": 1.073616623878479,
      "learning_rate": 2.2404109589041097e-05,
      "loss": 0.0341,
      "step": 5550
    },
    {
      "epoch": 0.25388127853881276,
      "grad_norm": 0.8111613988876343,
      "learning_rate": 2.2390410958904108e-05,
      "loss": 0.0436,
      "step": 5560
    },
    {
      "epoch": 0.254337899543379,
      "grad_norm": 0.7576157450675964,
      "learning_rate": 2.2376712328767122e-05,
      "loss": 0.0407,
      "step": 5570
    },
    {
      "epoch": 0.2547945205479452,
      "grad_norm": 0.676199197769165,
      "learning_rate": 2.2363013698630137e-05,
      "loss": 0.0355,
      "step": 5580
    },
    {
      "epoch": 0.25525114155251144,
      "grad_norm": 0.9544485807418823,
      "learning_rate": 2.234931506849315e-05,
      "loss": 0.0269,
      "step": 5590
    },
    {
      "epoch": 0.2557077625570776,
      "grad_norm": 0.5633203983306885,
      "learning_rate": 2.2335616438356166e-05,
      "loss": 0.0222,
      "step": 5600
    },
    {
      "epoch": 0.25616438356164384,
      "grad_norm": 1.0700143575668335,
      "learning_rate": 2.2321917808219177e-05,
      "loss": 0.0421,
      "step": 5610
    },
    {
      "epoch": 0.25662100456621006,
      "grad_norm": 0.9098392724990845,
      "learning_rate": 2.2308219178082192e-05,
      "loss": 0.0309,
      "step": 5620
    },
    {
      "epoch": 0.25707762557077624,
      "grad_norm": 2.287128210067749,
      "learning_rate": 2.2294520547945206e-05,
      "loss": 0.0434,
      "step": 5630
    },
    {
      "epoch": 0.25753424657534246,
      "grad_norm": 0.5930560231208801,
      "learning_rate": 2.2280821917808218e-05,
      "loss": 0.0292,
      "step": 5640
    },
    {
      "epoch": 0.2579908675799087,
      "grad_norm": 0.7914512753486633,
      "learning_rate": 2.2267123287671236e-05,
      "loss": 0.0332,
      "step": 5650
    },
    {
      "epoch": 0.25844748858447486,
      "grad_norm": 0.686682403087616,
      "learning_rate": 2.2253424657534247e-05,
      "loss": 0.0266,
      "step": 5660
    },
    {
      "epoch": 0.2589041095890411,
      "grad_norm": 0.501487672328949,
      "learning_rate": 2.223972602739726e-05,
      "loss": 0.036,
      "step": 5670
    },
    {
      "epoch": 0.2593607305936073,
      "grad_norm": 0.47052785754203796,
      "learning_rate": 2.2226027397260276e-05,
      "loss": 0.0307,
      "step": 5680
    },
    {
      "epoch": 0.25981735159817354,
      "grad_norm": 1.3018935918807983,
      "learning_rate": 2.2212328767123287e-05,
      "loss": 0.0369,
      "step": 5690
    },
    {
      "epoch": 0.2602739726027397,
      "grad_norm": 0.6364440321922302,
      "learning_rate": 2.21986301369863e-05,
      "loss": 0.0332,
      "step": 5700
    },
    {
      "epoch": 0.26073059360730594,
      "grad_norm": 0.13376551866531372,
      "learning_rate": 2.2184931506849313e-05,
      "loss": 0.0326,
      "step": 5710
    },
    {
      "epoch": 0.26118721461187216,
      "grad_norm": 0.8580445647239685,
      "learning_rate": 2.217123287671233e-05,
      "loss": 0.0347,
      "step": 5720
    },
    {
      "epoch": 0.26164383561643834,
      "grad_norm": 0.9500945210456848,
      "learning_rate": 2.2157534246575345e-05,
      "loss": 0.0366,
      "step": 5730
    },
    {
      "epoch": 0.26210045662100456,
      "grad_norm": 1.1477092504501343,
      "learning_rate": 2.2143835616438357e-05,
      "loss": 0.0384,
      "step": 5740
    },
    {
      "epoch": 0.2625570776255708,
      "grad_norm": 0.675316572189331,
      "learning_rate": 2.213013698630137e-05,
      "loss": 0.0266,
      "step": 5750
    },
    {
      "epoch": 0.26301369863013696,
      "grad_norm": 0.4342006742954254,
      "learning_rate": 2.2116438356164382e-05,
      "loss": 0.0261,
      "step": 5760
    },
    {
      "epoch": 0.2634703196347032,
      "grad_norm": 0.7533217072486877,
      "learning_rate": 2.2102739726027397e-05,
      "loss": 0.032,
      "step": 5770
    },
    {
      "epoch": 0.2639269406392694,
      "grad_norm": 0.911954939365387,
      "learning_rate": 2.208904109589041e-05,
      "loss": 0.0282,
      "step": 5780
    },
    {
      "epoch": 0.26438356164383564,
      "grad_norm": 0.6250646114349365,
      "learning_rate": 2.2075342465753426e-05,
      "loss": 0.035,
      "step": 5790
    },
    {
      "epoch": 0.2648401826484018,
      "grad_norm": 0.4520471692085266,
      "learning_rate": 2.206164383561644e-05,
      "loss": 0.025,
      "step": 5800
    },
    {
      "epoch": 0.26529680365296804,
      "grad_norm": 1.0626471042633057,
      "learning_rate": 2.2047945205479452e-05,
      "loss": 0.0284,
      "step": 5810
    },
    {
      "epoch": 0.26575342465753427,
      "grad_norm": 0.7215867042541504,
      "learning_rate": 2.2034246575342466e-05,
      "loss": 0.0448,
      "step": 5820
    },
    {
      "epoch": 0.26621004566210044,
      "grad_norm": 0.607090175151825,
      "learning_rate": 2.2020547945205478e-05,
      "loss": 0.0256,
      "step": 5830
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.365330696105957,
      "learning_rate": 2.2006849315068492e-05,
      "loss": 0.0326,
      "step": 5840
    },
    {
      "epoch": 0.2671232876712329,
      "grad_norm": 0.9489838480949402,
      "learning_rate": 2.199315068493151e-05,
      "loss": 0.0294,
      "step": 5850
    },
    {
      "epoch": 0.26757990867579906,
      "grad_norm": 0.8292926549911499,
      "learning_rate": 2.197945205479452e-05,
      "loss": 0.0276,
      "step": 5860
    },
    {
      "epoch": 0.2680365296803653,
      "grad_norm": 2.2139039039611816,
      "learning_rate": 2.1965753424657536e-05,
      "loss": 0.0305,
      "step": 5870
    },
    {
      "epoch": 0.2684931506849315,
      "grad_norm": 1.0810519456863403,
      "learning_rate": 2.1952054794520547e-05,
      "loss": 0.0252,
      "step": 5880
    },
    {
      "epoch": 0.26894977168949774,
      "grad_norm": 0.7416351437568665,
      "learning_rate": 2.193835616438356e-05,
      "loss": 0.0296,
      "step": 5890
    },
    {
      "epoch": 0.2694063926940639,
      "grad_norm": 0.6741458773612976,
      "learning_rate": 2.1924657534246576e-05,
      "loss": 0.0241,
      "step": 5900
    },
    {
      "epoch": 0.26986301369863014,
      "grad_norm": 0.864983856678009,
      "learning_rate": 2.1910958904109587e-05,
      "loss": 0.0351,
      "step": 5910
    },
    {
      "epoch": 0.27031963470319637,
      "grad_norm": 0.835616946220398,
      "learning_rate": 2.1897260273972605e-05,
      "loss": 0.0338,
      "step": 5920
    },
    {
      "epoch": 0.27077625570776254,
      "grad_norm": 0.6645146608352661,
      "learning_rate": 2.1883561643835617e-05,
      "loss": 0.0242,
      "step": 5930
    },
    {
      "epoch": 0.27123287671232876,
      "grad_norm": 1.140195369720459,
      "learning_rate": 2.186986301369863e-05,
      "loss": 0.0241,
      "step": 5940
    },
    {
      "epoch": 0.271689497716895,
      "grad_norm": 1.5414811372756958,
      "learning_rate": 2.1856164383561646e-05,
      "loss": 0.044,
      "step": 5950
    },
    {
      "epoch": 0.27214611872146116,
      "grad_norm": 0.6702741980552673,
      "learning_rate": 2.1842465753424657e-05,
      "loss": 0.0223,
      "step": 5960
    },
    {
      "epoch": 0.2726027397260274,
      "grad_norm": 1.4803911447525024,
      "learning_rate": 2.182876712328767e-05,
      "loss": 0.0442,
      "step": 5970
    },
    {
      "epoch": 0.2730593607305936,
      "grad_norm": 0.9324589371681213,
      "learning_rate": 2.1815068493150683e-05,
      "loss": 0.0237,
      "step": 5980
    },
    {
      "epoch": 0.27351598173515984,
      "grad_norm": 2.3761026859283447,
      "learning_rate": 2.18013698630137e-05,
      "loss": 0.0361,
      "step": 5990
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 1.0869444608688354,
      "learning_rate": 2.1787671232876715e-05,
      "loss": 0.0282,
      "step": 6000
    },
    {
      "epoch": 0.27442922374429224,
      "grad_norm": 0.8268925547599792,
      "learning_rate": 2.1773972602739726e-05,
      "loss": 0.0204,
      "step": 6010
    },
    {
      "epoch": 0.27488584474885847,
      "grad_norm": 0.7832085490226746,
      "learning_rate": 2.176027397260274e-05,
      "loss": 0.0314,
      "step": 6020
    },
    {
      "epoch": 0.27534246575342464,
      "grad_norm": 0.5974799990653992,
      "learning_rate": 2.1746575342465752e-05,
      "loss": 0.0361,
      "step": 6030
    },
    {
      "epoch": 0.27579908675799086,
      "grad_norm": 1.5729669332504272,
      "learning_rate": 2.1732876712328767e-05,
      "loss": 0.035,
      "step": 6040
    },
    {
      "epoch": 0.2762557077625571,
      "grad_norm": 0.5502580404281616,
      "learning_rate": 2.171917808219178e-05,
      "loss": 0.0308,
      "step": 6050
    },
    {
      "epoch": 0.27671232876712326,
      "grad_norm": 1.4291455745697021,
      "learning_rate": 2.1705479452054796e-05,
      "loss": 0.0283,
      "step": 6060
    },
    {
      "epoch": 0.2771689497716895,
      "grad_norm": 0.39559245109558105,
      "learning_rate": 2.169178082191781e-05,
      "loss": 0.0417,
      "step": 6070
    },
    {
      "epoch": 0.2776255707762557,
      "grad_norm": 0.31728580594062805,
      "learning_rate": 2.167808219178082e-05,
      "loss": 0.0226,
      "step": 6080
    },
    {
      "epoch": 0.27808219178082194,
      "grad_norm": 0.9508021473884583,
      "learning_rate": 2.1664383561643836e-05,
      "loss": 0.036,
      "step": 6090
    },
    {
      "epoch": 0.2785388127853881,
      "grad_norm": 0.8445401191711426,
      "learning_rate": 2.1650684931506847e-05,
      "loss": 0.0339,
      "step": 6100
    },
    {
      "epoch": 0.27899543378995434,
      "grad_norm": 1.072583556175232,
      "learning_rate": 2.1636986301369862e-05,
      "loss": 0.0323,
      "step": 6110
    },
    {
      "epoch": 0.27945205479452057,
      "grad_norm": 0.3979804813861847,
      "learning_rate": 2.162328767123288e-05,
      "loss": 0.0381,
      "step": 6120
    },
    {
      "epoch": 0.27990867579908674,
      "grad_norm": 0.6360875368118286,
      "learning_rate": 2.160958904109589e-05,
      "loss": 0.0298,
      "step": 6130
    },
    {
      "epoch": 0.28036529680365296,
      "grad_norm": 0.29748645424842834,
      "learning_rate": 2.1595890410958906e-05,
      "loss": 0.0271,
      "step": 6140
    },
    {
      "epoch": 0.2808219178082192,
      "grad_norm": 0.6395272612571716,
      "learning_rate": 2.1582191780821917e-05,
      "loss": 0.0293,
      "step": 6150
    },
    {
      "epoch": 0.28127853881278536,
      "grad_norm": 1.7751232385635376,
      "learning_rate": 2.156849315068493e-05,
      "loss": 0.0366,
      "step": 6160
    },
    {
      "epoch": 0.2817351598173516,
      "grad_norm": 0.5284055471420288,
      "learning_rate": 2.1554794520547946e-05,
      "loss": 0.0282,
      "step": 6170
    },
    {
      "epoch": 0.2821917808219178,
      "grad_norm": 0.5962764024734497,
      "learning_rate": 2.1541095890410957e-05,
      "loss": 0.0235,
      "step": 6180
    },
    {
      "epoch": 0.28264840182648404,
      "grad_norm": 0.7326750159263611,
      "learning_rate": 2.1527397260273975e-05,
      "loss": 0.0256,
      "step": 6190
    },
    {
      "epoch": 0.2831050228310502,
      "grad_norm": 2.295853614807129,
      "learning_rate": 2.1513698630136986e-05,
      "loss": 0.0393,
      "step": 6200
    },
    {
      "epoch": 0.28356164383561644,
      "grad_norm": 0.5409324169158936,
      "learning_rate": 2.15e-05,
      "loss": 0.0268,
      "step": 6210
    },
    {
      "epoch": 0.28401826484018267,
      "grad_norm": 1.6212620735168457,
      "learning_rate": 2.1486301369863015e-05,
      "loss": 0.0305,
      "step": 6220
    },
    {
      "epoch": 0.28447488584474884,
      "grad_norm": 0.7282208800315857,
      "learning_rate": 2.1472602739726027e-05,
      "loss": 0.0365,
      "step": 6230
    },
    {
      "epoch": 0.28493150684931506,
      "grad_norm": 0.743918776512146,
      "learning_rate": 2.145890410958904e-05,
      "loss": 0.0248,
      "step": 6240
    },
    {
      "epoch": 0.2853881278538813,
      "grad_norm": 0.7054497003555298,
      "learning_rate": 2.1445205479452056e-05,
      "loss": 0.0343,
      "step": 6250
    },
    {
      "epoch": 0.28584474885844746,
      "grad_norm": 0.8532518744468689,
      "learning_rate": 2.143150684931507e-05,
      "loss": 0.039,
      "step": 6260
    },
    {
      "epoch": 0.2863013698630137,
      "grad_norm": 0.5579772591590881,
      "learning_rate": 2.1417808219178085e-05,
      "loss": 0.0316,
      "step": 6270
    },
    {
      "epoch": 0.2867579908675799,
      "grad_norm": 0.8441351056098938,
      "learning_rate": 2.1404109589041096e-05,
      "loss": 0.0346,
      "step": 6280
    },
    {
      "epoch": 0.28721461187214614,
      "grad_norm": 4.475932598114014,
      "learning_rate": 2.139041095890411e-05,
      "loss": 0.0341,
      "step": 6290
    },
    {
      "epoch": 0.2876712328767123,
      "grad_norm": 1.0653116703033447,
      "learning_rate": 2.1376712328767122e-05,
      "loss": 0.0369,
      "step": 6300
    },
    {
      "epoch": 0.28812785388127854,
      "grad_norm": 0.5868424773216248,
      "learning_rate": 2.1363013698630136e-05,
      "loss": 0.0289,
      "step": 6310
    },
    {
      "epoch": 0.28858447488584477,
      "grad_norm": 0.9060064554214478,
      "learning_rate": 2.134931506849315e-05,
      "loss": 0.0328,
      "step": 6320
    },
    {
      "epoch": 0.28904109589041094,
      "grad_norm": 0.6566736102104187,
      "learning_rate": 2.1335616438356166e-05,
      "loss": 0.048,
      "step": 6330
    },
    {
      "epoch": 0.28949771689497716,
      "grad_norm": 0.7316800951957703,
      "learning_rate": 2.132191780821918e-05,
      "loss": 0.04,
      "step": 6340
    },
    {
      "epoch": 0.2899543378995434,
      "grad_norm": 0.3026822805404663,
      "learning_rate": 2.130821917808219e-05,
      "loss": 0.0314,
      "step": 6350
    },
    {
      "epoch": 0.29041095890410956,
      "grad_norm": 1.0473177433013916,
      "learning_rate": 2.1294520547945206e-05,
      "loss": 0.0333,
      "step": 6360
    },
    {
      "epoch": 0.2908675799086758,
      "grad_norm": 0.6504053473472595,
      "learning_rate": 2.1280821917808217e-05,
      "loss": 0.0224,
      "step": 6370
    },
    {
      "epoch": 0.291324200913242,
      "grad_norm": 0.293219655752182,
      "learning_rate": 2.126712328767123e-05,
      "loss": 0.028,
      "step": 6380
    },
    {
      "epoch": 0.29178082191780824,
      "grad_norm": 0.7960292100906372,
      "learning_rate": 2.125342465753425e-05,
      "loss": 0.0272,
      "step": 6390
    },
    {
      "epoch": 0.2922374429223744,
      "grad_norm": 0.42912429571151733,
      "learning_rate": 2.123972602739726e-05,
      "loss": 0.0338,
      "step": 6400
    },
    {
      "epoch": 0.29269406392694064,
      "grad_norm": 1.9020938873291016,
      "learning_rate": 2.1226027397260275e-05,
      "loss": 0.031,
      "step": 6410
    },
    {
      "epoch": 0.29315068493150687,
      "grad_norm": 1.6779239177703857,
      "learning_rate": 2.1212328767123287e-05,
      "loss": 0.0392,
      "step": 6420
    },
    {
      "epoch": 0.29360730593607304,
      "grad_norm": 0.5300507545471191,
      "learning_rate": 2.11986301369863e-05,
      "loss": 0.0345,
      "step": 6430
    },
    {
      "epoch": 0.29406392694063926,
      "grad_norm": 0.7468091249465942,
      "learning_rate": 2.1184931506849316e-05,
      "loss": 0.0334,
      "step": 6440
    },
    {
      "epoch": 0.2945205479452055,
      "grad_norm": 0.6416505575180054,
      "learning_rate": 2.117123287671233e-05,
      "loss": 0.0351,
      "step": 6450
    },
    {
      "epoch": 0.29497716894977166,
      "grad_norm": 0.82135009765625,
      "learning_rate": 2.1157534246575345e-05,
      "loss": 0.0345,
      "step": 6460
    },
    {
      "epoch": 0.2954337899543379,
      "grad_norm": 0.5238663554191589,
      "learning_rate": 2.1143835616438356e-05,
      "loss": 0.0244,
      "step": 6470
    },
    {
      "epoch": 0.2958904109589041,
      "grad_norm": 2.6200356483459473,
      "learning_rate": 2.113013698630137e-05,
      "loss": 0.0352,
      "step": 6480
    },
    {
      "epoch": 0.29634703196347034,
      "grad_norm": 0.7687937021255493,
      "learning_rate": 2.1116438356164385e-05,
      "loss": 0.0328,
      "step": 6490
    },
    {
      "epoch": 0.2968036529680365,
      "grad_norm": 0.6835467219352722,
      "learning_rate": 2.1102739726027396e-05,
      "loss": 0.0279,
      "step": 6500
    },
    {
      "epoch": 0.29726027397260274,
      "grad_norm": 0.8476779460906982,
      "learning_rate": 2.108904109589041e-05,
      "loss": 0.0382,
      "step": 6510
    },
    {
      "epoch": 0.29771689497716897,
      "grad_norm": 0.8402462601661682,
      "learning_rate": 2.1075342465753425e-05,
      "loss": 0.0357,
      "step": 6520
    },
    {
      "epoch": 0.29817351598173514,
      "grad_norm": 0.8257458209991455,
      "learning_rate": 2.106164383561644e-05,
      "loss": 0.0295,
      "step": 6530
    },
    {
      "epoch": 0.29863013698630136,
      "grad_norm": 0.8976971507072449,
      "learning_rate": 2.1047945205479455e-05,
      "loss": 0.0271,
      "step": 6540
    },
    {
      "epoch": 0.2990867579908676,
      "grad_norm": 0.4046984612941742,
      "learning_rate": 2.1034246575342466e-05,
      "loss": 0.0365,
      "step": 6550
    },
    {
      "epoch": 0.29954337899543376,
      "grad_norm": 1.3314876556396484,
      "learning_rate": 2.102054794520548e-05,
      "loss": 0.0294,
      "step": 6560
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.6186569929122925,
      "learning_rate": 2.100684931506849e-05,
      "loss": 0.03,
      "step": 6570
    },
    {
      "epoch": 0.3004566210045662,
      "grad_norm": 0.6228629946708679,
      "learning_rate": 2.0993150684931506e-05,
      "loss": 0.0238,
      "step": 6580
    },
    {
      "epoch": 0.30091324200913244,
      "grad_norm": 0.318204790353775,
      "learning_rate": 2.097945205479452e-05,
      "loss": 0.0324,
      "step": 6590
    },
    {
      "epoch": 0.3013698630136986,
      "grad_norm": 0.9674505591392517,
      "learning_rate": 2.0965753424657535e-05,
      "loss": 0.0279,
      "step": 6600
    },
    {
      "epoch": 0.30182648401826484,
      "grad_norm": 0.3608120381832123,
      "learning_rate": 2.095205479452055e-05,
      "loss": 0.0347,
      "step": 6610
    },
    {
      "epoch": 0.30228310502283107,
      "grad_norm": 0.31553328037261963,
      "learning_rate": 2.093835616438356e-05,
      "loss": 0.0285,
      "step": 6620
    },
    {
      "epoch": 0.30273972602739724,
      "grad_norm": 0.7132875919342041,
      "learning_rate": 2.0924657534246576e-05,
      "loss": 0.028,
      "step": 6630
    },
    {
      "epoch": 0.30319634703196346,
      "grad_norm": 0.8014639019966125,
      "learning_rate": 2.0910958904109587e-05,
      "loss": 0.0368,
      "step": 6640
    },
    {
      "epoch": 0.3036529680365297,
      "grad_norm": 1.7580068111419678,
      "learning_rate": 2.0897260273972605e-05,
      "loss": 0.0256,
      "step": 6650
    },
    {
      "epoch": 0.3041095890410959,
      "grad_norm": 1.3236565589904785,
      "learning_rate": 2.088356164383562e-05,
      "loss": 0.0296,
      "step": 6660
    },
    {
      "epoch": 0.3045662100456621,
      "grad_norm": 2.6965091228485107,
      "learning_rate": 2.086986301369863e-05,
      "loss": 0.0239,
      "step": 6670
    },
    {
      "epoch": 0.3050228310502283,
      "grad_norm": 2.2570366859436035,
      "learning_rate": 2.0856164383561645e-05,
      "loss": 0.0251,
      "step": 6680
    },
    {
      "epoch": 0.30547945205479454,
      "grad_norm": 0.7146481871604919,
      "learning_rate": 2.0842465753424656e-05,
      "loss": 0.0376,
      "step": 6690
    },
    {
      "epoch": 0.3059360730593607,
      "grad_norm": 0.6112844944000244,
      "learning_rate": 2.082876712328767e-05,
      "loss": 0.0253,
      "step": 6700
    },
    {
      "epoch": 0.30639269406392694,
      "grad_norm": 1.1126227378845215,
      "learning_rate": 2.0815068493150685e-05,
      "loss": 0.03,
      "step": 6710
    },
    {
      "epoch": 0.30684931506849317,
      "grad_norm": 0.4340590536594391,
      "learning_rate": 2.08013698630137e-05,
      "loss": 0.029,
      "step": 6720
    },
    {
      "epoch": 0.30730593607305934,
      "grad_norm": 0.6840541362762451,
      "learning_rate": 2.0787671232876715e-05,
      "loss": 0.0289,
      "step": 6730
    },
    {
      "epoch": 0.30776255707762556,
      "grad_norm": 1.605913758277893,
      "learning_rate": 2.0773972602739726e-05,
      "loss": 0.0307,
      "step": 6740
    },
    {
      "epoch": 0.3082191780821918,
      "grad_norm": 1.0941390991210938,
      "learning_rate": 2.076027397260274e-05,
      "loss": 0.023,
      "step": 6750
    },
    {
      "epoch": 0.308675799086758,
      "grad_norm": 0.26580408215522766,
      "learning_rate": 2.0746575342465755e-05,
      "loss": 0.0369,
      "step": 6760
    },
    {
      "epoch": 0.3091324200913242,
      "grad_norm": 1.5235579013824463,
      "learning_rate": 2.0732876712328766e-05,
      "loss": 0.0433,
      "step": 6770
    },
    {
      "epoch": 0.3095890410958904,
      "grad_norm": 1.7088145017623901,
      "learning_rate": 2.071917808219178e-05,
      "loss": 0.0327,
      "step": 6780
    },
    {
      "epoch": 0.31004566210045664,
      "grad_norm": 0.4961756765842438,
      "learning_rate": 2.0705479452054795e-05,
      "loss": 0.0316,
      "step": 6790
    },
    {
      "epoch": 0.3105022831050228,
      "grad_norm": 0.586467981338501,
      "learning_rate": 2.069178082191781e-05,
      "loss": 0.0334,
      "step": 6800
    },
    {
      "epoch": 0.31095890410958904,
      "grad_norm": 0.5260792970657349,
      "learning_rate": 2.0678082191780824e-05,
      "loss": 0.0429,
      "step": 6810
    },
    {
      "epoch": 0.31141552511415527,
      "grad_norm": 0.5937899947166443,
      "learning_rate": 2.0664383561643836e-05,
      "loss": 0.0363,
      "step": 6820
    },
    {
      "epoch": 0.31187214611872144,
      "grad_norm": 0.5323060750961304,
      "learning_rate": 2.065068493150685e-05,
      "loss": 0.0334,
      "step": 6830
    },
    {
      "epoch": 0.31232876712328766,
      "grad_norm": 1.0742017030715942,
      "learning_rate": 2.063698630136986e-05,
      "loss": 0.0316,
      "step": 6840
    },
    {
      "epoch": 0.3127853881278539,
      "grad_norm": 1.4093440771102905,
      "learning_rate": 2.062328767123288e-05,
      "loss": 0.0323,
      "step": 6850
    },
    {
      "epoch": 0.3132420091324201,
      "grad_norm": 0.569385290145874,
      "learning_rate": 2.060958904109589e-05,
      "loss": 0.0338,
      "step": 6860
    },
    {
      "epoch": 0.3136986301369863,
      "grad_norm": 0.9252224564552307,
      "learning_rate": 2.0595890410958905e-05,
      "loss": 0.027,
      "step": 6870
    },
    {
      "epoch": 0.3141552511415525,
      "grad_norm": 1.1434369087219238,
      "learning_rate": 2.058219178082192e-05,
      "loss": 0.0416,
      "step": 6880
    },
    {
      "epoch": 0.31461187214611874,
      "grad_norm": 1.15923011302948,
      "learning_rate": 2.056849315068493e-05,
      "loss": 0.0367,
      "step": 6890
    },
    {
      "epoch": 0.3150684931506849,
      "grad_norm": 0.34550967812538147,
      "learning_rate": 2.0554794520547945e-05,
      "loss": 0.0287,
      "step": 6900
    },
    {
      "epoch": 0.31552511415525114,
      "grad_norm": 0.7497113943099976,
      "learning_rate": 2.0541095890410957e-05,
      "loss": 0.0275,
      "step": 6910
    },
    {
      "epoch": 0.31598173515981737,
      "grad_norm": 0.383348286151886,
      "learning_rate": 2.0527397260273974e-05,
      "loss": 0.0232,
      "step": 6920
    },
    {
      "epoch": 0.31643835616438354,
      "grad_norm": 1.0420732498168945,
      "learning_rate": 2.051369863013699e-05,
      "loss": 0.0301,
      "step": 6930
    },
    {
      "epoch": 0.31689497716894977,
      "grad_norm": 0.3018048405647278,
      "learning_rate": 2.05e-05,
      "loss": 0.0323,
      "step": 6940
    },
    {
      "epoch": 0.317351598173516,
      "grad_norm": 1.1019808053970337,
      "learning_rate": 2.0486301369863015e-05,
      "loss": 0.0239,
      "step": 6950
    },
    {
      "epoch": 0.3178082191780822,
      "grad_norm": 0.05886993557214737,
      "learning_rate": 2.0472602739726026e-05,
      "loss": 0.0287,
      "step": 6960
    },
    {
      "epoch": 0.3182648401826484,
      "grad_norm": 0.801937460899353,
      "learning_rate": 2.045890410958904e-05,
      "loss": 0.0303,
      "step": 6970
    },
    {
      "epoch": 0.3187214611872146,
      "grad_norm": 0.5226731896400452,
      "learning_rate": 2.0445205479452055e-05,
      "loss": 0.0366,
      "step": 6980
    },
    {
      "epoch": 0.31917808219178084,
      "grad_norm": 0.42769336700439453,
      "learning_rate": 2.043150684931507e-05,
      "loss": 0.033,
      "step": 6990
    },
    {
      "epoch": 0.319634703196347,
      "grad_norm": 1.3235752582550049,
      "learning_rate": 2.0417808219178084e-05,
      "loss": 0.0325,
      "step": 7000
    },
    {
      "epoch": 0.32009132420091324,
      "grad_norm": 0.7022873759269714,
      "learning_rate": 2.0404109589041095e-05,
      "loss": 0.0406,
      "step": 7010
    },
    {
      "epoch": 0.32054794520547947,
      "grad_norm": 0.9627124071121216,
      "learning_rate": 2.039041095890411e-05,
      "loss": 0.0448,
      "step": 7020
    },
    {
      "epoch": 0.32100456621004564,
      "grad_norm": 0.5607812404632568,
      "learning_rate": 2.0376712328767125e-05,
      "loss": 0.0304,
      "step": 7030
    },
    {
      "epoch": 0.32146118721461187,
      "grad_norm": 0.5428342819213867,
      "learning_rate": 2.0363013698630136e-05,
      "loss": 0.025,
      "step": 7040
    },
    {
      "epoch": 0.3219178082191781,
      "grad_norm": 0.748930811882019,
      "learning_rate": 2.0349315068493154e-05,
      "loss": 0.0305,
      "step": 7050
    },
    {
      "epoch": 0.3223744292237443,
      "grad_norm": 2.7110540866851807,
      "learning_rate": 2.0335616438356165e-05,
      "loss": 0.0318,
      "step": 7060
    },
    {
      "epoch": 0.3228310502283105,
      "grad_norm": 0.8730856776237488,
      "learning_rate": 2.032191780821918e-05,
      "loss": 0.0331,
      "step": 7070
    },
    {
      "epoch": 0.3232876712328767,
      "grad_norm": 0.8909680247306824,
      "learning_rate": 2.0308219178082194e-05,
      "loss": 0.0277,
      "step": 7080
    },
    {
      "epoch": 0.32374429223744294,
      "grad_norm": 1.3235329389572144,
      "learning_rate": 2.0294520547945205e-05,
      "loss": 0.0314,
      "step": 7090
    },
    {
      "epoch": 0.3242009132420091,
      "grad_norm": 0.7841416597366333,
      "learning_rate": 2.028082191780822e-05,
      "loss": 0.045,
      "step": 7100
    },
    {
      "epoch": 0.32465753424657534,
      "grad_norm": 0.5608004927635193,
      "learning_rate": 2.026712328767123e-05,
      "loss": 0.0277,
      "step": 7110
    },
    {
      "epoch": 0.32511415525114157,
      "grad_norm": 0.7818133234977722,
      "learning_rate": 2.025342465753425e-05,
      "loss": 0.0303,
      "step": 7120
    },
    {
      "epoch": 0.32557077625570774,
      "grad_norm": 0.503495454788208,
      "learning_rate": 2.023972602739726e-05,
      "loss": 0.0237,
      "step": 7130
    },
    {
      "epoch": 0.32602739726027397,
      "grad_norm": 0.6694138050079346,
      "learning_rate": 2.0226027397260275e-05,
      "loss": 0.0331,
      "step": 7140
    },
    {
      "epoch": 0.3264840182648402,
      "grad_norm": 0.9298840165138245,
      "learning_rate": 2.021232876712329e-05,
      "loss": 0.0306,
      "step": 7150
    },
    {
      "epoch": 0.3269406392694064,
      "grad_norm": 0.8767754435539246,
      "learning_rate": 2.01986301369863e-05,
      "loss": 0.0326,
      "step": 7160
    },
    {
      "epoch": 0.3273972602739726,
      "grad_norm": 0.9167017340660095,
      "learning_rate": 2.0184931506849315e-05,
      "loss": 0.0306,
      "step": 7170
    },
    {
      "epoch": 0.3278538812785388,
      "grad_norm": 0.648317277431488,
      "learning_rate": 2.0171232876712326e-05,
      "loss": 0.0342,
      "step": 7180
    },
    {
      "epoch": 0.32831050228310504,
      "grad_norm": 0.9958868622779846,
      "learning_rate": 2.0157534246575344e-05,
      "loss": 0.0282,
      "step": 7190
    },
    {
      "epoch": 0.3287671232876712,
      "grad_norm": 0.5732278227806091,
      "learning_rate": 2.014383561643836e-05,
      "loss": 0.0324,
      "step": 7200
    },
    {
      "epoch": 0.32922374429223744,
      "grad_norm": 0.4111344516277313,
      "learning_rate": 2.013013698630137e-05,
      "loss": 0.0274,
      "step": 7210
    },
    {
      "epoch": 0.32968036529680367,
      "grad_norm": 0.5248451232910156,
      "learning_rate": 2.0116438356164385e-05,
      "loss": 0.0304,
      "step": 7220
    },
    {
      "epoch": 0.33013698630136984,
      "grad_norm": 1.1085225343704224,
      "learning_rate": 2.0102739726027396e-05,
      "loss": 0.0268,
      "step": 7230
    },
    {
      "epoch": 0.33059360730593607,
      "grad_norm": 0.4748759865760803,
      "learning_rate": 2.008904109589041e-05,
      "loss": 0.0253,
      "step": 7240
    },
    {
      "epoch": 0.3310502283105023,
      "grad_norm": 0.6939767003059387,
      "learning_rate": 2.0075342465753428e-05,
      "loss": 0.0288,
      "step": 7250
    },
    {
      "epoch": 0.3315068493150685,
      "grad_norm": 1.4457757472991943,
      "learning_rate": 2.006164383561644e-05,
      "loss": 0.0343,
      "step": 7260
    },
    {
      "epoch": 0.3319634703196347,
      "grad_norm": 0.8113288283348083,
      "learning_rate": 2.0047945205479454e-05,
      "loss": 0.0382,
      "step": 7270
    },
    {
      "epoch": 0.3324200913242009,
      "grad_norm": 1.8923739194869995,
      "learning_rate": 2.0034246575342465e-05,
      "loss": 0.0387,
      "step": 7280
    },
    {
      "epoch": 0.33287671232876714,
      "grad_norm": 0.9921340346336365,
      "learning_rate": 2.002054794520548e-05,
      "loss": 0.0389,
      "step": 7290
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.1453008651733398,
      "learning_rate": 2.0006849315068494e-05,
      "loss": 0.0316,
      "step": 7300
    },
    {
      "epoch": 0.33378995433789954,
      "grad_norm": 2.926654100418091,
      "learning_rate": 1.9993150684931506e-05,
      "loss": 0.0334,
      "step": 7310
    },
    {
      "epoch": 0.33424657534246577,
      "grad_norm": 0.28653261065483093,
      "learning_rate": 1.9979452054794523e-05,
      "loss": 0.0294,
      "step": 7320
    },
    {
      "epoch": 0.33470319634703194,
      "grad_norm": 0.3087242841720581,
      "learning_rate": 1.9965753424657535e-05,
      "loss": 0.0235,
      "step": 7330
    },
    {
      "epoch": 0.33515981735159817,
      "grad_norm": 4.904961585998535,
      "learning_rate": 1.995205479452055e-05,
      "loss": 0.0398,
      "step": 7340
    },
    {
      "epoch": 0.3356164383561644,
      "grad_norm": 1.4360620975494385,
      "learning_rate": 1.993835616438356e-05,
      "loss": 0.0409,
      "step": 7350
    },
    {
      "epoch": 0.3360730593607306,
      "grad_norm": 0.9316932559013367,
      "learning_rate": 1.9924657534246575e-05,
      "loss": 0.0286,
      "step": 7360
    },
    {
      "epoch": 0.3365296803652968,
      "grad_norm": 0.7479730248451233,
      "learning_rate": 1.991095890410959e-05,
      "loss": 0.0298,
      "step": 7370
    },
    {
      "epoch": 0.336986301369863,
      "grad_norm": 0.4701128304004669,
      "learning_rate": 1.98972602739726e-05,
      "loss": 0.0216,
      "step": 7380
    },
    {
      "epoch": 0.33744292237442924,
      "grad_norm": 0.5419796705245972,
      "learning_rate": 1.988356164383562e-05,
      "loss": 0.0371,
      "step": 7390
    },
    {
      "epoch": 0.3378995433789954,
      "grad_norm": 1.1134333610534668,
      "learning_rate": 1.986986301369863e-05,
      "loss": 0.042,
      "step": 7400
    },
    {
      "epoch": 0.33835616438356164,
      "grad_norm": 0.8483400940895081,
      "learning_rate": 1.9856164383561644e-05,
      "loss": 0.0231,
      "step": 7410
    },
    {
      "epoch": 0.33881278538812787,
      "grad_norm": 1.2938956022262573,
      "learning_rate": 1.984246575342466e-05,
      "loss": 0.0277,
      "step": 7420
    },
    {
      "epoch": 0.33926940639269404,
      "grad_norm": 0.7983950972557068,
      "learning_rate": 1.982876712328767e-05,
      "loss": 0.0227,
      "step": 7430
    },
    {
      "epoch": 0.33972602739726027,
      "grad_norm": 0.9293715357780457,
      "learning_rate": 1.9815068493150685e-05,
      "loss": 0.0344,
      "step": 7440
    },
    {
      "epoch": 0.3401826484018265,
      "grad_norm": 0.43367886543273926,
      "learning_rate": 1.98013698630137e-05,
      "loss": 0.0245,
      "step": 7450
    },
    {
      "epoch": 0.3406392694063927,
      "grad_norm": 0.5302857160568237,
      "learning_rate": 1.9787671232876714e-05,
      "loss": 0.0409,
      "step": 7460
    },
    {
      "epoch": 0.3410958904109589,
      "grad_norm": 1.0061149597167969,
      "learning_rate": 1.977397260273973e-05,
      "loss": 0.0404,
      "step": 7470
    },
    {
      "epoch": 0.3415525114155251,
      "grad_norm": 0.8067498803138733,
      "learning_rate": 1.976027397260274e-05,
      "loss": 0.0374,
      "step": 7480
    },
    {
      "epoch": 0.34200913242009134,
      "grad_norm": 0.8344900012016296,
      "learning_rate": 1.9746575342465754e-05,
      "loss": 0.025,
      "step": 7490
    },
    {
      "epoch": 0.3424657534246575,
      "grad_norm": 0.9959998726844788,
      "learning_rate": 1.9732876712328765e-05,
      "loss": 0.0268,
      "step": 7500
    },
    {
      "epoch": 0.34292237442922374,
      "grad_norm": 0.5202383995056152,
      "learning_rate": 1.971917808219178e-05,
      "loss": 0.045,
      "step": 7510
    },
    {
      "epoch": 0.34337899543378997,
      "grad_norm": 0.5208657383918762,
      "learning_rate": 1.9705479452054798e-05,
      "loss": 0.0214,
      "step": 7520
    },
    {
      "epoch": 0.34383561643835614,
      "grad_norm": 0.9223235845565796,
      "learning_rate": 1.969178082191781e-05,
      "loss": 0.03,
      "step": 7530
    },
    {
      "epoch": 0.34429223744292237,
      "grad_norm": 0.7838751077651978,
      "learning_rate": 1.9678082191780824e-05,
      "loss": 0.035,
      "step": 7540
    },
    {
      "epoch": 0.3447488584474886,
      "grad_norm": 0.4899643361568451,
      "learning_rate": 1.9664383561643835e-05,
      "loss": 0.029,
      "step": 7550
    },
    {
      "epoch": 0.3452054794520548,
      "grad_norm": 0.6607058048248291,
      "learning_rate": 1.965068493150685e-05,
      "loss": 0.04,
      "step": 7560
    },
    {
      "epoch": 0.345662100456621,
      "grad_norm": 0.784255862236023,
      "learning_rate": 1.9636986301369864e-05,
      "loss": 0.0275,
      "step": 7570
    },
    {
      "epoch": 0.3461187214611872,
      "grad_norm": 1.0292681455612183,
      "learning_rate": 1.9623287671232875e-05,
      "loss": 0.0359,
      "step": 7580
    },
    {
      "epoch": 0.34657534246575344,
      "grad_norm": 1.6808589696884155,
      "learning_rate": 1.9609589041095893e-05,
      "loss": 0.0434,
      "step": 7590
    },
    {
      "epoch": 0.3470319634703196,
      "grad_norm": 2.222062587738037,
      "learning_rate": 1.9595890410958904e-05,
      "loss": 0.0367,
      "step": 7600
    },
    {
      "epoch": 0.34748858447488584,
      "grad_norm": 3.02150559425354,
      "learning_rate": 1.958219178082192e-05,
      "loss": 0.0266,
      "step": 7610
    },
    {
      "epoch": 0.34794520547945207,
      "grad_norm": 0.4970158338546753,
      "learning_rate": 1.956849315068493e-05,
      "loss": 0.0241,
      "step": 7620
    },
    {
      "epoch": 0.34840182648401824,
      "grad_norm": 0.9159604907035828,
      "learning_rate": 1.9554794520547945e-05,
      "loss": 0.0258,
      "step": 7630
    },
    {
      "epoch": 0.34885844748858447,
      "grad_norm": 0.48370298743247986,
      "learning_rate": 1.954109589041096e-05,
      "loss": 0.0322,
      "step": 7640
    },
    {
      "epoch": 0.3493150684931507,
      "grad_norm": 0.9572984576225281,
      "learning_rate": 1.9527397260273974e-05,
      "loss": 0.0456,
      "step": 7650
    },
    {
      "epoch": 0.3497716894977169,
      "grad_norm": 0.7117444276809692,
      "learning_rate": 1.951369863013699e-05,
      "loss": 0.0272,
      "step": 7660
    },
    {
      "epoch": 0.3502283105022831,
      "grad_norm": 1.5598505735397339,
      "learning_rate": 1.95e-05,
      "loss": 0.0329,
      "step": 7670
    },
    {
      "epoch": 0.3506849315068493,
      "grad_norm": 0.47206395864486694,
      "learning_rate": 1.9486301369863014e-05,
      "loss": 0.0318,
      "step": 7680
    },
    {
      "epoch": 0.35114155251141554,
      "grad_norm": 0.4981687366962433,
      "learning_rate": 1.947260273972603e-05,
      "loss": 0.0348,
      "step": 7690
    },
    {
      "epoch": 0.3515981735159817,
      "grad_norm": 1.0161967277526855,
      "learning_rate": 1.945890410958904e-05,
      "loss": 0.0236,
      "step": 7700
    },
    {
      "epoch": 0.35205479452054794,
      "grad_norm": 1.0317027568817139,
      "learning_rate": 1.9445205479452055e-05,
      "loss": 0.0287,
      "step": 7710
    },
    {
      "epoch": 0.35251141552511417,
      "grad_norm": 0.6338157653808594,
      "learning_rate": 1.943150684931507e-05,
      "loss": 0.0497,
      "step": 7720
    },
    {
      "epoch": 0.35296803652968034,
      "grad_norm": 0.7860185503959656,
      "learning_rate": 1.9417808219178084e-05,
      "loss": 0.0249,
      "step": 7730
    },
    {
      "epoch": 0.35342465753424657,
      "grad_norm": 0.45061197876930237,
      "learning_rate": 1.9404109589041098e-05,
      "loss": 0.0227,
      "step": 7740
    },
    {
      "epoch": 0.3538812785388128,
      "grad_norm": 0.649206280708313,
      "learning_rate": 1.939041095890411e-05,
      "loss": 0.0359,
      "step": 7750
    },
    {
      "epoch": 0.354337899543379,
      "grad_norm": 0.7396815419197083,
      "learning_rate": 1.9376712328767124e-05,
      "loss": 0.0268,
      "step": 7760
    },
    {
      "epoch": 0.3547945205479452,
      "grad_norm": 0.8868834972381592,
      "learning_rate": 1.9363013698630135e-05,
      "loss": 0.0413,
      "step": 7770
    },
    {
      "epoch": 0.3552511415525114,
      "grad_norm": 0.8633404970169067,
      "learning_rate": 1.934931506849315e-05,
      "loss": 0.0317,
      "step": 7780
    },
    {
      "epoch": 0.35570776255707764,
      "grad_norm": 0.7118576169013977,
      "learning_rate": 1.9335616438356168e-05,
      "loss": 0.0301,
      "step": 7790
    },
    {
      "epoch": 0.3561643835616438,
      "grad_norm": 0.9766123294830322,
      "learning_rate": 1.932191780821918e-05,
      "loss": 0.0356,
      "step": 7800
    },
    {
      "epoch": 0.35662100456621004,
      "grad_norm": 0.47247517108917236,
      "learning_rate": 1.9308219178082193e-05,
      "loss": 0.0278,
      "step": 7810
    },
    {
      "epoch": 0.35707762557077627,
      "grad_norm": 0.3952285051345825,
      "learning_rate": 1.9294520547945205e-05,
      "loss": 0.028,
      "step": 7820
    },
    {
      "epoch": 0.35753424657534244,
      "grad_norm": 0.9332624673843384,
      "learning_rate": 1.928082191780822e-05,
      "loss": 0.0364,
      "step": 7830
    },
    {
      "epoch": 0.35799086757990867,
      "grad_norm": 0.8730196952819824,
      "learning_rate": 1.9267123287671234e-05,
      "loss": 0.0308,
      "step": 7840
    },
    {
      "epoch": 0.3584474885844749,
      "grad_norm": 0.7180289626121521,
      "learning_rate": 1.9253424657534245e-05,
      "loss": 0.0338,
      "step": 7850
    },
    {
      "epoch": 0.3589041095890411,
      "grad_norm": 1.0304689407348633,
      "learning_rate": 1.9239726027397263e-05,
      "loss": 0.0305,
      "step": 7860
    },
    {
      "epoch": 0.3593607305936073,
      "grad_norm": 1.3047287464141846,
      "learning_rate": 1.9226027397260274e-05,
      "loss": 0.0277,
      "step": 7870
    },
    {
      "epoch": 0.3598173515981735,
      "grad_norm": 0.6806463599205017,
      "learning_rate": 1.921232876712329e-05,
      "loss": 0.0334,
      "step": 7880
    },
    {
      "epoch": 0.36027397260273974,
      "grad_norm": 2.4644010066986084,
      "learning_rate": 1.91986301369863e-05,
      "loss": 0.037,
      "step": 7890
    },
    {
      "epoch": 0.3607305936073059,
      "grad_norm": 0.805317759513855,
      "learning_rate": 1.9184931506849314e-05,
      "loss": 0.0323,
      "step": 7900
    },
    {
      "epoch": 0.36118721461187214,
      "grad_norm": 0.841823399066925,
      "learning_rate": 1.917123287671233e-05,
      "loss": 0.0276,
      "step": 7910
    },
    {
      "epoch": 0.36164383561643837,
      "grad_norm": 0.20581938326358795,
      "learning_rate": 1.9157534246575344e-05,
      "loss": 0.0217,
      "step": 7920
    },
    {
      "epoch": 0.36210045662100454,
      "grad_norm": 1.3514776229858398,
      "learning_rate": 1.9143835616438358e-05,
      "loss": 0.0231,
      "step": 7930
    },
    {
      "epoch": 0.36255707762557077,
      "grad_norm": 0.33940985798835754,
      "learning_rate": 1.913013698630137e-05,
      "loss": 0.0311,
      "step": 7940
    },
    {
      "epoch": 0.363013698630137,
      "grad_norm": 0.5032796263694763,
      "learning_rate": 1.9116438356164384e-05,
      "loss": 0.0246,
      "step": 7950
    },
    {
      "epoch": 0.3634703196347032,
      "grad_norm": 1.7096508741378784,
      "learning_rate": 1.91027397260274e-05,
      "loss": 0.0302,
      "step": 7960
    },
    {
      "epoch": 0.3639269406392694,
      "grad_norm": 0.282193660736084,
      "learning_rate": 1.908904109589041e-05,
      "loss": 0.0259,
      "step": 7970
    },
    {
      "epoch": 0.3643835616438356,
      "grad_norm": 0.7260923385620117,
      "learning_rate": 1.9075342465753424e-05,
      "loss": 0.0305,
      "step": 7980
    },
    {
      "epoch": 0.36484018264840185,
      "grad_norm": 0.6341601610183716,
      "learning_rate": 1.906164383561644e-05,
      "loss": 0.0279,
      "step": 7990
    },
    {
      "epoch": 0.365296803652968,
      "grad_norm": 0.6107823252677917,
      "learning_rate": 1.9047945205479453e-05,
      "loss": 0.035,
      "step": 8000
    },
    {
      "epoch": 0.36575342465753424,
      "grad_norm": 0.19998666644096375,
      "learning_rate": 1.9034246575342468e-05,
      "loss": 0.0326,
      "step": 8010
    },
    {
      "epoch": 0.36621004566210047,
      "grad_norm": 0.7176399827003479,
      "learning_rate": 1.902054794520548e-05,
      "loss": 0.0272,
      "step": 8020
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 1.2108128070831299,
      "learning_rate": 1.9006849315068494e-05,
      "loss": 0.0312,
      "step": 8030
    },
    {
      "epoch": 0.36712328767123287,
      "grad_norm": 0.6698301434516907,
      "learning_rate": 1.8993150684931505e-05,
      "loss": 0.0314,
      "step": 8040
    },
    {
      "epoch": 0.3675799086757991,
      "grad_norm": 0.9852925539016724,
      "learning_rate": 1.897945205479452e-05,
      "loss": 0.0316,
      "step": 8050
    },
    {
      "epoch": 0.3680365296803653,
      "grad_norm": 0.8935205340385437,
      "learning_rate": 1.8965753424657537e-05,
      "loss": 0.0301,
      "step": 8060
    },
    {
      "epoch": 0.3684931506849315,
      "grad_norm": 1.7280192375183105,
      "learning_rate": 1.895205479452055e-05,
      "loss": 0.0307,
      "step": 8070
    },
    {
      "epoch": 0.3689497716894977,
      "grad_norm": 0.815977156162262,
      "learning_rate": 1.8938356164383563e-05,
      "loss": 0.03,
      "step": 8080
    },
    {
      "epoch": 0.36940639269406395,
      "grad_norm": 0.8731340765953064,
      "learning_rate": 1.8924657534246574e-05,
      "loss": 0.0259,
      "step": 8090
    },
    {
      "epoch": 0.3698630136986301,
      "grad_norm": 0.9733182191848755,
      "learning_rate": 1.891095890410959e-05,
      "loss": 0.0311,
      "step": 8100
    },
    {
      "epoch": 0.37031963470319634,
      "grad_norm": 0.5741811394691467,
      "learning_rate": 1.8897260273972604e-05,
      "loss": 0.0281,
      "step": 8110
    },
    {
      "epoch": 0.37077625570776257,
      "grad_norm": 1.07158362865448,
      "learning_rate": 1.8883561643835618e-05,
      "loss": 0.0239,
      "step": 8120
    },
    {
      "epoch": 0.37123287671232874,
      "grad_norm": 0.32175832986831665,
      "learning_rate": 1.8869863013698633e-05,
      "loss": 0.0242,
      "step": 8130
    },
    {
      "epoch": 0.37168949771689497,
      "grad_norm": 0.4548457860946655,
      "learning_rate": 1.8856164383561644e-05,
      "loss": 0.0271,
      "step": 8140
    },
    {
      "epoch": 0.3721461187214612,
      "grad_norm": 0.4425073564052582,
      "learning_rate": 1.884246575342466e-05,
      "loss": 0.0243,
      "step": 8150
    },
    {
      "epoch": 0.3726027397260274,
      "grad_norm": 0.8203202486038208,
      "learning_rate": 1.882876712328767e-05,
      "loss": 0.0196,
      "step": 8160
    },
    {
      "epoch": 0.3730593607305936,
      "grad_norm": 0.3218481242656708,
      "learning_rate": 1.8815068493150684e-05,
      "loss": 0.0168,
      "step": 8170
    },
    {
      "epoch": 0.3735159817351598,
      "grad_norm": 1.041033148765564,
      "learning_rate": 1.88013698630137e-05,
      "loss": 0.0295,
      "step": 8180
    },
    {
      "epoch": 0.37397260273972605,
      "grad_norm": 0.256535142660141,
      "learning_rate": 1.8787671232876713e-05,
      "loss": 0.0235,
      "step": 8190
    },
    {
      "epoch": 0.3744292237442922,
      "grad_norm": 0.5951950550079346,
      "learning_rate": 1.8773972602739728e-05,
      "loss": 0.0295,
      "step": 8200
    },
    {
      "epoch": 0.37488584474885844,
      "grad_norm": 0.542578935623169,
      "learning_rate": 1.876027397260274e-05,
      "loss": 0.0269,
      "step": 8210
    },
    {
      "epoch": 0.37534246575342467,
      "grad_norm": 1.7955994606018066,
      "learning_rate": 1.8746575342465754e-05,
      "loss": 0.0248,
      "step": 8220
    },
    {
      "epoch": 0.37579908675799084,
      "grad_norm": 0.6382858753204346,
      "learning_rate": 1.8732876712328768e-05,
      "loss": 0.0415,
      "step": 8230
    },
    {
      "epoch": 0.37625570776255707,
      "grad_norm": 0.33018556237220764,
      "learning_rate": 1.871917808219178e-05,
      "loss": 0.022,
      "step": 8240
    },
    {
      "epoch": 0.3767123287671233,
      "grad_norm": 0.9409323930740356,
      "learning_rate": 1.8705479452054794e-05,
      "loss": 0.0344,
      "step": 8250
    },
    {
      "epoch": 0.3771689497716895,
      "grad_norm": 0.5748110413551331,
      "learning_rate": 1.869178082191781e-05,
      "loss": 0.0321,
      "step": 8260
    },
    {
      "epoch": 0.3776255707762557,
      "grad_norm": 0.6735018491744995,
      "learning_rate": 1.8678082191780823e-05,
      "loss": 0.0232,
      "step": 8270
    },
    {
      "epoch": 0.3780821917808219,
      "grad_norm": 1.6211460828781128,
      "learning_rate": 1.8664383561643838e-05,
      "loss": 0.0252,
      "step": 8280
    },
    {
      "epoch": 0.37853881278538815,
      "grad_norm": 0.5606389045715332,
      "learning_rate": 1.865068493150685e-05,
      "loss": 0.0302,
      "step": 8290
    },
    {
      "epoch": 0.3789954337899543,
      "grad_norm": 0.3504773676395416,
      "learning_rate": 1.8636986301369863e-05,
      "loss": 0.0409,
      "step": 8300
    },
    {
      "epoch": 0.37945205479452054,
      "grad_norm": 0.8434942364692688,
      "learning_rate": 1.8623287671232875e-05,
      "loss": 0.0369,
      "step": 8310
    },
    {
      "epoch": 0.37990867579908677,
      "grad_norm": 1.0035918951034546,
      "learning_rate": 1.8609589041095893e-05,
      "loss": 0.031,
      "step": 8320
    },
    {
      "epoch": 0.38036529680365294,
      "grad_norm": 0.6389926671981812,
      "learning_rate": 1.8595890410958907e-05,
      "loss": 0.0284,
      "step": 8330
    },
    {
      "epoch": 0.38082191780821917,
      "grad_norm": 0.3163051903247833,
      "learning_rate": 1.858219178082192e-05,
      "loss": 0.0309,
      "step": 8340
    },
    {
      "epoch": 0.3812785388127854,
      "grad_norm": 0.39841869473457336,
      "learning_rate": 1.8568493150684933e-05,
      "loss": 0.0213,
      "step": 8350
    },
    {
      "epoch": 0.3817351598173516,
      "grad_norm": 1.8706088066101074,
      "learning_rate": 1.8554794520547944e-05,
      "loss": 0.0281,
      "step": 8360
    },
    {
      "epoch": 0.3821917808219178,
      "grad_norm": 0.6365149021148682,
      "learning_rate": 1.854109589041096e-05,
      "loss": 0.0338,
      "step": 8370
    },
    {
      "epoch": 0.382648401826484,
      "grad_norm": 1.791859745979309,
      "learning_rate": 1.8527397260273973e-05,
      "loss": 0.0333,
      "step": 8380
    },
    {
      "epoch": 0.38310502283105025,
      "grad_norm": 1.6452001333236694,
      "learning_rate": 1.8513698630136988e-05,
      "loss": 0.0304,
      "step": 8390
    },
    {
      "epoch": 0.3835616438356164,
      "grad_norm": 1.013444423675537,
      "learning_rate": 1.8500000000000002e-05,
      "loss": 0.032,
      "step": 8400
    },
    {
      "epoch": 0.38401826484018264,
      "grad_norm": 1.465605616569519,
      "learning_rate": 1.8486301369863014e-05,
      "loss": 0.0293,
      "step": 8410
    },
    {
      "epoch": 0.38447488584474887,
      "grad_norm": 0.9624452590942383,
      "learning_rate": 1.8472602739726028e-05,
      "loss": 0.0238,
      "step": 8420
    },
    {
      "epoch": 0.38493150684931504,
      "grad_norm": 1.0913255214691162,
      "learning_rate": 1.845890410958904e-05,
      "loss": 0.0194,
      "step": 8430
    },
    {
      "epoch": 0.38538812785388127,
      "grad_norm": 0.9138051271438599,
      "learning_rate": 1.8445205479452054e-05,
      "loss": 0.025,
      "step": 8440
    },
    {
      "epoch": 0.3858447488584475,
      "grad_norm": 0.7865869998931885,
      "learning_rate": 1.843150684931507e-05,
      "loss": 0.0289,
      "step": 8450
    },
    {
      "epoch": 0.3863013698630137,
      "grad_norm": 0.759337842464447,
      "learning_rate": 1.8417808219178083e-05,
      "loss": 0.0311,
      "step": 8460
    },
    {
      "epoch": 0.3867579908675799,
      "grad_norm": 1.561389446258545,
      "learning_rate": 1.8404109589041098e-05,
      "loss": 0.029,
      "step": 8470
    },
    {
      "epoch": 0.3872146118721461,
      "grad_norm": 0.8881829380989075,
      "learning_rate": 1.839041095890411e-05,
      "loss": 0.0186,
      "step": 8480
    },
    {
      "epoch": 0.38767123287671235,
      "grad_norm": 0.4168879985809326,
      "learning_rate": 1.8376712328767123e-05,
      "loss": 0.0221,
      "step": 8490
    },
    {
      "epoch": 0.3881278538812785,
      "grad_norm": 0.5540768504142761,
      "learning_rate": 1.8363013698630138e-05,
      "loss": 0.0259,
      "step": 8500
    },
    {
      "epoch": 0.38858447488584474,
      "grad_norm": 0.9103793501853943,
      "learning_rate": 1.834931506849315e-05,
      "loss": 0.0289,
      "step": 8510
    },
    {
      "epoch": 0.38904109589041097,
      "grad_norm": 0.5760685205459595,
      "learning_rate": 1.8335616438356167e-05,
      "loss": 0.0253,
      "step": 8520
    },
    {
      "epoch": 0.38949771689497714,
      "grad_norm": 0.9138177037239075,
      "learning_rate": 1.832191780821918e-05,
      "loss": 0.0307,
      "step": 8530
    },
    {
      "epoch": 0.38995433789954337,
      "grad_norm": 0.4921656548976898,
      "learning_rate": 1.8308219178082193e-05,
      "loss": 0.0329,
      "step": 8540
    },
    {
      "epoch": 0.3904109589041096,
      "grad_norm": 0.5629436373710632,
      "learning_rate": 1.8294520547945207e-05,
      "loss": 0.0306,
      "step": 8550
    },
    {
      "epoch": 0.3908675799086758,
      "grad_norm": 1.0263607501983643,
      "learning_rate": 1.828082191780822e-05,
      "loss": 0.0377,
      "step": 8560
    },
    {
      "epoch": 0.391324200913242,
      "grad_norm": 0.6684034466743469,
      "learning_rate": 1.8267123287671233e-05,
      "loss": 0.0202,
      "step": 8570
    },
    {
      "epoch": 0.3917808219178082,
      "grad_norm": 0.6756128072738647,
      "learning_rate": 1.8253424657534244e-05,
      "loss": 0.0312,
      "step": 8580
    },
    {
      "epoch": 0.39223744292237445,
      "grad_norm": 0.549956738948822,
      "learning_rate": 1.8239726027397262e-05,
      "loss": 0.0285,
      "step": 8590
    },
    {
      "epoch": 0.3926940639269406,
      "grad_norm": 0.6471825242042542,
      "learning_rate": 1.8226027397260277e-05,
      "loss": 0.0229,
      "step": 8600
    },
    {
      "epoch": 0.39315068493150684,
      "grad_norm": 0.8940890431404114,
      "learning_rate": 1.8212328767123288e-05,
      "loss": 0.0315,
      "step": 8610
    },
    {
      "epoch": 0.39360730593607307,
      "grad_norm": 0.8927336931228638,
      "learning_rate": 1.8198630136986303e-05,
      "loss": 0.0198,
      "step": 8620
    },
    {
      "epoch": 0.39406392694063924,
      "grad_norm": 3.4085965156555176,
      "learning_rate": 1.8184931506849314e-05,
      "loss": 0.0269,
      "step": 8630
    },
    {
      "epoch": 0.39452054794520547,
      "grad_norm": 0.241181418299675,
      "learning_rate": 1.817123287671233e-05,
      "loss": 0.0205,
      "step": 8640
    },
    {
      "epoch": 0.3949771689497717,
      "grad_norm": 0.3485579788684845,
      "learning_rate": 1.815753424657534e-05,
      "loss": 0.0305,
      "step": 8650
    },
    {
      "epoch": 0.3954337899543379,
      "grad_norm": 0.7293670177459717,
      "learning_rate": 1.8143835616438358e-05,
      "loss": 0.034,
      "step": 8660
    },
    {
      "epoch": 0.3958904109589041,
      "grad_norm": 0.8844612240791321,
      "learning_rate": 1.8130136986301372e-05,
      "loss": 0.032,
      "step": 8670
    },
    {
      "epoch": 0.3963470319634703,
      "grad_norm": 1.0358420610427856,
      "learning_rate": 1.8116438356164383e-05,
      "loss": 0.03,
      "step": 8680
    },
    {
      "epoch": 0.39680365296803655,
      "grad_norm": 0.6096936464309692,
      "learning_rate": 1.8102739726027398e-05,
      "loss": 0.0278,
      "step": 8690
    },
    {
      "epoch": 0.3972602739726027,
      "grad_norm": 0.48437780141830444,
      "learning_rate": 1.808904109589041e-05,
      "loss": 0.0279,
      "step": 8700
    },
    {
      "epoch": 0.39771689497716894,
      "grad_norm": 2.6727044582366943,
      "learning_rate": 1.8075342465753424e-05,
      "loss": 0.0281,
      "step": 8710
    },
    {
      "epoch": 0.39817351598173517,
      "grad_norm": 0.8933859467506409,
      "learning_rate": 1.806164383561644e-05,
      "loss": 0.0265,
      "step": 8720
    },
    {
      "epoch": 0.39863013698630134,
      "grad_norm": 1.387864589691162,
      "learning_rate": 1.8047945205479453e-05,
      "loss": 0.0279,
      "step": 8730
    },
    {
      "epoch": 0.39908675799086757,
      "grad_norm": 0.7955613732337952,
      "learning_rate": 1.8034246575342467e-05,
      "loss": 0.0293,
      "step": 8740
    },
    {
      "epoch": 0.3995433789954338,
      "grad_norm": 0.4397881329059601,
      "learning_rate": 1.802054794520548e-05,
      "loss": 0.0213,
      "step": 8750
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.6128576397895813,
      "learning_rate": 1.8006849315068493e-05,
      "loss": 0.0268,
      "step": 8760
    },
    {
      "epoch": 0.4004566210045662,
      "grad_norm": 0.4490954279899597,
      "learning_rate": 1.7993150684931508e-05,
      "loss": 0.029,
      "step": 8770
    },
    {
      "epoch": 0.4009132420091324,
      "grad_norm": 0.6600049734115601,
      "learning_rate": 1.797945205479452e-05,
      "loss": 0.0317,
      "step": 8780
    },
    {
      "epoch": 0.40136986301369865,
      "grad_norm": 0.7567429542541504,
      "learning_rate": 1.7965753424657537e-05,
      "loss": 0.0256,
      "step": 8790
    },
    {
      "epoch": 0.4018264840182648,
      "grad_norm": 0.9584996104240417,
      "learning_rate": 1.7952054794520548e-05,
      "loss": 0.0285,
      "step": 8800
    },
    {
      "epoch": 0.40228310502283104,
      "grad_norm": 0.44268563389778137,
      "learning_rate": 1.7938356164383563e-05,
      "loss": 0.0238,
      "step": 8810
    },
    {
      "epoch": 0.40273972602739727,
      "grad_norm": 0.9318729639053345,
      "learning_rate": 1.7924657534246577e-05,
      "loss": 0.034,
      "step": 8820
    },
    {
      "epoch": 0.4031963470319635,
      "grad_norm": 0.5734882950782776,
      "learning_rate": 1.791095890410959e-05,
      "loss": 0.0448,
      "step": 8830
    },
    {
      "epoch": 0.40365296803652967,
      "grad_norm": 1.0070502758026123,
      "learning_rate": 1.7897260273972603e-05,
      "loss": 0.0238,
      "step": 8840
    },
    {
      "epoch": 0.4041095890410959,
      "grad_norm": 0.29287415742874146,
      "learning_rate": 1.7883561643835614e-05,
      "loss": 0.0271,
      "step": 8850
    },
    {
      "epoch": 0.4045662100456621,
      "grad_norm": 0.514440655708313,
      "learning_rate": 1.7869863013698632e-05,
      "loss": 0.0231,
      "step": 8860
    },
    {
      "epoch": 0.4050228310502283,
      "grad_norm": 0.42013218998908997,
      "learning_rate": 1.7856164383561647e-05,
      "loss": 0.0257,
      "step": 8870
    },
    {
      "epoch": 0.4054794520547945,
      "grad_norm": 0.7418811321258545,
      "learning_rate": 1.7842465753424658e-05,
      "loss": 0.0318,
      "step": 8880
    },
    {
      "epoch": 0.40593607305936075,
      "grad_norm": 0.5835978984832764,
      "learning_rate": 1.7828767123287672e-05,
      "loss": 0.032,
      "step": 8890
    },
    {
      "epoch": 0.4063926940639269,
      "grad_norm": 0.46701714396476746,
      "learning_rate": 1.7815068493150684e-05,
      "loss": 0.0352,
      "step": 8900
    },
    {
      "epoch": 0.40684931506849314,
      "grad_norm": 0.6830207109451294,
      "learning_rate": 1.7801369863013698e-05,
      "loss": 0.0215,
      "step": 8910
    },
    {
      "epoch": 0.40730593607305937,
      "grad_norm": 0.5306940674781799,
      "learning_rate": 1.7787671232876713e-05,
      "loss": 0.0275,
      "step": 8920
    },
    {
      "epoch": 0.4077625570776256,
      "grad_norm": 0.590941309928894,
      "learning_rate": 1.7773972602739727e-05,
      "loss": 0.031,
      "step": 8930
    },
    {
      "epoch": 0.40821917808219177,
      "grad_norm": 1.8286815881729126,
      "learning_rate": 1.7760273972602742e-05,
      "loss": 0.0357,
      "step": 8940
    },
    {
      "epoch": 0.408675799086758,
      "grad_norm": 0.7582992315292358,
      "learning_rate": 1.7746575342465753e-05,
      "loss": 0.0301,
      "step": 8950
    },
    {
      "epoch": 0.4091324200913242,
      "grad_norm": 1.3838093280792236,
      "learning_rate": 1.7732876712328768e-05,
      "loss": 0.0317,
      "step": 8960
    },
    {
      "epoch": 0.4095890410958904,
      "grad_norm": 0.5052372813224792,
      "learning_rate": 1.771917808219178e-05,
      "loss": 0.0326,
      "step": 8970
    },
    {
      "epoch": 0.4100456621004566,
      "grad_norm": 0.29588940739631653,
      "learning_rate": 1.7705479452054793e-05,
      "loss": 0.0266,
      "step": 8980
    },
    {
      "epoch": 0.41050228310502285,
      "grad_norm": 1.1172634363174438,
      "learning_rate": 1.769178082191781e-05,
      "loss": 0.0138,
      "step": 8990
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 1.2579537630081177,
      "learning_rate": 1.7678082191780823e-05,
      "loss": 0.0421,
      "step": 9000
    },
    {
      "epoch": 0.41141552511415524,
      "grad_norm": 0.765623152256012,
      "learning_rate": 1.7664383561643837e-05,
      "loss": 0.0235,
      "step": 9010
    },
    {
      "epoch": 0.41187214611872147,
      "grad_norm": 0.8000991344451904,
      "learning_rate": 1.765068493150685e-05,
      "loss": 0.0289,
      "step": 9020
    },
    {
      "epoch": 0.4123287671232877,
      "grad_norm": 0.4213976562023163,
      "learning_rate": 1.7636986301369863e-05,
      "loss": 0.0247,
      "step": 9030
    },
    {
      "epoch": 0.41278538812785387,
      "grad_norm": 0.6951884627342224,
      "learning_rate": 1.7623287671232877e-05,
      "loss": 0.0281,
      "step": 9040
    },
    {
      "epoch": 0.4132420091324201,
      "grad_norm": 0.3719956874847412,
      "learning_rate": 1.760958904109589e-05,
      "loss": 0.0269,
      "step": 9050
    },
    {
      "epoch": 0.4136986301369863,
      "grad_norm": 0.7439400553703308,
      "learning_rate": 1.7595890410958907e-05,
      "loss": 0.0235,
      "step": 9060
    },
    {
      "epoch": 0.4141552511415525,
      "grad_norm": 0.7568884491920471,
      "learning_rate": 1.7582191780821918e-05,
      "loss": 0.033,
      "step": 9070
    },
    {
      "epoch": 0.4146118721461187,
      "grad_norm": 0.39767926931381226,
      "learning_rate": 1.7568493150684932e-05,
      "loss": 0.0322,
      "step": 9080
    },
    {
      "epoch": 0.41506849315068495,
      "grad_norm": 0.3026350438594818,
      "learning_rate": 1.7554794520547947e-05,
      "loss": 0.026,
      "step": 9090
    },
    {
      "epoch": 0.4155251141552511,
      "grad_norm": 0.5721660256385803,
      "learning_rate": 1.7541095890410958e-05,
      "loss": 0.0324,
      "step": 9100
    },
    {
      "epoch": 0.41598173515981735,
      "grad_norm": 0.5549749732017517,
      "learning_rate": 1.7527397260273973e-05,
      "loss": 0.0177,
      "step": 9110
    },
    {
      "epoch": 0.41643835616438357,
      "grad_norm": 0.4553723633289337,
      "learning_rate": 1.7513698630136987e-05,
      "loss": 0.0224,
      "step": 9120
    },
    {
      "epoch": 0.4168949771689498,
      "grad_norm": 1.9023690223693848,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 0.0225,
      "step": 9130
    },
    {
      "epoch": 0.41735159817351597,
      "grad_norm": 0.7688285708427429,
      "learning_rate": 1.7486301369863016e-05,
      "loss": 0.0379,
      "step": 9140
    },
    {
      "epoch": 0.4178082191780822,
      "grad_norm": 1.2942781448364258,
      "learning_rate": 1.7472602739726028e-05,
      "loss": 0.0355,
      "step": 9150
    },
    {
      "epoch": 0.4182648401826484,
      "grad_norm": 0.5246925354003906,
      "learning_rate": 1.7458904109589042e-05,
      "loss": 0.0435,
      "step": 9160
    },
    {
      "epoch": 0.4187214611872146,
      "grad_norm": 0.7790400981903076,
      "learning_rate": 1.7445205479452053e-05,
      "loss": 0.0233,
      "step": 9170
    },
    {
      "epoch": 0.4191780821917808,
      "grad_norm": 0.8263158202171326,
      "learning_rate": 1.7431506849315068e-05,
      "loss": 0.0288,
      "step": 9180
    },
    {
      "epoch": 0.41963470319634705,
      "grad_norm": 0.5817291140556335,
      "learning_rate": 1.7417808219178083e-05,
      "loss": 0.0276,
      "step": 9190
    },
    {
      "epoch": 0.4200913242009132,
      "grad_norm": 0.479882150888443,
      "learning_rate": 1.7404109589041097e-05,
      "loss": 0.0272,
      "step": 9200
    },
    {
      "epoch": 0.42054794520547945,
      "grad_norm": 0.5421848297119141,
      "learning_rate": 1.739041095890411e-05,
      "loss": 0.0345,
      "step": 9210
    },
    {
      "epoch": 0.42100456621004567,
      "grad_norm": 1.269191026687622,
      "learning_rate": 1.7376712328767123e-05,
      "loss": 0.028,
      "step": 9220
    },
    {
      "epoch": 0.4214611872146119,
      "grad_norm": 0.4680539667606354,
      "learning_rate": 1.7363013698630137e-05,
      "loss": 0.0201,
      "step": 9230
    },
    {
      "epoch": 0.42191780821917807,
      "grad_norm": 0.10971256345510483,
      "learning_rate": 1.734931506849315e-05,
      "loss": 0.0226,
      "step": 9240
    },
    {
      "epoch": 0.4223744292237443,
      "grad_norm": 0.5445984601974487,
      "learning_rate": 1.7335616438356163e-05,
      "loss": 0.028,
      "step": 9250
    },
    {
      "epoch": 0.4228310502283105,
      "grad_norm": 1.4013817310333252,
      "learning_rate": 1.732191780821918e-05,
      "loss": 0.03,
      "step": 9260
    },
    {
      "epoch": 0.4232876712328767,
      "grad_norm": 0.8567643761634827,
      "learning_rate": 1.7308219178082192e-05,
      "loss": 0.0341,
      "step": 9270
    },
    {
      "epoch": 0.4237442922374429,
      "grad_norm": 0.6961230635643005,
      "learning_rate": 1.7294520547945207e-05,
      "loss": 0.0308,
      "step": 9280
    },
    {
      "epoch": 0.42420091324200915,
      "grad_norm": 0.8256348371505737,
      "learning_rate": 1.7280821917808218e-05,
      "loss": 0.0272,
      "step": 9290
    },
    {
      "epoch": 0.4246575342465753,
      "grad_norm": 0.7823105454444885,
      "learning_rate": 1.7267123287671233e-05,
      "loss": 0.0288,
      "step": 9300
    },
    {
      "epoch": 0.42511415525114155,
      "grad_norm": 1.1408393383026123,
      "learning_rate": 1.7253424657534247e-05,
      "loss": 0.0235,
      "step": 9310
    },
    {
      "epoch": 0.42557077625570777,
      "grad_norm": 1.0115615129470825,
      "learning_rate": 1.7239726027397262e-05,
      "loss": 0.0233,
      "step": 9320
    },
    {
      "epoch": 0.426027397260274,
      "grad_norm": 0.49147704243659973,
      "learning_rate": 1.7226027397260276e-05,
      "loss": 0.0269,
      "step": 9330
    },
    {
      "epoch": 0.42648401826484017,
      "grad_norm": 0.6529240608215332,
      "learning_rate": 1.7212328767123288e-05,
      "loss": 0.0198,
      "step": 9340
    },
    {
      "epoch": 0.4269406392694064,
      "grad_norm": 0.3151921331882477,
      "learning_rate": 1.7198630136986302e-05,
      "loss": 0.0256,
      "step": 9350
    },
    {
      "epoch": 0.4273972602739726,
      "grad_norm": 0.38915735483169556,
      "learning_rate": 1.7184931506849317e-05,
      "loss": 0.0267,
      "step": 9360
    },
    {
      "epoch": 0.4278538812785388,
      "grad_norm": 0.9495638608932495,
      "learning_rate": 1.7171232876712328e-05,
      "loss": 0.0325,
      "step": 9370
    },
    {
      "epoch": 0.428310502283105,
      "grad_norm": 0.8147823810577393,
      "learning_rate": 1.7157534246575342e-05,
      "loss": 0.028,
      "step": 9380
    },
    {
      "epoch": 0.42876712328767125,
      "grad_norm": 0.6028589010238647,
      "learning_rate": 1.7143835616438357e-05,
      "loss": 0.0307,
      "step": 9390
    },
    {
      "epoch": 0.4292237442922374,
      "grad_norm": 0.14244051277637482,
      "learning_rate": 1.713013698630137e-05,
      "loss": 0.0258,
      "step": 9400
    },
    {
      "epoch": 0.42968036529680365,
      "grad_norm": 1.181275486946106,
      "learning_rate": 1.7116438356164383e-05,
      "loss": 0.0306,
      "step": 9410
    },
    {
      "epoch": 0.4301369863013699,
      "grad_norm": 0.9625024795532227,
      "learning_rate": 1.7102739726027397e-05,
      "loss": 0.0452,
      "step": 9420
    },
    {
      "epoch": 0.4305936073059361,
      "grad_norm": 1.6104717254638672,
      "learning_rate": 1.7089041095890412e-05,
      "loss": 0.0226,
      "step": 9430
    },
    {
      "epoch": 0.43105022831050227,
      "grad_norm": 0.8405230045318604,
      "learning_rate": 1.7076712328767122e-05,
      "loss": 0.0297,
      "step": 9440
    },
    {
      "epoch": 0.4315068493150685,
      "grad_norm": 0.7266689538955688,
      "learning_rate": 1.7063013698630137e-05,
      "loss": 0.0296,
      "step": 9450
    },
    {
      "epoch": 0.4319634703196347,
      "grad_norm": 0.8586393594741821,
      "learning_rate": 1.704931506849315e-05,
      "loss": 0.0399,
      "step": 9460
    },
    {
      "epoch": 0.4324200913242009,
      "grad_norm": 0.3961942791938782,
      "learning_rate": 1.7035616438356166e-05,
      "loss": 0.0329,
      "step": 9470
    },
    {
      "epoch": 0.4328767123287671,
      "grad_norm": 0.6890533566474915,
      "learning_rate": 1.702191780821918e-05,
      "loss": 0.0247,
      "step": 9480
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.4993564784526825,
      "learning_rate": 1.7008219178082192e-05,
      "loss": 0.0314,
      "step": 9490
    },
    {
      "epoch": 0.4337899543378995,
      "grad_norm": 0.7032297849655151,
      "learning_rate": 1.6994520547945206e-05,
      "loss": 0.0254,
      "step": 9500
    },
    {
      "epoch": 0.43424657534246575,
      "grad_norm": 0.13023807108402252,
      "learning_rate": 1.6980821917808218e-05,
      "loss": 0.0239,
      "step": 9510
    },
    {
      "epoch": 0.434703196347032,
      "grad_norm": 0.45173215866088867,
      "learning_rate": 1.6967123287671232e-05,
      "loss": 0.0208,
      "step": 9520
    },
    {
      "epoch": 0.4351598173515982,
      "grad_norm": 0.5570506453514099,
      "learning_rate": 1.6953424657534247e-05,
      "loss": 0.026,
      "step": 9530
    },
    {
      "epoch": 0.43561643835616437,
      "grad_norm": 0.17638742923736572,
      "learning_rate": 1.693972602739726e-05,
      "loss": 0.0243,
      "step": 9540
    },
    {
      "epoch": 0.4360730593607306,
      "grad_norm": 0.4361788034439087,
      "learning_rate": 1.6926027397260276e-05,
      "loss": 0.023,
      "step": 9550
    },
    {
      "epoch": 0.4365296803652968,
      "grad_norm": 0.5482596755027771,
      "learning_rate": 1.6912328767123287e-05,
      "loss": 0.0325,
      "step": 9560
    },
    {
      "epoch": 0.436986301369863,
      "grad_norm": 0.466251939535141,
      "learning_rate": 1.68986301369863e-05,
      "loss": 0.036,
      "step": 9570
    },
    {
      "epoch": 0.4374429223744292,
      "grad_norm": 0.7898319959640503,
      "learning_rate": 1.6884931506849316e-05,
      "loss": 0.0317,
      "step": 9580
    },
    {
      "epoch": 0.43789954337899545,
      "grad_norm": 0.3824157118797302,
      "learning_rate": 1.6871232876712327e-05,
      "loss": 0.0328,
      "step": 9590
    },
    {
      "epoch": 0.4383561643835616,
      "grad_norm": 0.6748829483985901,
      "learning_rate": 1.6857534246575342e-05,
      "loss": 0.0267,
      "step": 9600
    },
    {
      "epoch": 0.43881278538812785,
      "grad_norm": 1.6105477809906006,
      "learning_rate": 1.6843835616438357e-05,
      "loss": 0.0317,
      "step": 9610
    },
    {
      "epoch": 0.4392694063926941,
      "grad_norm": 1.1747736930847168,
      "learning_rate": 1.683013698630137e-05,
      "loss": 0.0254,
      "step": 9620
    },
    {
      "epoch": 0.4397260273972603,
      "grad_norm": 0.7491133213043213,
      "learning_rate": 1.6816438356164386e-05,
      "loss": 0.037,
      "step": 9630
    },
    {
      "epoch": 0.44018264840182647,
      "grad_norm": 0.7842443585395813,
      "learning_rate": 1.6802739726027397e-05,
      "loss": 0.0265,
      "step": 9640
    },
    {
      "epoch": 0.4406392694063927,
      "grad_norm": 0.935350775718689,
      "learning_rate": 1.678904109589041e-05,
      "loss": 0.0238,
      "step": 9650
    },
    {
      "epoch": 0.4410958904109589,
      "grad_norm": 0.9011167287826538,
      "learning_rate": 1.6775342465753423e-05,
      "loss": 0.0231,
      "step": 9660
    },
    {
      "epoch": 0.4415525114155251,
      "grad_norm": 0.7911438345909119,
      "learning_rate": 1.676164383561644e-05,
      "loss": 0.0234,
      "step": 9670
    },
    {
      "epoch": 0.4420091324200913,
      "grad_norm": 0.8484595417976379,
      "learning_rate": 1.6747945205479455e-05,
      "loss": 0.0257,
      "step": 9680
    },
    {
      "epoch": 0.44246575342465755,
      "grad_norm": 0.2935827076435089,
      "learning_rate": 1.6734246575342466e-05,
      "loss": 0.0285,
      "step": 9690
    },
    {
      "epoch": 0.4429223744292237,
      "grad_norm": 0.3328363001346588,
      "learning_rate": 1.672054794520548e-05,
      "loss": 0.0257,
      "step": 9700
    },
    {
      "epoch": 0.44337899543378995,
      "grad_norm": 0.641880989074707,
      "learning_rate": 1.6706849315068492e-05,
      "loss": 0.0355,
      "step": 9710
    },
    {
      "epoch": 0.4438356164383562,
      "grad_norm": 1.03374183177948,
      "learning_rate": 1.6693150684931507e-05,
      "loss": 0.0351,
      "step": 9720
    },
    {
      "epoch": 0.4442922374429224,
      "grad_norm": 0.4677525758743286,
      "learning_rate": 1.6679452054794518e-05,
      "loss": 0.0301,
      "step": 9730
    },
    {
      "epoch": 0.44474885844748857,
      "grad_norm": 0.8010308742523193,
      "learning_rate": 1.6665753424657536e-05,
      "loss": 0.0286,
      "step": 9740
    },
    {
      "epoch": 0.4452054794520548,
      "grad_norm": 0.347551167011261,
      "learning_rate": 1.665205479452055e-05,
      "loss": 0.0233,
      "step": 9750
    },
    {
      "epoch": 0.445662100456621,
      "grad_norm": 0.7823700904846191,
      "learning_rate": 1.663835616438356e-05,
      "loss": 0.0412,
      "step": 9760
    },
    {
      "epoch": 0.4461187214611872,
      "grad_norm": 0.6136126518249512,
      "learning_rate": 1.6624657534246576e-05,
      "loss": 0.0303,
      "step": 9770
    },
    {
      "epoch": 0.4465753424657534,
      "grad_norm": 0.8812817335128784,
      "learning_rate": 1.6610958904109587e-05,
      "loss": 0.0263,
      "step": 9780
    },
    {
      "epoch": 0.44703196347031965,
      "grad_norm": 0.502252995967865,
      "learning_rate": 1.6597260273972602e-05,
      "loss": 0.025,
      "step": 9790
    },
    {
      "epoch": 0.4474885844748858,
      "grad_norm": 0.6754285097122192,
      "learning_rate": 1.6583561643835616e-05,
      "loss": 0.0275,
      "step": 9800
    },
    {
      "epoch": 0.44794520547945205,
      "grad_norm": 0.3327227234840393,
      "learning_rate": 1.656986301369863e-05,
      "loss": 0.0375,
      "step": 9810
    },
    {
      "epoch": 0.4484018264840183,
      "grad_norm": 0.7465355396270752,
      "learning_rate": 1.6556164383561646e-05,
      "loss": 0.0276,
      "step": 9820
    },
    {
      "epoch": 0.4488584474885845,
      "grad_norm": 0.9372804760932922,
      "learning_rate": 1.6542465753424657e-05,
      "loss": 0.0316,
      "step": 9830
    },
    {
      "epoch": 0.44931506849315067,
      "grad_norm": 0.701808512210846,
      "learning_rate": 1.652876712328767e-05,
      "loss": 0.0183,
      "step": 9840
    },
    {
      "epoch": 0.4497716894977169,
      "grad_norm": 0.8043548464775085,
      "learning_rate": 1.6515068493150686e-05,
      "loss": 0.0353,
      "step": 9850
    },
    {
      "epoch": 0.4502283105022831,
      "grad_norm": 0.6702455878257751,
      "learning_rate": 1.6501369863013697e-05,
      "loss": 0.0244,
      "step": 9860
    },
    {
      "epoch": 0.4506849315068493,
      "grad_norm": 0.9323266744613647,
      "learning_rate": 1.6487671232876715e-05,
      "loss": 0.0297,
      "step": 9870
    },
    {
      "epoch": 0.4511415525114155,
      "grad_norm": 1.9227615594863892,
      "learning_rate": 1.6473972602739726e-05,
      "loss": 0.031,
      "step": 9880
    },
    {
      "epoch": 0.45159817351598175,
      "grad_norm": 1.1067242622375488,
      "learning_rate": 1.646027397260274e-05,
      "loss": 0.0372,
      "step": 9890
    },
    {
      "epoch": 0.4520547945205479,
      "grad_norm": 2.299252510070801,
      "learning_rate": 1.6446575342465755e-05,
      "loss": 0.0305,
      "step": 9900
    },
    {
      "epoch": 0.45251141552511415,
      "grad_norm": 0.481578528881073,
      "learning_rate": 1.6432876712328767e-05,
      "loss": 0.0302,
      "step": 9910
    },
    {
      "epoch": 0.4529680365296804,
      "grad_norm": 0.7764214873313904,
      "learning_rate": 1.641917808219178e-05,
      "loss": 0.0313,
      "step": 9920
    },
    {
      "epoch": 0.4534246575342466,
      "grad_norm": 0.9420430064201355,
      "learning_rate": 1.6405479452054792e-05,
      "loss": 0.0206,
      "step": 9930
    },
    {
      "epoch": 0.45388127853881277,
      "grad_norm": 1.1515851020812988,
      "learning_rate": 1.639178082191781e-05,
      "loss": 0.031,
      "step": 9940
    },
    {
      "epoch": 0.454337899543379,
      "grad_norm": 0.2604047954082489,
      "learning_rate": 1.6378082191780825e-05,
      "loss": 0.0264,
      "step": 9950
    },
    {
      "epoch": 0.4547945205479452,
      "grad_norm": 0.3723217248916626,
      "learning_rate": 1.6364383561643836e-05,
      "loss": 0.0391,
      "step": 9960
    },
    {
      "epoch": 0.4552511415525114,
      "grad_norm": 3.851811647415161,
      "learning_rate": 1.635068493150685e-05,
      "loss": 0.0409,
      "step": 9970
    },
    {
      "epoch": 0.4557077625570776,
      "grad_norm": 1.076546311378479,
      "learning_rate": 1.6336986301369862e-05,
      "loss": 0.027,
      "step": 9980
    },
    {
      "epoch": 0.45616438356164385,
      "grad_norm": 0.7868819832801819,
      "learning_rate": 1.6323287671232876e-05,
      "loss": 0.0263,
      "step": 9990
    },
    {
      "epoch": 0.45662100456621,
      "grad_norm": 0.8719764947891235,
      "learning_rate": 1.6309589041095888e-05,
      "loss": 0.0322,
      "step": 10000
    },
    {
      "epoch": 0.45707762557077625,
      "grad_norm": 0.6681948304176331,
      "learning_rate": 1.6295890410958906e-05,
      "loss": 0.0263,
      "step": 10010
    },
    {
      "epoch": 0.4575342465753425,
      "grad_norm": 0.7209538817405701,
      "learning_rate": 1.628219178082192e-05,
      "loss": 0.0318,
      "step": 10020
    },
    {
      "epoch": 0.4579908675799087,
      "grad_norm": 0.3393003046512604,
      "learning_rate": 1.626849315068493e-05,
      "loss": 0.0281,
      "step": 10030
    },
    {
      "epoch": 0.45844748858447487,
      "grad_norm": 0.6195233464241028,
      "learning_rate": 1.6254794520547946e-05,
      "loss": 0.0314,
      "step": 10040
    },
    {
      "epoch": 0.4589041095890411,
      "grad_norm": 0.508156955242157,
      "learning_rate": 1.6241095890410957e-05,
      "loss": 0.0162,
      "step": 10050
    },
    {
      "epoch": 0.4593607305936073,
      "grad_norm": 0.5470999479293823,
      "learning_rate": 1.622739726027397e-05,
      "loss": 0.0304,
      "step": 10060
    },
    {
      "epoch": 0.4598173515981735,
      "grad_norm": 0.46851590275764465,
      "learning_rate": 1.621369863013699e-05,
      "loss": 0.0293,
      "step": 10070
    },
    {
      "epoch": 0.4602739726027397,
      "grad_norm": 0.7966452836990356,
      "learning_rate": 1.62e-05,
      "loss": 0.0212,
      "step": 10080
    },
    {
      "epoch": 0.46073059360730595,
      "grad_norm": 0.6226071715354919,
      "learning_rate": 1.6186301369863015e-05,
      "loss": 0.0274,
      "step": 10090
    },
    {
      "epoch": 0.4611872146118721,
      "grad_norm": 1.146418809890747,
      "learning_rate": 1.6172602739726027e-05,
      "loss": 0.0221,
      "step": 10100
    },
    {
      "epoch": 0.46164383561643835,
      "grad_norm": 0.9241582751274109,
      "learning_rate": 1.615890410958904e-05,
      "loss": 0.0454,
      "step": 10110
    },
    {
      "epoch": 0.4621004566210046,
      "grad_norm": 0.5637359023094177,
      "learning_rate": 1.6145205479452056e-05,
      "loss": 0.028,
      "step": 10120
    },
    {
      "epoch": 0.4625570776255708,
      "grad_norm": 1.4717663526535034,
      "learning_rate": 1.6131506849315067e-05,
      "loss": 0.0233,
      "step": 10130
    },
    {
      "epoch": 0.46301369863013697,
      "grad_norm": 0.869580864906311,
      "learning_rate": 1.6117808219178085e-05,
      "loss": 0.0259,
      "step": 10140
    },
    {
      "epoch": 0.4634703196347032,
      "grad_norm": 0.7886480093002319,
      "learning_rate": 1.6104109589041096e-05,
      "loss": 0.0223,
      "step": 10150
    },
    {
      "epoch": 0.4639269406392694,
      "grad_norm": 1.0634448528289795,
      "learning_rate": 1.609041095890411e-05,
      "loss": 0.0236,
      "step": 10160
    },
    {
      "epoch": 0.4643835616438356,
      "grad_norm": 0.40349382162094116,
      "learning_rate": 1.6076712328767125e-05,
      "loss": 0.0377,
      "step": 10170
    },
    {
      "epoch": 0.4648401826484018,
      "grad_norm": 0.9996083378791809,
      "learning_rate": 1.6063013698630136e-05,
      "loss": 0.036,
      "step": 10180
    },
    {
      "epoch": 0.46529680365296805,
      "grad_norm": 0.6706119775772095,
      "learning_rate": 1.604931506849315e-05,
      "loss": 0.0389,
      "step": 10190
    },
    {
      "epoch": 0.4657534246575342,
      "grad_norm": 2.251314163208008,
      "learning_rate": 1.6035616438356162e-05,
      "loss": 0.031,
      "step": 10200
    },
    {
      "epoch": 0.46621004566210045,
      "grad_norm": 0.7072768807411194,
      "learning_rate": 1.602191780821918e-05,
      "loss": 0.0197,
      "step": 10210
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.6932006478309631,
      "learning_rate": 1.6008219178082195e-05,
      "loss": 0.0309,
      "step": 10220
    },
    {
      "epoch": 0.4671232876712329,
      "grad_norm": 0.38658663630485535,
      "learning_rate": 1.5994520547945206e-05,
      "loss": 0.0312,
      "step": 10230
    },
    {
      "epoch": 0.46757990867579907,
      "grad_norm": 0.8880840539932251,
      "learning_rate": 1.598082191780822e-05,
      "loss": 0.0265,
      "step": 10240
    },
    {
      "epoch": 0.4680365296803653,
      "grad_norm": 1.8358628749847412,
      "learning_rate": 1.596712328767123e-05,
      "loss": 0.0362,
      "step": 10250
    },
    {
      "epoch": 0.4684931506849315,
      "grad_norm": 0.768896758556366,
      "learning_rate": 1.5953424657534246e-05,
      "loss": 0.0372,
      "step": 10260
    },
    {
      "epoch": 0.4689497716894977,
      "grad_norm": 0.6682006120681763,
      "learning_rate": 1.593972602739726e-05,
      "loss": 0.0318,
      "step": 10270
    },
    {
      "epoch": 0.4694063926940639,
      "grad_norm": 0.8154324293136597,
      "learning_rate": 1.5926027397260275e-05,
      "loss": 0.0297,
      "step": 10280
    },
    {
      "epoch": 0.46986301369863015,
      "grad_norm": 0.7558693885803223,
      "learning_rate": 1.591232876712329e-05,
      "loss": 0.0302,
      "step": 10290
    },
    {
      "epoch": 0.4703196347031963,
      "grad_norm": 2.001007556915283,
      "learning_rate": 1.58986301369863e-05,
      "loss": 0.0304,
      "step": 10300
    },
    {
      "epoch": 0.47077625570776255,
      "grad_norm": 0.6086898446083069,
      "learning_rate": 1.5884931506849316e-05,
      "loss": 0.0284,
      "step": 10310
    },
    {
      "epoch": 0.4712328767123288,
      "grad_norm": 0.47625020146369934,
      "learning_rate": 1.5871232876712327e-05,
      "loss": 0.0369,
      "step": 10320
    },
    {
      "epoch": 0.471689497716895,
      "grad_norm": 0.6557756066322327,
      "learning_rate": 1.585753424657534e-05,
      "loss": 0.0187,
      "step": 10330
    },
    {
      "epoch": 0.47214611872146117,
      "grad_norm": 0.5333325862884521,
      "learning_rate": 1.584383561643836e-05,
      "loss": 0.0226,
      "step": 10340
    },
    {
      "epoch": 0.4726027397260274,
      "grad_norm": 0.623799741268158,
      "learning_rate": 1.583013698630137e-05,
      "loss": 0.0291,
      "step": 10350
    },
    {
      "epoch": 0.4730593607305936,
      "grad_norm": 0.8758369088172913,
      "learning_rate": 1.5816438356164385e-05,
      "loss": 0.0313,
      "step": 10360
    },
    {
      "epoch": 0.4735159817351598,
      "grad_norm": 0.7954309582710266,
      "learning_rate": 1.5802739726027396e-05,
      "loss": 0.023,
      "step": 10370
    },
    {
      "epoch": 0.473972602739726,
      "grad_norm": 0.8354175090789795,
      "learning_rate": 1.578904109589041e-05,
      "loss": 0.0284,
      "step": 10380
    },
    {
      "epoch": 0.47442922374429225,
      "grad_norm": 0.17964968085289001,
      "learning_rate": 1.5775342465753425e-05,
      "loss": 0.0303,
      "step": 10390
    },
    {
      "epoch": 0.4748858447488584,
      "grad_norm": 0.7068265080451965,
      "learning_rate": 1.5761643835616437e-05,
      "loss": 0.0332,
      "step": 10400
    },
    {
      "epoch": 0.47534246575342465,
      "grad_norm": 1.1763063669204712,
      "learning_rate": 1.5747945205479455e-05,
      "loss": 0.0281,
      "step": 10410
    },
    {
      "epoch": 0.4757990867579909,
      "grad_norm": 1.7029852867126465,
      "learning_rate": 1.5734246575342466e-05,
      "loss": 0.0319,
      "step": 10420
    },
    {
      "epoch": 0.4762557077625571,
      "grad_norm": 0.6261646151542664,
      "learning_rate": 1.572054794520548e-05,
      "loss": 0.0237,
      "step": 10430
    },
    {
      "epoch": 0.4767123287671233,
      "grad_norm": 1.1627973318099976,
      "learning_rate": 1.5706849315068495e-05,
      "loss": 0.0352,
      "step": 10440
    },
    {
      "epoch": 0.4771689497716895,
      "grad_norm": 1.2861297130584717,
      "learning_rate": 1.5693150684931506e-05,
      "loss": 0.0187,
      "step": 10450
    },
    {
      "epoch": 0.4776255707762557,
      "grad_norm": 0.8060535788536072,
      "learning_rate": 1.567945205479452e-05,
      "loss": 0.032,
      "step": 10460
    },
    {
      "epoch": 0.4780821917808219,
      "grad_norm": 0.4201686680316925,
      "learning_rate": 1.5665753424657535e-05,
      "loss": 0.0205,
      "step": 10470
    },
    {
      "epoch": 0.4785388127853881,
      "grad_norm": 0.4433104991912842,
      "learning_rate": 1.565205479452055e-05,
      "loss": 0.024,
      "step": 10480
    },
    {
      "epoch": 0.47899543378995435,
      "grad_norm": 1.2209999561309814,
      "learning_rate": 1.563835616438356e-05,
      "loss": 0.0158,
      "step": 10490
    },
    {
      "epoch": 0.4794520547945205,
      "grad_norm": 1.3519847393035889,
      "learning_rate": 1.5624657534246576e-05,
      "loss": 0.0213,
      "step": 10500
    },
    {
      "epoch": 0.47990867579908675,
      "grad_norm": 0.862914502620697,
      "learning_rate": 1.561095890410959e-05,
      "loss": 0.035,
      "step": 10510
    },
    {
      "epoch": 0.480365296803653,
      "grad_norm": 0.6481006145477295,
      "learning_rate": 1.55972602739726e-05,
      "loss": 0.0242,
      "step": 10520
    },
    {
      "epoch": 0.4808219178082192,
      "grad_norm": 0.37583687901496887,
      "learning_rate": 1.5583561643835616e-05,
      "loss": 0.0231,
      "step": 10530
    },
    {
      "epoch": 0.4812785388127854,
      "grad_norm": 0.4367542862892151,
      "learning_rate": 1.556986301369863e-05,
      "loss": 0.0243,
      "step": 10540
    },
    {
      "epoch": 0.4817351598173516,
      "grad_norm": 0.28858378529548645,
      "learning_rate": 1.5556164383561645e-05,
      "loss": 0.0249,
      "step": 10550
    },
    {
      "epoch": 0.4821917808219178,
      "grad_norm": 1.251011610031128,
      "learning_rate": 1.554246575342466e-05,
      "loss": 0.0211,
      "step": 10560
    },
    {
      "epoch": 0.482648401826484,
      "grad_norm": 0.7122374773025513,
      "learning_rate": 1.552876712328767e-05,
      "loss": 0.0233,
      "step": 10570
    },
    {
      "epoch": 0.4831050228310502,
      "grad_norm": 1.091339349746704,
      "learning_rate": 1.5515068493150685e-05,
      "loss": 0.0263,
      "step": 10580
    },
    {
      "epoch": 0.48356164383561645,
      "grad_norm": 1.099007248878479,
      "learning_rate": 1.5501369863013697e-05,
      "loss": 0.0299,
      "step": 10590
    },
    {
      "epoch": 0.4840182648401826,
      "grad_norm": 0.5883226990699768,
      "learning_rate": 1.548767123287671e-05,
      "loss": 0.0337,
      "step": 10600
    },
    {
      "epoch": 0.48447488584474885,
      "grad_norm": 0.4924408495426178,
      "learning_rate": 1.547397260273973e-05,
      "loss": 0.0223,
      "step": 10610
    },
    {
      "epoch": 0.4849315068493151,
      "grad_norm": 0.15284989774227142,
      "learning_rate": 1.546027397260274e-05,
      "loss": 0.0301,
      "step": 10620
    },
    {
      "epoch": 0.4853881278538813,
      "grad_norm": 1.0573902130126953,
      "learning_rate": 1.5446575342465755e-05,
      "loss": 0.0166,
      "step": 10630
    },
    {
      "epoch": 0.4858447488584475,
      "grad_norm": 0.6414885520935059,
      "learning_rate": 1.5432876712328766e-05,
      "loss": 0.0278,
      "step": 10640
    },
    {
      "epoch": 0.4863013698630137,
      "grad_norm": 0.9388964176177979,
      "learning_rate": 1.541917808219178e-05,
      "loss": 0.0252,
      "step": 10650
    },
    {
      "epoch": 0.4867579908675799,
      "grad_norm": 1.3254663944244385,
      "learning_rate": 1.5405479452054795e-05,
      "loss": 0.0259,
      "step": 10660
    },
    {
      "epoch": 0.4872146118721461,
      "grad_norm": 1.0892823934555054,
      "learning_rate": 1.539178082191781e-05,
      "loss": 0.0387,
      "step": 10670
    },
    {
      "epoch": 0.4876712328767123,
      "grad_norm": 0.6563096046447754,
      "learning_rate": 1.5378082191780824e-05,
      "loss": 0.0175,
      "step": 10680
    },
    {
      "epoch": 0.48812785388127855,
      "grad_norm": 0.2960280179977417,
      "learning_rate": 1.5364383561643835e-05,
      "loss": 0.0232,
      "step": 10690
    },
    {
      "epoch": 0.4885844748858447,
      "grad_norm": 0.778256893157959,
      "learning_rate": 1.535068493150685e-05,
      "loss": 0.0318,
      "step": 10700
    },
    {
      "epoch": 0.48904109589041095,
      "grad_norm": 0.6244747638702393,
      "learning_rate": 1.5336986301369865e-05,
      "loss": 0.0279,
      "step": 10710
    },
    {
      "epoch": 0.4894977168949772,
      "grad_norm": 1.099302887916565,
      "learning_rate": 1.5323287671232876e-05,
      "loss": 0.0262,
      "step": 10720
    },
    {
      "epoch": 0.4899543378995434,
      "grad_norm": 0.7086403369903564,
      "learning_rate": 1.530958904109589e-05,
      "loss": 0.0241,
      "step": 10730
    },
    {
      "epoch": 0.4904109589041096,
      "grad_norm": 0.45991620421409607,
      "learning_rate": 1.5295890410958905e-05,
      "loss": 0.0254,
      "step": 10740
    },
    {
      "epoch": 0.4908675799086758,
      "grad_norm": 0.7119296789169312,
      "learning_rate": 1.528219178082192e-05,
      "loss": 0.0285,
      "step": 10750
    },
    {
      "epoch": 0.491324200913242,
      "grad_norm": 0.2766788601875305,
      "learning_rate": 1.526849315068493e-05,
      "loss": 0.0319,
      "step": 10760
    },
    {
      "epoch": 0.4917808219178082,
      "grad_norm": 0.6753426194190979,
      "learning_rate": 1.5254794520547945e-05,
      "loss": 0.0244,
      "step": 10770
    },
    {
      "epoch": 0.4922374429223744,
      "grad_norm": 1.2086573839187622,
      "learning_rate": 1.5241095890410958e-05,
      "loss": 0.0352,
      "step": 10780
    },
    {
      "epoch": 0.49269406392694065,
      "grad_norm": 0.643290638923645,
      "learning_rate": 1.5227397260273973e-05,
      "loss": 0.023,
      "step": 10790
    },
    {
      "epoch": 0.4931506849315068,
      "grad_norm": 0.46144235134124756,
      "learning_rate": 1.5213698630136986e-05,
      "loss": 0.0262,
      "step": 10800
    },
    {
      "epoch": 0.49360730593607305,
      "grad_norm": 0.9415695667266846,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 0.0331,
      "step": 10810
    },
    {
      "epoch": 0.4940639269406393,
      "grad_norm": 0.9473763704299927,
      "learning_rate": 1.5186301369863015e-05,
      "loss": 0.0358,
      "step": 10820
    },
    {
      "epoch": 0.4945205479452055,
      "grad_norm": 0.6153883934020996,
      "learning_rate": 1.5172602739726028e-05,
      "loss": 0.0277,
      "step": 10830
    },
    {
      "epoch": 0.4949771689497717,
      "grad_norm": 0.3565620481967926,
      "learning_rate": 1.515890410958904e-05,
      "loss": 0.0305,
      "step": 10840
    },
    {
      "epoch": 0.4954337899543379,
      "grad_norm": 0.4379630982875824,
      "learning_rate": 1.5145205479452055e-05,
      "loss": 0.0263,
      "step": 10850
    },
    {
      "epoch": 0.4958904109589041,
      "grad_norm": 0.6690340638160706,
      "learning_rate": 1.5131506849315068e-05,
      "loss": 0.0212,
      "step": 10860
    },
    {
      "epoch": 0.4963470319634703,
      "grad_norm": 0.9622676968574524,
      "learning_rate": 1.5117808219178084e-05,
      "loss": 0.0313,
      "step": 10870
    },
    {
      "epoch": 0.4968036529680365,
      "grad_norm": 0.7283843159675598,
      "learning_rate": 1.5104109589041097e-05,
      "loss": 0.0268,
      "step": 10880
    },
    {
      "epoch": 0.49726027397260275,
      "grad_norm": 0.6465986967086792,
      "learning_rate": 1.509041095890411e-05,
      "loss": 0.0209,
      "step": 10890
    },
    {
      "epoch": 0.4977168949771689,
      "grad_norm": 0.5131672024726868,
      "learning_rate": 1.5076712328767125e-05,
      "loss": 0.0231,
      "step": 10900
    },
    {
      "epoch": 0.49817351598173515,
      "grad_norm": 0.8020852208137512,
      "learning_rate": 1.5063013698630137e-05,
      "loss": 0.0267,
      "step": 10910
    },
    {
      "epoch": 0.4986301369863014,
      "grad_norm": 0.4006616175174713,
      "learning_rate": 1.504931506849315e-05,
      "loss": 0.0212,
      "step": 10920
    },
    {
      "epoch": 0.4990867579908676,
      "grad_norm": 1.025197148323059,
      "learning_rate": 1.5035616438356163e-05,
      "loss": 0.0395,
      "step": 10930
    },
    {
      "epoch": 0.4995433789954338,
      "grad_norm": 0.47184351086616516,
      "learning_rate": 1.502191780821918e-05,
      "loss": 0.0255,
      "step": 10940
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.7374187111854553,
      "learning_rate": 1.5008219178082192e-05,
      "loss": 0.0249,
      "step": 10950
    },
    {
      "epoch": 0.5004566210045662,
      "grad_norm": 0.9398074150085449,
      "learning_rate": 1.4994520547945207e-05,
      "loss": 0.0278,
      "step": 10960
    },
    {
      "epoch": 0.5009132420091325,
      "grad_norm": 0.6477452516555786,
      "learning_rate": 1.498082191780822e-05,
      "loss": 0.0364,
      "step": 10970
    },
    {
      "epoch": 0.5013698630136987,
      "grad_norm": 1.6075522899627686,
      "learning_rate": 1.4967123287671233e-05,
      "loss": 0.0236,
      "step": 10980
    },
    {
      "epoch": 0.5018264840182648,
      "grad_norm": 1.1061060428619385,
      "learning_rate": 1.4953424657534247e-05,
      "loss": 0.0257,
      "step": 10990
    },
    {
      "epoch": 0.502283105022831,
      "grad_norm": 0.32345449924468994,
      "learning_rate": 1.493972602739726e-05,
      "loss": 0.0277,
      "step": 11000
    },
    {
      "epoch": 0.5027397260273972,
      "grad_norm": 1.023221492767334,
      "learning_rate": 1.4926027397260275e-05,
      "loss": 0.0262,
      "step": 11010
    },
    {
      "epoch": 0.5031963470319635,
      "grad_norm": 2.956681728363037,
      "learning_rate": 1.491232876712329e-05,
      "loss": 0.0368,
      "step": 11020
    },
    {
      "epoch": 0.5036529680365297,
      "grad_norm": 0.4605308473110199,
      "learning_rate": 1.4898630136986302e-05,
      "loss": 0.0314,
      "step": 11030
    },
    {
      "epoch": 0.5041095890410959,
      "grad_norm": 0.5917571783065796,
      "learning_rate": 1.4884931506849315e-05,
      "loss": 0.0242,
      "step": 11040
    },
    {
      "epoch": 0.5045662100456622,
      "grad_norm": 1.451424241065979,
      "learning_rate": 1.4871232876712328e-05,
      "loss": 0.0362,
      "step": 11050
    },
    {
      "epoch": 0.5050228310502283,
      "grad_norm": 0.8742598295211792,
      "learning_rate": 1.4857534246575344e-05,
      "loss": 0.0204,
      "step": 11060
    },
    {
      "epoch": 0.5054794520547945,
      "grad_norm": 1.6276664733886719,
      "learning_rate": 1.4843835616438357e-05,
      "loss": 0.0294,
      "step": 11070
    },
    {
      "epoch": 0.5059360730593607,
      "grad_norm": 1.690280795097351,
      "learning_rate": 1.483013698630137e-05,
      "loss": 0.0429,
      "step": 11080
    },
    {
      "epoch": 0.506392694063927,
      "grad_norm": 0.8663957715034485,
      "learning_rate": 1.4816438356164384e-05,
      "loss": 0.0288,
      "step": 11090
    },
    {
      "epoch": 0.5068493150684932,
      "grad_norm": 0.7400691509246826,
      "learning_rate": 1.4802739726027397e-05,
      "loss": 0.0285,
      "step": 11100
    },
    {
      "epoch": 0.5073059360730594,
      "grad_norm": 0.5670140385627747,
      "learning_rate": 1.478904109589041e-05,
      "loss": 0.0292,
      "step": 11110
    },
    {
      "epoch": 0.5077625570776255,
      "grad_norm": 0.5830381512641907,
      "learning_rate": 1.4775342465753426e-05,
      "loss": 0.0232,
      "step": 11120
    },
    {
      "epoch": 0.5082191780821917,
      "grad_norm": 0.4150000214576721,
      "learning_rate": 1.476164383561644e-05,
      "loss": 0.0238,
      "step": 11130
    },
    {
      "epoch": 0.508675799086758,
      "grad_norm": 1.5886962413787842,
      "learning_rate": 1.4747945205479452e-05,
      "loss": 0.0338,
      "step": 11140
    },
    {
      "epoch": 0.5091324200913242,
      "grad_norm": 0.63909912109375,
      "learning_rate": 1.4734246575342465e-05,
      "loss": 0.0181,
      "step": 11150
    },
    {
      "epoch": 0.5095890410958904,
      "grad_norm": 0.5003219842910767,
      "learning_rate": 1.472054794520548e-05,
      "loss": 0.0384,
      "step": 11160
    },
    {
      "epoch": 0.5100456621004567,
      "grad_norm": 0.7171802520751953,
      "learning_rate": 1.4706849315068494e-05,
      "loss": 0.0259,
      "step": 11170
    },
    {
      "epoch": 0.5105022831050229,
      "grad_norm": 0.6697818636894226,
      "learning_rate": 1.4693150684931507e-05,
      "loss": 0.031,
      "step": 11180
    },
    {
      "epoch": 0.510958904109589,
      "grad_norm": 0.55730140209198,
      "learning_rate": 1.4679452054794522e-05,
      "loss": 0.0264,
      "step": 11190
    },
    {
      "epoch": 0.5114155251141552,
      "grad_norm": 0.42501452565193176,
      "learning_rate": 1.4665753424657535e-05,
      "loss": 0.032,
      "step": 11200
    },
    {
      "epoch": 0.5118721461187214,
      "grad_norm": 0.6173924207687378,
      "learning_rate": 1.4652054794520547e-05,
      "loss": 0.0346,
      "step": 11210
    },
    {
      "epoch": 0.5123287671232877,
      "grad_norm": 1.27781081199646,
      "learning_rate": 1.4638356164383562e-05,
      "loss": 0.0353,
      "step": 11220
    },
    {
      "epoch": 0.5127853881278539,
      "grad_norm": 1.053409218788147,
      "learning_rate": 1.4624657534246577e-05,
      "loss": 0.03,
      "step": 11230
    },
    {
      "epoch": 0.5132420091324201,
      "grad_norm": 1.317496418952942,
      "learning_rate": 1.461095890410959e-05,
      "loss": 0.0281,
      "step": 11240
    },
    {
      "epoch": 0.5136986301369864,
      "grad_norm": 0.5806271433830261,
      "learning_rate": 1.4597260273972602e-05,
      "loss": 0.0219,
      "step": 11250
    },
    {
      "epoch": 0.5141552511415525,
      "grad_norm": 0.7422447204589844,
      "learning_rate": 1.4583561643835617e-05,
      "loss": 0.0238,
      "step": 11260
    },
    {
      "epoch": 0.5146118721461187,
      "grad_norm": 0.7865344285964966,
      "learning_rate": 1.456986301369863e-05,
      "loss": 0.0333,
      "step": 11270
    },
    {
      "epoch": 0.5150684931506849,
      "grad_norm": 0.8579336404800415,
      "learning_rate": 1.4556164383561644e-05,
      "loss": 0.029,
      "step": 11280
    },
    {
      "epoch": 0.5155251141552512,
      "grad_norm": 1.5441038608551025,
      "learning_rate": 1.4542465753424659e-05,
      "loss": 0.0242,
      "step": 11290
    },
    {
      "epoch": 0.5159817351598174,
      "grad_norm": 0.5831427574157715,
      "learning_rate": 1.4528767123287672e-05,
      "loss": 0.0221,
      "step": 11300
    },
    {
      "epoch": 0.5164383561643836,
      "grad_norm": 0.18536292016506195,
      "learning_rate": 1.4515068493150685e-05,
      "loss": 0.0257,
      "step": 11310
    },
    {
      "epoch": 0.5168949771689497,
      "grad_norm": 0.3718271851539612,
      "learning_rate": 1.45013698630137e-05,
      "loss": 0.0333,
      "step": 11320
    },
    {
      "epoch": 0.517351598173516,
      "grad_norm": 0.7221677899360657,
      "learning_rate": 1.4487671232876712e-05,
      "loss": 0.0264,
      "step": 11330
    },
    {
      "epoch": 0.5178082191780822,
      "grad_norm": 0.8905743956565857,
      "learning_rate": 1.4473972602739727e-05,
      "loss": 0.0269,
      "step": 11340
    },
    {
      "epoch": 0.5182648401826484,
      "grad_norm": 0.8127612471580505,
      "learning_rate": 1.446027397260274e-05,
      "loss": 0.0356,
      "step": 11350
    },
    {
      "epoch": 0.5187214611872146,
      "grad_norm": 0.457339882850647,
      "learning_rate": 1.4446575342465754e-05,
      "loss": 0.047,
      "step": 11360
    },
    {
      "epoch": 0.5191780821917809,
      "grad_norm": 0.400574266910553,
      "learning_rate": 1.4432876712328767e-05,
      "loss": 0.0172,
      "step": 11370
    },
    {
      "epoch": 0.5196347031963471,
      "grad_norm": 0.9155896902084351,
      "learning_rate": 1.441917808219178e-05,
      "loss": 0.0194,
      "step": 11380
    },
    {
      "epoch": 0.5200913242009132,
      "grad_norm": 1.6613374948501587,
      "learning_rate": 1.4405479452054796e-05,
      "loss": 0.0279,
      "step": 11390
    },
    {
      "epoch": 0.5205479452054794,
      "grad_norm": 0.9006209373474121,
      "learning_rate": 1.4391780821917809e-05,
      "loss": 0.0316,
      "step": 11400
    },
    {
      "epoch": 0.5210045662100456,
      "grad_norm": 0.2067783772945404,
      "learning_rate": 1.4378082191780822e-05,
      "loss": 0.0276,
      "step": 11410
    },
    {
      "epoch": 0.5214611872146119,
      "grad_norm": 1.0196830034255981,
      "learning_rate": 1.4364383561643835e-05,
      "loss": 0.0295,
      "step": 11420
    },
    {
      "epoch": 0.5219178082191781,
      "grad_norm": 0.6898556351661682,
      "learning_rate": 1.435068493150685e-05,
      "loss": 0.0339,
      "step": 11430
    },
    {
      "epoch": 0.5223744292237443,
      "grad_norm": 0.8732392191886902,
      "learning_rate": 1.4336986301369864e-05,
      "loss": 0.0314,
      "step": 11440
    },
    {
      "epoch": 0.5228310502283106,
      "grad_norm": 0.2561597526073456,
      "learning_rate": 1.4323287671232877e-05,
      "loss": 0.0338,
      "step": 11450
    },
    {
      "epoch": 0.5232876712328767,
      "grad_norm": 0.7717390060424805,
      "learning_rate": 1.4309589041095891e-05,
      "loss": 0.0371,
      "step": 11460
    },
    {
      "epoch": 0.5237442922374429,
      "grad_norm": 0.9325017333030701,
      "learning_rate": 1.4295890410958904e-05,
      "loss": 0.0335,
      "step": 11470
    },
    {
      "epoch": 0.5242009132420091,
      "grad_norm": 1.520041823387146,
      "learning_rate": 1.4282191780821917e-05,
      "loss": 0.0216,
      "step": 11480
    },
    {
      "epoch": 0.5246575342465754,
      "grad_norm": 0.40263521671295166,
      "learning_rate": 1.4268493150684932e-05,
      "loss": 0.031,
      "step": 11490
    },
    {
      "epoch": 0.5251141552511416,
      "grad_norm": 0.8717712759971619,
      "learning_rate": 1.4254794520547946e-05,
      "loss": 0.0326,
      "step": 11500
    },
    {
      "epoch": 0.5255707762557078,
      "grad_norm": 3.003221035003662,
      "learning_rate": 1.424109589041096e-05,
      "loss": 0.0323,
      "step": 11510
    },
    {
      "epoch": 0.5260273972602739,
      "grad_norm": 1.048864483833313,
      "learning_rate": 1.4227397260273972e-05,
      "loss": 0.026,
      "step": 11520
    },
    {
      "epoch": 0.5264840182648401,
      "grad_norm": 0.5024330019950867,
      "learning_rate": 1.4213698630136987e-05,
      "loss": 0.0185,
      "step": 11530
    },
    {
      "epoch": 0.5269406392694064,
      "grad_norm": 0.5935339331626892,
      "learning_rate": 1.42e-05,
      "loss": 0.0216,
      "step": 11540
    },
    {
      "epoch": 0.5273972602739726,
      "grad_norm": 0.8469303846359253,
      "learning_rate": 1.4186301369863014e-05,
      "loss": 0.0251,
      "step": 11550
    },
    {
      "epoch": 0.5278538812785388,
      "grad_norm": 0.5421903133392334,
      "learning_rate": 1.4172602739726029e-05,
      "loss": 0.0291,
      "step": 11560
    },
    {
      "epoch": 0.528310502283105,
      "grad_norm": 0.3904058039188385,
      "learning_rate": 1.4158904109589042e-05,
      "loss": 0.0239,
      "step": 11570
    },
    {
      "epoch": 0.5287671232876713,
      "grad_norm": 0.47568872570991516,
      "learning_rate": 1.4145205479452054e-05,
      "loss": 0.0298,
      "step": 11580
    },
    {
      "epoch": 0.5292237442922374,
      "grad_norm": 0.366824746131897,
      "learning_rate": 1.4131506849315069e-05,
      "loss": 0.0334,
      "step": 11590
    },
    {
      "epoch": 0.5296803652968036,
      "grad_norm": 0.60154128074646,
      "learning_rate": 1.4117808219178082e-05,
      "loss": 0.0296,
      "step": 11600
    },
    {
      "epoch": 0.5301369863013699,
      "grad_norm": 0.34205061197280884,
      "learning_rate": 1.4104109589041096e-05,
      "loss": 0.0273,
      "step": 11610
    },
    {
      "epoch": 0.5305936073059361,
      "grad_norm": 0.38850703835487366,
      "learning_rate": 1.409041095890411e-05,
      "loss": 0.0292,
      "step": 11620
    },
    {
      "epoch": 0.5310502283105023,
      "grad_norm": 0.6454951167106628,
      "learning_rate": 1.4076712328767124e-05,
      "loss": 0.0315,
      "step": 11630
    },
    {
      "epoch": 0.5315068493150685,
      "grad_norm": 0.4158848524093628,
      "learning_rate": 1.4063013698630137e-05,
      "loss": 0.0327,
      "step": 11640
    },
    {
      "epoch": 0.5319634703196348,
      "grad_norm": 0.5765141844749451,
      "learning_rate": 1.404931506849315e-05,
      "loss": 0.0279,
      "step": 11650
    },
    {
      "epoch": 0.5324200913242009,
      "grad_norm": 0.5638694167137146,
      "learning_rate": 1.4035616438356166e-05,
      "loss": 0.0246,
      "step": 11660
    },
    {
      "epoch": 0.5328767123287671,
      "grad_norm": 0.7122138738632202,
      "learning_rate": 1.4021917808219179e-05,
      "loss": 0.0313,
      "step": 11670
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.6425865888595581,
      "learning_rate": 1.4008219178082192e-05,
      "loss": 0.018,
      "step": 11680
    },
    {
      "epoch": 0.5337899543378996,
      "grad_norm": 0.8411828875541687,
      "learning_rate": 1.3994520547945206e-05,
      "loss": 0.0299,
      "step": 11690
    },
    {
      "epoch": 0.5342465753424658,
      "grad_norm": 1.5232442617416382,
      "learning_rate": 1.398082191780822e-05,
      "loss": 0.0285,
      "step": 11700
    },
    {
      "epoch": 0.534703196347032,
      "grad_norm": 0.47950559854507446,
      "learning_rate": 1.3967123287671234e-05,
      "loss": 0.033,
      "step": 11710
    },
    {
      "epoch": 0.5351598173515981,
      "grad_norm": 0.27609995007514954,
      "learning_rate": 1.3953424657534247e-05,
      "loss": 0.0257,
      "step": 11720
    },
    {
      "epoch": 0.5356164383561643,
      "grad_norm": 0.7322826981544495,
      "learning_rate": 1.3939726027397261e-05,
      "loss": 0.0336,
      "step": 11730
    },
    {
      "epoch": 0.5360730593607306,
      "grad_norm": 1.306564211845398,
      "learning_rate": 1.3926027397260274e-05,
      "loss": 0.0304,
      "step": 11740
    },
    {
      "epoch": 0.5365296803652968,
      "grad_norm": 0.338979572057724,
      "learning_rate": 1.3912328767123287e-05,
      "loss": 0.0372,
      "step": 11750
    },
    {
      "epoch": 0.536986301369863,
      "grad_norm": 1.5207312107086182,
      "learning_rate": 1.3898630136986302e-05,
      "loss": 0.03,
      "step": 11760
    },
    {
      "epoch": 0.5374429223744293,
      "grad_norm": 0.9009897708892822,
      "learning_rate": 1.3884931506849316e-05,
      "loss": 0.0222,
      "step": 11770
    },
    {
      "epoch": 0.5378995433789955,
      "grad_norm": 0.6848528981208801,
      "learning_rate": 1.3871232876712329e-05,
      "loss": 0.0359,
      "step": 11780
    },
    {
      "epoch": 0.5383561643835616,
      "grad_norm": 0.9683523774147034,
      "learning_rate": 1.3857534246575344e-05,
      "loss": 0.0232,
      "step": 11790
    },
    {
      "epoch": 0.5388127853881278,
      "grad_norm": 0.8942880630493164,
      "learning_rate": 1.3843835616438356e-05,
      "loss": 0.0285,
      "step": 11800
    },
    {
      "epoch": 0.539269406392694,
      "grad_norm": 0.5015718936920166,
      "learning_rate": 1.383013698630137e-05,
      "loss": 0.0264,
      "step": 11810
    },
    {
      "epoch": 0.5397260273972603,
      "grad_norm": 0.6478370428085327,
      "learning_rate": 1.3816438356164384e-05,
      "loss": 0.0207,
      "step": 11820
    },
    {
      "epoch": 0.5401826484018265,
      "grad_norm": 0.5502506494522095,
      "learning_rate": 1.3802739726027398e-05,
      "loss": 0.022,
      "step": 11830
    },
    {
      "epoch": 0.5406392694063927,
      "grad_norm": 0.5345935225486755,
      "learning_rate": 1.3789041095890411e-05,
      "loss": 0.0292,
      "step": 11840
    },
    {
      "epoch": 0.541095890410959,
      "grad_norm": 0.6677759885787964,
      "learning_rate": 1.3775342465753424e-05,
      "loss": 0.0352,
      "step": 11850
    },
    {
      "epoch": 0.5415525114155251,
      "grad_norm": 0.6054534912109375,
      "learning_rate": 1.3761643835616439e-05,
      "loss": 0.028,
      "step": 11860
    },
    {
      "epoch": 0.5420091324200913,
      "grad_norm": 0.43173840641975403,
      "learning_rate": 1.3747945205479452e-05,
      "loss": 0.0277,
      "step": 11870
    },
    {
      "epoch": 0.5424657534246575,
      "grad_norm": 0.5229641795158386,
      "learning_rate": 1.3734246575342466e-05,
      "loss": 0.0223,
      "step": 11880
    },
    {
      "epoch": 0.5429223744292238,
      "grad_norm": 0.5605542659759521,
      "learning_rate": 1.372054794520548e-05,
      "loss": 0.0372,
      "step": 11890
    },
    {
      "epoch": 0.54337899543379,
      "grad_norm": 0.9172779321670532,
      "learning_rate": 1.3706849315068494e-05,
      "loss": 0.0248,
      "step": 11900
    },
    {
      "epoch": 0.5438356164383562,
      "grad_norm": 0.5259960889816284,
      "learning_rate": 1.3693150684931507e-05,
      "loss": 0.0166,
      "step": 11910
    },
    {
      "epoch": 0.5442922374429223,
      "grad_norm": 1.1991010904312134,
      "learning_rate": 1.367945205479452e-05,
      "loss": 0.0374,
      "step": 11920
    },
    {
      "epoch": 0.5447488584474885,
      "grad_norm": 0.48959556221961975,
      "learning_rate": 1.3665753424657536e-05,
      "loss": 0.0272,
      "step": 11930
    },
    {
      "epoch": 0.5452054794520548,
      "grad_norm": 0.724913477897644,
      "learning_rate": 1.3652054794520549e-05,
      "loss": 0.0339,
      "step": 11940
    },
    {
      "epoch": 0.545662100456621,
      "grad_norm": 0.36624449491500854,
      "learning_rate": 1.3638356164383561e-05,
      "loss": 0.0235,
      "step": 11950
    },
    {
      "epoch": 0.5461187214611872,
      "grad_norm": 0.7767555713653564,
      "learning_rate": 1.3624657534246576e-05,
      "loss": 0.0282,
      "step": 11960
    },
    {
      "epoch": 0.5465753424657535,
      "grad_norm": 2.106166362762451,
      "learning_rate": 1.3610958904109589e-05,
      "loss": 0.0274,
      "step": 11970
    },
    {
      "epoch": 0.5470319634703197,
      "grad_norm": 1.0696724653244019,
      "learning_rate": 1.3597260273972602e-05,
      "loss": 0.0237,
      "step": 11980
    },
    {
      "epoch": 0.5474885844748858,
      "grad_norm": 1.184145212173462,
      "learning_rate": 1.3583561643835618e-05,
      "loss": 0.0337,
      "step": 11990
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 0.5928221344947815,
      "learning_rate": 1.3569863013698631e-05,
      "loss": 0.0234,
      "step": 12000
    },
    {
      "epoch": 0.5484018264840183,
      "grad_norm": 0.719474732875824,
      "learning_rate": 1.3556164383561644e-05,
      "loss": 0.0279,
      "step": 12010
    },
    {
      "epoch": 0.5488584474885845,
      "grad_norm": 0.8197904229164124,
      "learning_rate": 1.3542465753424657e-05,
      "loss": 0.0334,
      "step": 12020
    },
    {
      "epoch": 0.5493150684931507,
      "grad_norm": 1.2330141067504883,
      "learning_rate": 1.3528767123287671e-05,
      "loss": 0.0419,
      "step": 12030
    },
    {
      "epoch": 0.5497716894977169,
      "grad_norm": 0.3344706892967224,
      "learning_rate": 1.3515068493150686e-05,
      "loss": 0.0258,
      "step": 12040
    },
    {
      "epoch": 0.5502283105022832,
      "grad_norm": 1.1161575317382812,
      "learning_rate": 1.3501369863013699e-05,
      "loss": 0.0286,
      "step": 12050
    },
    {
      "epoch": 0.5506849315068493,
      "grad_norm": 0.7694293856620789,
      "learning_rate": 1.3487671232876713e-05,
      "loss": 0.0212,
      "step": 12060
    },
    {
      "epoch": 0.5511415525114155,
      "grad_norm": 0.8000162243843079,
      "learning_rate": 1.3473972602739726e-05,
      "loss": 0.0251,
      "step": 12070
    },
    {
      "epoch": 0.5515981735159817,
      "grad_norm": 0.8896071910858154,
      "learning_rate": 1.3460273972602739e-05,
      "loss": 0.0226,
      "step": 12080
    },
    {
      "epoch": 0.552054794520548,
      "grad_norm": 1.029162049293518,
      "learning_rate": 1.3446575342465755e-05,
      "loss": 0.0328,
      "step": 12090
    },
    {
      "epoch": 0.5525114155251142,
      "grad_norm": 0.5824541449546814,
      "learning_rate": 1.3432876712328768e-05,
      "loss": 0.0279,
      "step": 12100
    },
    {
      "epoch": 0.5529680365296804,
      "grad_norm": 1.1713453531265259,
      "learning_rate": 1.3419178082191781e-05,
      "loss": 0.0314,
      "step": 12110
    },
    {
      "epoch": 0.5534246575342465,
      "grad_norm": 1.0263752937316895,
      "learning_rate": 1.3405479452054794e-05,
      "loss": 0.0394,
      "step": 12120
    },
    {
      "epoch": 0.5538812785388127,
      "grad_norm": 0.5755251049995422,
      "learning_rate": 1.3391780821917809e-05,
      "loss": 0.0284,
      "step": 12130
    },
    {
      "epoch": 0.554337899543379,
      "grad_norm": 0.6909914612770081,
      "learning_rate": 1.3378082191780821e-05,
      "loss": 0.0252,
      "step": 12140
    },
    {
      "epoch": 0.5547945205479452,
      "grad_norm": 0.8103480339050293,
      "learning_rate": 1.3364383561643836e-05,
      "loss": 0.0264,
      "step": 12150
    },
    {
      "epoch": 0.5552511415525114,
      "grad_norm": 0.6658095121383667,
      "learning_rate": 1.335068493150685e-05,
      "loss": 0.0315,
      "step": 12160
    },
    {
      "epoch": 0.5557077625570777,
      "grad_norm": 1.4918323755264282,
      "learning_rate": 1.3336986301369863e-05,
      "loss": 0.0202,
      "step": 12170
    },
    {
      "epoch": 0.5561643835616439,
      "grad_norm": 1.2350605726242065,
      "learning_rate": 1.3323287671232876e-05,
      "loss": 0.0319,
      "step": 12180
    },
    {
      "epoch": 0.55662100456621,
      "grad_norm": 0.6964091062545776,
      "learning_rate": 1.3309589041095891e-05,
      "loss": 0.028,
      "step": 12190
    },
    {
      "epoch": 0.5570776255707762,
      "grad_norm": 0.8561729192733765,
      "learning_rate": 1.3295890410958905e-05,
      "loss": 0.0257,
      "step": 12200
    },
    {
      "epoch": 0.5575342465753425,
      "grad_norm": 0.65873122215271,
      "learning_rate": 1.3282191780821918e-05,
      "loss": 0.0282,
      "step": 12210
    },
    {
      "epoch": 0.5579908675799087,
      "grad_norm": 0.7568768262863159,
      "learning_rate": 1.3268493150684931e-05,
      "loss": 0.0222,
      "step": 12220
    },
    {
      "epoch": 0.5584474885844749,
      "grad_norm": 0.6336814165115356,
      "learning_rate": 1.3254794520547946e-05,
      "loss": 0.0388,
      "step": 12230
    },
    {
      "epoch": 0.5589041095890411,
      "grad_norm": 0.8595716953277588,
      "learning_rate": 1.3241095890410959e-05,
      "loss": 0.0346,
      "step": 12240
    },
    {
      "epoch": 0.5593607305936074,
      "grad_norm": 0.4663134813308716,
      "learning_rate": 1.3227397260273972e-05,
      "loss": 0.0313,
      "step": 12250
    },
    {
      "epoch": 0.5598173515981735,
      "grad_norm": 0.34022173285484314,
      "learning_rate": 1.3213698630136988e-05,
      "loss": 0.032,
      "step": 12260
    },
    {
      "epoch": 0.5602739726027397,
      "grad_norm": 0.34975382685661316,
      "learning_rate": 1.32e-05,
      "loss": 0.0265,
      "step": 12270
    },
    {
      "epoch": 0.5607305936073059,
      "grad_norm": 0.8316414952278137,
      "learning_rate": 1.3186301369863014e-05,
      "loss": 0.0301,
      "step": 12280
    },
    {
      "epoch": 0.5611872146118722,
      "grad_norm": 1.1030741930007935,
      "learning_rate": 1.3172602739726028e-05,
      "loss": 0.0229,
      "step": 12290
    },
    {
      "epoch": 0.5616438356164384,
      "grad_norm": 0.5271957516670227,
      "learning_rate": 1.3158904109589041e-05,
      "loss": 0.0371,
      "step": 12300
    },
    {
      "epoch": 0.5621004566210046,
      "grad_norm": 0.8147308230400085,
      "learning_rate": 1.3145205479452056e-05,
      "loss": 0.0256,
      "step": 12310
    },
    {
      "epoch": 0.5625570776255707,
      "grad_norm": 0.5508854985237122,
      "learning_rate": 1.3131506849315068e-05,
      "loss": 0.0345,
      "step": 12320
    },
    {
      "epoch": 0.563013698630137,
      "grad_norm": 0.9340817928314209,
      "learning_rate": 1.3117808219178083e-05,
      "loss": 0.0312,
      "step": 12330
    },
    {
      "epoch": 0.5634703196347032,
      "grad_norm": 1.9557026624679565,
      "learning_rate": 1.3104109589041096e-05,
      "loss": 0.0269,
      "step": 12340
    },
    {
      "epoch": 0.5639269406392694,
      "grad_norm": 0.6139368414878845,
      "learning_rate": 1.3090410958904109e-05,
      "loss": 0.0326,
      "step": 12350
    },
    {
      "epoch": 0.5643835616438356,
      "grad_norm": 0.4644712805747986,
      "learning_rate": 1.3076712328767123e-05,
      "loss": 0.0191,
      "step": 12360
    },
    {
      "epoch": 0.5648401826484019,
      "grad_norm": 0.5159928798675537,
      "learning_rate": 1.3063013698630138e-05,
      "loss": 0.0214,
      "step": 12370
    },
    {
      "epoch": 0.5652968036529681,
      "grad_norm": 0.6588208675384521,
      "learning_rate": 1.304931506849315e-05,
      "loss": 0.0266,
      "step": 12380
    },
    {
      "epoch": 0.5657534246575342,
      "grad_norm": 0.4113977551460266,
      "learning_rate": 1.3035616438356165e-05,
      "loss": 0.0147,
      "step": 12390
    },
    {
      "epoch": 0.5662100456621004,
      "grad_norm": 0.6897862553596497,
      "learning_rate": 1.3021917808219178e-05,
      "loss": 0.0215,
      "step": 12400
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.4437747299671173,
      "learning_rate": 1.3008219178082191e-05,
      "loss": 0.028,
      "step": 12410
    },
    {
      "epoch": 0.5671232876712329,
      "grad_norm": 0.8189589977264404,
      "learning_rate": 1.2994520547945206e-05,
      "loss": 0.0196,
      "step": 12420
    },
    {
      "epoch": 0.5675799086757991,
      "grad_norm": 0.875078558921814,
      "learning_rate": 1.298082191780822e-05,
      "loss": 0.0346,
      "step": 12430
    },
    {
      "epoch": 0.5680365296803653,
      "grad_norm": 0.3493233919143677,
      "learning_rate": 1.2967123287671233e-05,
      "loss": 0.0287,
      "step": 12440
    },
    {
      "epoch": 0.5684931506849316,
      "grad_norm": 0.5205592513084412,
      "learning_rate": 1.2953424657534246e-05,
      "loss": 0.0195,
      "step": 12450
    },
    {
      "epoch": 0.5689497716894977,
      "grad_norm": 0.903963029384613,
      "learning_rate": 1.293972602739726e-05,
      "loss": 0.0212,
      "step": 12460
    },
    {
      "epoch": 0.5694063926940639,
      "grad_norm": 0.6737943887710571,
      "learning_rate": 1.2926027397260275e-05,
      "loss": 0.0263,
      "step": 12470
    },
    {
      "epoch": 0.5698630136986301,
      "grad_norm": 1.0195170640945435,
      "learning_rate": 1.2912328767123288e-05,
      "loss": 0.0251,
      "step": 12480
    },
    {
      "epoch": 0.5703196347031964,
      "grad_norm": 0.4445922374725342,
      "learning_rate": 1.2898630136986303e-05,
      "loss": 0.02,
      "step": 12490
    },
    {
      "epoch": 0.5707762557077626,
      "grad_norm": 0.26326659321784973,
      "learning_rate": 1.2884931506849316e-05,
      "loss": 0.0287,
      "step": 12500
    },
    {
      "epoch": 0.5712328767123288,
      "grad_norm": 0.9396044611930847,
      "learning_rate": 1.2871232876712328e-05,
      "loss": 0.0227,
      "step": 12510
    },
    {
      "epoch": 0.5716894977168949,
      "grad_norm": 0.985554039478302,
      "learning_rate": 1.2857534246575341e-05,
      "loss": 0.0257,
      "step": 12520
    },
    {
      "epoch": 0.5721461187214611,
      "grad_norm": 0.338105171918869,
      "learning_rate": 1.2843835616438358e-05,
      "loss": 0.0221,
      "step": 12530
    },
    {
      "epoch": 0.5726027397260274,
      "grad_norm": 0.9474761486053467,
      "learning_rate": 1.283013698630137e-05,
      "loss": 0.0236,
      "step": 12540
    },
    {
      "epoch": 0.5730593607305936,
      "grad_norm": 0.568449854850769,
      "learning_rate": 1.2816438356164383e-05,
      "loss": 0.0277,
      "step": 12550
    },
    {
      "epoch": 0.5735159817351598,
      "grad_norm": 1.1485602855682373,
      "learning_rate": 1.2802739726027398e-05,
      "loss": 0.0354,
      "step": 12560
    },
    {
      "epoch": 0.5739726027397261,
      "grad_norm": 0.6861159205436707,
      "learning_rate": 1.278904109589041e-05,
      "loss": 0.0233,
      "step": 12570
    },
    {
      "epoch": 0.5744292237442923,
      "grad_norm": 0.621948778629303,
      "learning_rate": 1.2775342465753425e-05,
      "loss": 0.0195,
      "step": 12580
    },
    {
      "epoch": 0.5748858447488584,
      "grad_norm": 0.4473279118537903,
      "learning_rate": 1.276164383561644e-05,
      "loss": 0.0342,
      "step": 12590
    },
    {
      "epoch": 0.5753424657534246,
      "grad_norm": 0.8415918946266174,
      "learning_rate": 1.2747945205479453e-05,
      "loss": 0.0302,
      "step": 12600
    },
    {
      "epoch": 0.5757990867579909,
      "grad_norm": 0.6476325988769531,
      "learning_rate": 1.2734246575342466e-05,
      "loss": 0.024,
      "step": 12610
    },
    {
      "epoch": 0.5762557077625571,
      "grad_norm": 0.5540661811828613,
      "learning_rate": 1.2720547945205479e-05,
      "loss": 0.0208,
      "step": 12620
    },
    {
      "epoch": 0.5767123287671233,
      "grad_norm": 0.8087280988693237,
      "learning_rate": 1.2706849315068493e-05,
      "loss": 0.0292,
      "step": 12630
    },
    {
      "epoch": 0.5771689497716895,
      "grad_norm": 1.4656671285629272,
      "learning_rate": 1.2693150684931508e-05,
      "loss": 0.0314,
      "step": 12640
    },
    {
      "epoch": 0.5776255707762558,
      "grad_norm": 0.8131570219993591,
      "learning_rate": 1.267945205479452e-05,
      "loss": 0.0396,
      "step": 12650
    },
    {
      "epoch": 0.5780821917808219,
      "grad_norm": 0.8532752990722656,
      "learning_rate": 1.2665753424657535e-05,
      "loss": 0.0298,
      "step": 12660
    },
    {
      "epoch": 0.5785388127853881,
      "grad_norm": 0.5962378978729248,
      "learning_rate": 1.2652054794520548e-05,
      "loss": 0.0334,
      "step": 12670
    },
    {
      "epoch": 0.5789954337899543,
      "grad_norm": 1.4503400325775146,
      "learning_rate": 1.2638356164383561e-05,
      "loss": 0.0179,
      "step": 12680
    },
    {
      "epoch": 0.5794520547945206,
      "grad_norm": 1.8561675548553467,
      "learning_rate": 1.2624657534246577e-05,
      "loss": 0.0323,
      "step": 12690
    },
    {
      "epoch": 0.5799086757990868,
      "grad_norm": 0.8789303302764893,
      "learning_rate": 1.261095890410959e-05,
      "loss": 0.0358,
      "step": 12700
    },
    {
      "epoch": 0.580365296803653,
      "grad_norm": 0.5110979676246643,
      "learning_rate": 1.2597260273972603e-05,
      "loss": 0.0227,
      "step": 12710
    },
    {
      "epoch": 0.5808219178082191,
      "grad_norm": 0.7872947454452515,
      "learning_rate": 1.2583561643835616e-05,
      "loss": 0.0229,
      "step": 12720
    },
    {
      "epoch": 0.5812785388127854,
      "grad_norm": 0.6189655065536499,
      "learning_rate": 1.256986301369863e-05,
      "loss": 0.0243,
      "step": 12730
    },
    {
      "epoch": 0.5817351598173516,
      "grad_norm": 0.36320072412490845,
      "learning_rate": 1.2556164383561645e-05,
      "loss": 0.0299,
      "step": 12740
    },
    {
      "epoch": 0.5821917808219178,
      "grad_norm": 0.5801365971565247,
      "learning_rate": 1.2542465753424658e-05,
      "loss": 0.0323,
      "step": 12750
    },
    {
      "epoch": 0.582648401826484,
      "grad_norm": 0.8397256731987,
      "learning_rate": 1.2528767123287672e-05,
      "loss": 0.0224,
      "step": 12760
    },
    {
      "epoch": 0.5831050228310503,
      "grad_norm": 0.6390312910079956,
      "learning_rate": 1.2515068493150685e-05,
      "loss": 0.025,
      "step": 12770
    },
    {
      "epoch": 0.5835616438356165,
      "grad_norm": 0.3183980882167816,
      "learning_rate": 1.2501369863013698e-05,
      "loss": 0.0204,
      "step": 12780
    },
    {
      "epoch": 0.5840182648401826,
      "grad_norm": 1.2342894077301025,
      "learning_rate": 1.2487671232876713e-05,
      "loss": 0.0272,
      "step": 12790
    },
    {
      "epoch": 0.5844748858447488,
      "grad_norm": 0.7700494527816772,
      "learning_rate": 1.2473972602739727e-05,
      "loss": 0.0243,
      "step": 12800
    },
    {
      "epoch": 0.584931506849315,
      "grad_norm": 0.45235973596572876,
      "learning_rate": 1.246027397260274e-05,
      "loss": 0.0313,
      "step": 12810
    },
    {
      "epoch": 0.5853881278538813,
      "grad_norm": 0.9109848141670227,
      "learning_rate": 1.2446575342465753e-05,
      "loss": 0.027,
      "step": 12820
    },
    {
      "epoch": 0.5858447488584475,
      "grad_norm": 0.8322257399559021,
      "learning_rate": 1.2432876712328768e-05,
      "loss": 0.0342,
      "step": 12830
    },
    {
      "epoch": 0.5863013698630137,
      "grad_norm": 0.6204583644866943,
      "learning_rate": 1.241917808219178e-05,
      "loss": 0.0263,
      "step": 12840
    },
    {
      "epoch": 0.58675799086758,
      "grad_norm": 0.6137658357620239,
      "learning_rate": 1.2405479452054795e-05,
      "loss": 0.0306,
      "step": 12850
    },
    {
      "epoch": 0.5872146118721461,
      "grad_norm": 0.5419483780860901,
      "learning_rate": 1.239178082191781e-05,
      "loss": 0.0269,
      "step": 12860
    },
    {
      "epoch": 0.5876712328767123,
      "grad_norm": 0.8258044123649597,
      "learning_rate": 1.2378082191780823e-05,
      "loss": 0.0278,
      "step": 12870
    },
    {
      "epoch": 0.5881278538812785,
      "grad_norm": 0.5227780342102051,
      "learning_rate": 1.2364383561643835e-05,
      "loss": 0.0319,
      "step": 12880
    },
    {
      "epoch": 0.5885844748858448,
      "grad_norm": 0.1651519387960434,
      "learning_rate": 1.235068493150685e-05,
      "loss": 0.0252,
      "step": 12890
    },
    {
      "epoch": 0.589041095890411,
      "grad_norm": 0.894990861415863,
      "learning_rate": 1.2336986301369863e-05,
      "loss": 0.0288,
      "step": 12900
    },
    {
      "epoch": 0.5894977168949772,
      "grad_norm": 0.4160439670085907,
      "learning_rate": 1.2323287671232877e-05,
      "loss": 0.042,
      "step": 12910
    },
    {
      "epoch": 0.5899543378995433,
      "grad_norm": 0.6872431039810181,
      "learning_rate": 1.230958904109589e-05,
      "loss": 0.0297,
      "step": 12920
    },
    {
      "epoch": 0.5904109589041096,
      "grad_norm": 0.8584662079811096,
      "learning_rate": 1.2295890410958905e-05,
      "loss": 0.0204,
      "step": 12930
    },
    {
      "epoch": 0.5908675799086758,
      "grad_norm": 0.5604633688926697,
      "learning_rate": 1.2282191780821918e-05,
      "loss": 0.0256,
      "step": 12940
    },
    {
      "epoch": 0.591324200913242,
      "grad_norm": 0.25730183720588684,
      "learning_rate": 1.226849315068493e-05,
      "loss": 0.0242,
      "step": 12950
    },
    {
      "epoch": 0.5917808219178082,
      "grad_norm": 0.8293687105178833,
      "learning_rate": 1.2254794520547947e-05,
      "loss": 0.0261,
      "step": 12960
    },
    {
      "epoch": 0.5922374429223745,
      "grad_norm": 0.6994448304176331,
      "learning_rate": 1.224109589041096e-05,
      "loss": 0.0254,
      "step": 12970
    },
    {
      "epoch": 0.5926940639269407,
      "grad_norm": 1.313433051109314,
      "learning_rate": 1.2227397260273973e-05,
      "loss": 0.0224,
      "step": 12980
    },
    {
      "epoch": 0.5931506849315068,
      "grad_norm": 0.5422999858856201,
      "learning_rate": 1.2213698630136987e-05,
      "loss": 0.0257,
      "step": 12990
    },
    {
      "epoch": 0.593607305936073,
      "grad_norm": 0.7832332849502563,
      "learning_rate": 1.22e-05,
      "loss": 0.0313,
      "step": 13000
    },
    {
      "epoch": 0.5940639269406393,
      "grad_norm": 0.03909368813037872,
      "learning_rate": 1.2186301369863013e-05,
      "loss": 0.019,
      "step": 13010
    },
    {
      "epoch": 0.5945205479452055,
      "grad_norm": 0.45831385254859924,
      "learning_rate": 1.2172602739726028e-05,
      "loss": 0.025,
      "step": 13020
    },
    {
      "epoch": 0.5949771689497717,
      "grad_norm": 0.6328061819076538,
      "learning_rate": 1.2158904109589042e-05,
      "loss": 0.0374,
      "step": 13030
    },
    {
      "epoch": 0.5954337899543379,
      "grad_norm": 0.6061350703239441,
      "learning_rate": 1.2145205479452055e-05,
      "loss": 0.0266,
      "step": 13040
    },
    {
      "epoch": 0.5958904109589042,
      "grad_norm": 0.23269164562225342,
      "learning_rate": 1.2131506849315068e-05,
      "loss": 0.024,
      "step": 13050
    },
    {
      "epoch": 0.5963470319634703,
      "grad_norm": 0.24855278432369232,
      "learning_rate": 1.2117808219178082e-05,
      "loss": 0.0227,
      "step": 13060
    },
    {
      "epoch": 0.5968036529680365,
      "grad_norm": 1.078769326210022,
      "learning_rate": 1.2104109589041097e-05,
      "loss": 0.0306,
      "step": 13070
    },
    {
      "epoch": 0.5972602739726027,
      "grad_norm": 1.7270876169204712,
      "learning_rate": 1.209041095890411e-05,
      "loss": 0.0218,
      "step": 13080
    },
    {
      "epoch": 0.597716894977169,
      "grad_norm": 0.4931570291519165,
      "learning_rate": 1.2076712328767124e-05,
      "loss": 0.0354,
      "step": 13090
    },
    {
      "epoch": 0.5981735159817352,
      "grad_norm": 0.5594464540481567,
      "learning_rate": 1.2063013698630137e-05,
      "loss": 0.0184,
      "step": 13100
    },
    {
      "epoch": 0.5986301369863014,
      "grad_norm": 0.461469829082489,
      "learning_rate": 1.204931506849315e-05,
      "loss": 0.0269,
      "step": 13110
    },
    {
      "epoch": 0.5990867579908675,
      "grad_norm": 0.568508505821228,
      "learning_rate": 1.2035616438356165e-05,
      "loss": 0.0232,
      "step": 13120
    },
    {
      "epoch": 0.5995433789954338,
      "grad_norm": 0.5666772127151489,
      "learning_rate": 1.202191780821918e-05,
      "loss": 0.0276,
      "step": 13130
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.7316057682037354,
      "learning_rate": 1.2008219178082192e-05,
      "loss": 0.0191,
      "step": 13140
    },
    {
      "epoch": 0.6004566210045662,
      "grad_norm": 0.6917864084243774,
      "learning_rate": 1.1994520547945205e-05,
      "loss": 0.0193,
      "step": 13150
    },
    {
      "epoch": 0.6009132420091324,
      "grad_norm": 0.9938563108444214,
      "learning_rate": 1.198082191780822e-05,
      "loss": 0.0286,
      "step": 13160
    },
    {
      "epoch": 0.6013698630136987,
      "grad_norm": 1.0228946208953857,
      "learning_rate": 1.1967123287671233e-05,
      "loss": 0.0274,
      "step": 13170
    },
    {
      "epoch": 0.6018264840182649,
      "grad_norm": 0.09463891386985779,
      "learning_rate": 1.1953424657534247e-05,
      "loss": 0.016,
      "step": 13180
    },
    {
      "epoch": 0.602283105022831,
      "grad_norm": 0.8216572403907776,
      "learning_rate": 1.1939726027397262e-05,
      "loss": 0.0281,
      "step": 13190
    },
    {
      "epoch": 0.6027397260273972,
      "grad_norm": 0.7219253778457642,
      "learning_rate": 1.1926027397260275e-05,
      "loss": 0.0272,
      "step": 13200
    },
    {
      "epoch": 0.6031963470319635,
      "grad_norm": 1.3912498950958252,
      "learning_rate": 1.1912328767123287e-05,
      "loss": 0.0257,
      "step": 13210
    },
    {
      "epoch": 0.6036529680365297,
      "grad_norm": 3.0169568061828613,
      "learning_rate": 1.18986301369863e-05,
      "loss": 0.0264,
      "step": 13220
    },
    {
      "epoch": 0.6041095890410959,
      "grad_norm": 0.4994506239891052,
      "learning_rate": 1.1884931506849317e-05,
      "loss": 0.0241,
      "step": 13230
    },
    {
      "epoch": 0.6045662100456621,
      "grad_norm": 0.4814358353614807,
      "learning_rate": 1.187123287671233e-05,
      "loss": 0.0274,
      "step": 13240
    },
    {
      "epoch": 0.6050228310502284,
      "grad_norm": 1.1338363885879517,
      "learning_rate": 1.1857534246575342e-05,
      "loss": 0.0357,
      "step": 13250
    },
    {
      "epoch": 0.6054794520547945,
      "grad_norm": 1.1561312675476074,
      "learning_rate": 1.1843835616438357e-05,
      "loss": 0.0278,
      "step": 13260
    },
    {
      "epoch": 0.6059360730593607,
      "grad_norm": 0.6777310967445374,
      "learning_rate": 1.183013698630137e-05,
      "loss": 0.0384,
      "step": 13270
    },
    {
      "epoch": 0.6063926940639269,
      "grad_norm": 1.04737389087677,
      "learning_rate": 1.1816438356164383e-05,
      "loss": 0.0261,
      "step": 13280
    },
    {
      "epoch": 0.6068493150684932,
      "grad_norm": 0.6490103602409363,
      "learning_rate": 1.1802739726027399e-05,
      "loss": 0.0285,
      "step": 13290
    },
    {
      "epoch": 0.6073059360730594,
      "grad_norm": 1.302112102508545,
      "learning_rate": 1.1789041095890412e-05,
      "loss": 0.0308,
      "step": 13300
    },
    {
      "epoch": 0.6077625570776256,
      "grad_norm": 0.727270245552063,
      "learning_rate": 1.1775342465753425e-05,
      "loss": 0.0309,
      "step": 13310
    },
    {
      "epoch": 0.6082191780821918,
      "grad_norm": 0.4644169211387634,
      "learning_rate": 1.1761643835616438e-05,
      "loss": 0.0291,
      "step": 13320
    },
    {
      "epoch": 0.608675799086758,
      "grad_norm": 0.6593525409698486,
      "learning_rate": 1.1747945205479452e-05,
      "loss": 0.0402,
      "step": 13330
    },
    {
      "epoch": 0.6091324200913242,
      "grad_norm": 0.5021959543228149,
      "learning_rate": 1.1734246575342467e-05,
      "loss": 0.0178,
      "step": 13340
    },
    {
      "epoch": 0.6095890410958904,
      "grad_norm": 0.5991101264953613,
      "learning_rate": 1.172054794520548e-05,
      "loss": 0.0201,
      "step": 13350
    },
    {
      "epoch": 0.6100456621004566,
      "grad_norm": 0.9310768842697144,
      "learning_rate": 1.1706849315068494e-05,
      "loss": 0.0283,
      "step": 13360
    },
    {
      "epoch": 0.6105022831050229,
      "grad_norm": 0.2758508026599884,
      "learning_rate": 1.1693150684931507e-05,
      "loss": 0.0164,
      "step": 13370
    },
    {
      "epoch": 0.6109589041095891,
      "grad_norm": 0.1868685930967331,
      "learning_rate": 1.167945205479452e-05,
      "loss": 0.0282,
      "step": 13380
    },
    {
      "epoch": 0.6114155251141552,
      "grad_norm": 1.9490387439727783,
      "learning_rate": 1.1665753424657535e-05,
      "loss": 0.0342,
      "step": 13390
    },
    {
      "epoch": 0.6118721461187214,
      "grad_norm": 0.7932485342025757,
      "learning_rate": 1.1652054794520549e-05,
      "loss": 0.0225,
      "step": 13400
    },
    {
      "epoch": 0.6123287671232877,
      "grad_norm": 1.2631350755691528,
      "learning_rate": 1.1638356164383562e-05,
      "loss": 0.0278,
      "step": 13410
    },
    {
      "epoch": 0.6127853881278539,
      "grad_norm": 0.7254375219345093,
      "learning_rate": 1.1624657534246575e-05,
      "loss": 0.0252,
      "step": 13420
    },
    {
      "epoch": 0.6132420091324201,
      "grad_norm": 0.2185502052307129,
      "learning_rate": 1.161095890410959e-05,
      "loss": 0.0244,
      "step": 13430
    },
    {
      "epoch": 0.6136986301369863,
      "grad_norm": 0.6698663830757141,
      "learning_rate": 1.1597260273972602e-05,
      "loss": 0.0236,
      "step": 13440
    },
    {
      "epoch": 0.6141552511415526,
      "grad_norm": 0.452655553817749,
      "learning_rate": 1.1583561643835617e-05,
      "loss": 0.0217,
      "step": 13450
    },
    {
      "epoch": 0.6146118721461187,
      "grad_norm": 1.1601247787475586,
      "learning_rate": 1.1569863013698631e-05,
      "loss": 0.0221,
      "step": 13460
    },
    {
      "epoch": 0.6150684931506849,
      "grad_norm": 0.9422977566719055,
      "learning_rate": 1.1556164383561644e-05,
      "loss": 0.0234,
      "step": 13470
    },
    {
      "epoch": 0.6155251141552511,
      "grad_norm": 0.5251849293708801,
      "learning_rate": 1.1542465753424657e-05,
      "loss": 0.0175,
      "step": 13480
    },
    {
      "epoch": 0.6159817351598174,
      "grad_norm": 0.5170539617538452,
      "learning_rate": 1.1528767123287672e-05,
      "loss": 0.0386,
      "step": 13490
    },
    {
      "epoch": 0.6164383561643836,
      "grad_norm": 0.7335573434829712,
      "learning_rate": 1.1515068493150686e-05,
      "loss": 0.0279,
      "step": 13500
    },
    {
      "epoch": 0.6168949771689498,
      "grad_norm": 0.4607776403427124,
      "learning_rate": 1.15013698630137e-05,
      "loss": 0.0285,
      "step": 13510
    },
    {
      "epoch": 0.617351598173516,
      "grad_norm": 0.5099015831947327,
      "learning_rate": 1.1487671232876712e-05,
      "loss": 0.0193,
      "step": 13520
    },
    {
      "epoch": 0.6178082191780822,
      "grad_norm": 0.4964334964752197,
      "learning_rate": 1.1473972602739727e-05,
      "loss": 0.0254,
      "step": 13530
    },
    {
      "epoch": 0.6182648401826484,
      "grad_norm": 0.36752310395240784,
      "learning_rate": 1.146027397260274e-05,
      "loss": 0.0257,
      "step": 13540
    },
    {
      "epoch": 0.6187214611872146,
      "grad_norm": 0.076518714427948,
      "learning_rate": 1.1446575342465752e-05,
      "loss": 0.0199,
      "step": 13550
    },
    {
      "epoch": 0.6191780821917808,
      "grad_norm": 0.4184512495994568,
      "learning_rate": 1.1432876712328769e-05,
      "loss": 0.0237,
      "step": 13560
    },
    {
      "epoch": 0.6196347031963471,
      "grad_norm": 0.6743447184562683,
      "learning_rate": 1.1419178082191782e-05,
      "loss": 0.0204,
      "step": 13570
    },
    {
      "epoch": 0.6200913242009133,
      "grad_norm": 0.8915464878082275,
      "learning_rate": 1.1405479452054794e-05,
      "loss": 0.0386,
      "step": 13580
    },
    {
      "epoch": 0.6205479452054794,
      "grad_norm": 0.9070093035697937,
      "learning_rate": 1.1391780821917807e-05,
      "loss": 0.0282,
      "step": 13590
    },
    {
      "epoch": 0.6210045662100456,
      "grad_norm": 1.210167407989502,
      "learning_rate": 1.1378082191780822e-05,
      "loss": 0.0243,
      "step": 13600
    },
    {
      "epoch": 0.6214611872146119,
      "grad_norm": 0.29050853848457336,
      "learning_rate": 1.1364383561643836e-05,
      "loss": 0.0256,
      "step": 13610
    },
    {
      "epoch": 0.6219178082191781,
      "grad_norm": 0.9281500577926636,
      "learning_rate": 1.135068493150685e-05,
      "loss": 0.0246,
      "step": 13620
    },
    {
      "epoch": 0.6223744292237443,
      "grad_norm": 0.38402822613716125,
      "learning_rate": 1.1336986301369864e-05,
      "loss": 0.0303,
      "step": 13630
    },
    {
      "epoch": 0.6228310502283105,
      "grad_norm": 0.23933912813663483,
      "learning_rate": 1.1323287671232877e-05,
      "loss": 0.0244,
      "step": 13640
    },
    {
      "epoch": 0.6232876712328768,
      "grad_norm": 1.5119132995605469,
      "learning_rate": 1.130958904109589e-05,
      "loss": 0.0268,
      "step": 13650
    },
    {
      "epoch": 0.6237442922374429,
      "grad_norm": 0.6669697761535645,
      "learning_rate": 1.1295890410958904e-05,
      "loss": 0.0299,
      "step": 13660
    },
    {
      "epoch": 0.6242009132420091,
      "grad_norm": 0.904870867729187,
      "learning_rate": 1.1282191780821919e-05,
      "loss": 0.0266,
      "step": 13670
    },
    {
      "epoch": 0.6246575342465753,
      "grad_norm": 1.6781703233718872,
      "learning_rate": 1.1268493150684932e-05,
      "loss": 0.0286,
      "step": 13680
    },
    {
      "epoch": 0.6251141552511416,
      "grad_norm": 0.701714813709259,
      "learning_rate": 1.1254794520547945e-05,
      "loss": 0.0289,
      "step": 13690
    },
    {
      "epoch": 0.6255707762557078,
      "grad_norm": 0.4281004071235657,
      "learning_rate": 1.124109589041096e-05,
      "loss": 0.0208,
      "step": 13700
    },
    {
      "epoch": 0.626027397260274,
      "grad_norm": 0.32294413447380066,
      "learning_rate": 1.1227397260273972e-05,
      "loss": 0.036,
      "step": 13710
    },
    {
      "epoch": 0.6264840182648402,
      "grad_norm": 0.5905538201332092,
      "learning_rate": 1.1213698630136987e-05,
      "loss": 0.019,
      "step": 13720
    },
    {
      "epoch": 0.6269406392694064,
      "grad_norm": 0.5069560408592224,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.029,
      "step": 13730
    },
    {
      "epoch": 0.6273972602739726,
      "grad_norm": 0.6259255409240723,
      "learning_rate": 1.1186301369863014e-05,
      "loss": 0.0225,
      "step": 13740
    },
    {
      "epoch": 0.6278538812785388,
      "grad_norm": 0.1730683594942093,
      "learning_rate": 1.1172602739726027e-05,
      "loss": 0.029,
      "step": 13750
    },
    {
      "epoch": 0.628310502283105,
      "grad_norm": 0.5171607732772827,
      "learning_rate": 1.1158904109589042e-05,
      "loss": 0.0327,
      "step": 13760
    },
    {
      "epoch": 0.6287671232876713,
      "grad_norm": 0.2657701373100281,
      "learning_rate": 1.1145205479452056e-05,
      "loss": 0.0285,
      "step": 13770
    },
    {
      "epoch": 0.6292237442922375,
      "grad_norm": 1.3207041025161743,
      "learning_rate": 1.1131506849315069e-05,
      "loss": 0.0241,
      "step": 13780
    },
    {
      "epoch": 0.6296803652968036,
      "grad_norm": 0.4282715618610382,
      "learning_rate": 1.1117808219178082e-05,
      "loss": 0.0173,
      "step": 13790
    },
    {
      "epoch": 0.6301369863013698,
      "grad_norm": 0.29362058639526367,
      "learning_rate": 1.1104109589041096e-05,
      "loss": 0.0235,
      "step": 13800
    },
    {
      "epoch": 0.630593607305936,
      "grad_norm": 1.3605914115905762,
      "learning_rate": 1.109041095890411e-05,
      "loss": 0.022,
      "step": 13810
    },
    {
      "epoch": 0.6310502283105023,
      "grad_norm": 1.0180997848510742,
      "learning_rate": 1.1076712328767122e-05,
      "loss": 0.0285,
      "step": 13820
    },
    {
      "epoch": 0.6315068493150685,
      "grad_norm": 0.7814504504203796,
      "learning_rate": 1.1063013698630138e-05,
      "loss": 0.0284,
      "step": 13830
    },
    {
      "epoch": 0.6319634703196347,
      "grad_norm": 0.652445912361145,
      "learning_rate": 1.1049315068493151e-05,
      "loss": 0.0219,
      "step": 13840
    },
    {
      "epoch": 0.632420091324201,
      "grad_norm": 0.8340928554534912,
      "learning_rate": 1.1035616438356164e-05,
      "loss": 0.0249,
      "step": 13850
    },
    {
      "epoch": 0.6328767123287671,
      "grad_norm": 1.5827339887619019,
      "learning_rate": 1.1021917808219179e-05,
      "loss": 0.0307,
      "step": 13860
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.25789570808410645,
      "learning_rate": 1.1008219178082192e-05,
      "loss": 0.0261,
      "step": 13870
    },
    {
      "epoch": 0.6337899543378995,
      "grad_norm": 0.4811283349990845,
      "learning_rate": 1.0994520547945206e-05,
      "loss": 0.0222,
      "step": 13880
    },
    {
      "epoch": 0.6342465753424658,
      "grad_norm": 0.6599250435829163,
      "learning_rate": 1.0980821917808219e-05,
      "loss": 0.0292,
      "step": 13890
    },
    {
      "epoch": 0.634703196347032,
      "grad_norm": 1.2380636930465698,
      "learning_rate": 1.0967123287671234e-05,
      "loss": 0.0305,
      "step": 13900
    },
    {
      "epoch": 0.6351598173515982,
      "grad_norm": 1.0418334007263184,
      "learning_rate": 1.0953424657534247e-05,
      "loss": 0.0198,
      "step": 13910
    },
    {
      "epoch": 0.6356164383561644,
      "grad_norm": 0.4739842116832733,
      "learning_rate": 1.093972602739726e-05,
      "loss": 0.0244,
      "step": 13920
    },
    {
      "epoch": 0.6360730593607306,
      "grad_norm": 0.16143009066581726,
      "learning_rate": 1.0926027397260274e-05,
      "loss": 0.0302,
      "step": 13930
    },
    {
      "epoch": 0.6365296803652968,
      "grad_norm": 0.6114454865455627,
      "learning_rate": 1.0912328767123289e-05,
      "loss": 0.0296,
      "step": 13940
    },
    {
      "epoch": 0.636986301369863,
      "grad_norm": 0.15606757998466492,
      "learning_rate": 1.0898630136986301e-05,
      "loss": 0.0243,
      "step": 13950
    },
    {
      "epoch": 0.6374429223744292,
      "grad_norm": 1.2479311227798462,
      "learning_rate": 1.0884931506849316e-05,
      "loss": 0.0371,
      "step": 13960
    },
    {
      "epoch": 0.6378995433789955,
      "grad_norm": 0.5578665733337402,
      "learning_rate": 1.0871232876712329e-05,
      "loss": 0.0228,
      "step": 13970
    },
    {
      "epoch": 0.6383561643835617,
      "grad_norm": 0.8055039048194885,
      "learning_rate": 1.0857534246575342e-05,
      "loss": 0.0247,
      "step": 13980
    },
    {
      "epoch": 0.6388127853881278,
      "grad_norm": 0.6317770481109619,
      "learning_rate": 1.0843835616438356e-05,
      "loss": 0.0238,
      "step": 13990
    },
    {
      "epoch": 0.639269406392694,
      "grad_norm": 0.5559584498405457,
      "learning_rate": 1.0830136986301371e-05,
      "loss": 0.0286,
      "step": 14000
    },
    {
      "epoch": 0.6397260273972603,
      "grad_norm": 0.5563886165618896,
      "learning_rate": 1.0816438356164384e-05,
      "loss": 0.0271,
      "step": 14010
    },
    {
      "epoch": 0.6401826484018265,
      "grad_norm": 0.7488645315170288,
      "learning_rate": 1.0802739726027397e-05,
      "loss": 0.0339,
      "step": 14020
    },
    {
      "epoch": 0.6406392694063927,
      "grad_norm": 0.8232294321060181,
      "learning_rate": 1.0789041095890411e-05,
      "loss": 0.0186,
      "step": 14030
    },
    {
      "epoch": 0.6410958904109589,
      "grad_norm": 0.5052440762519836,
      "learning_rate": 1.0775342465753424e-05,
      "loss": 0.0245,
      "step": 14040
    },
    {
      "epoch": 0.6415525114155252,
      "grad_norm": 0.8814342021942139,
      "learning_rate": 1.0761643835616439e-05,
      "loss": 0.0229,
      "step": 14050
    },
    {
      "epoch": 0.6420091324200913,
      "grad_norm": 0.33738282322883606,
      "learning_rate": 1.0747945205479453e-05,
      "loss": 0.0218,
      "step": 14060
    },
    {
      "epoch": 0.6424657534246575,
      "grad_norm": 0.35787293314933777,
      "learning_rate": 1.0734246575342466e-05,
      "loss": 0.0136,
      "step": 14070
    },
    {
      "epoch": 0.6429223744292237,
      "grad_norm": 0.362447589635849,
      "learning_rate": 1.0720547945205479e-05,
      "loss": 0.0309,
      "step": 14080
    },
    {
      "epoch": 0.64337899543379,
      "grad_norm": 0.5731841921806335,
      "learning_rate": 1.0706849315068492e-05,
      "loss": 0.0306,
      "step": 14090
    },
    {
      "epoch": 0.6438356164383562,
      "grad_norm": 0.7072535753250122,
      "learning_rate": 1.0693150684931508e-05,
      "loss": 0.019,
      "step": 14100
    },
    {
      "epoch": 0.6442922374429224,
      "grad_norm": 0.6771588921546936,
      "learning_rate": 1.0679452054794521e-05,
      "loss": 0.0225,
      "step": 14110
    },
    {
      "epoch": 0.6447488584474886,
      "grad_norm": 0.20591461658477783,
      "learning_rate": 1.0665753424657534e-05,
      "loss": 0.0317,
      "step": 14120
    },
    {
      "epoch": 0.6452054794520548,
      "grad_norm": 0.8721083998680115,
      "learning_rate": 1.0652054794520549e-05,
      "loss": 0.0352,
      "step": 14130
    },
    {
      "epoch": 0.645662100456621,
      "grad_norm": 0.8470759391784668,
      "learning_rate": 1.0638356164383561e-05,
      "loss": 0.0296,
      "step": 14140
    },
    {
      "epoch": 0.6461187214611872,
      "grad_norm": 3.591296434402466,
      "learning_rate": 1.0624657534246576e-05,
      "loss": 0.0253,
      "step": 14150
    },
    {
      "epoch": 0.6465753424657534,
      "grad_norm": 0.7238916754722595,
      "learning_rate": 1.0612328767123288e-05,
      "loss": 0.0362,
      "step": 14160
    },
    {
      "epoch": 0.6470319634703197,
      "grad_norm": 0.8648175001144409,
      "learning_rate": 1.0598630136986301e-05,
      "loss": 0.0215,
      "step": 14170
    },
    {
      "epoch": 0.6474885844748859,
      "grad_norm": 0.853956937789917,
      "learning_rate": 1.0584931506849316e-05,
      "loss": 0.0255,
      "step": 14180
    },
    {
      "epoch": 0.647945205479452,
      "grad_norm": 0.6366683840751648,
      "learning_rate": 1.0571232876712328e-05,
      "loss": 0.0228,
      "step": 14190
    },
    {
      "epoch": 0.6484018264840182,
      "grad_norm": 0.45821666717529297,
      "learning_rate": 1.0557534246575343e-05,
      "loss": 0.0254,
      "step": 14200
    },
    {
      "epoch": 0.6488584474885845,
      "grad_norm": 1.072752594947815,
      "learning_rate": 1.0543835616438358e-05,
      "loss": 0.0232,
      "step": 14210
    },
    {
      "epoch": 0.6493150684931507,
      "grad_norm": 0.7091348171234131,
      "learning_rate": 1.053013698630137e-05,
      "loss": 0.0237,
      "step": 14220
    },
    {
      "epoch": 0.6497716894977169,
      "grad_norm": 0.4740678071975708,
      "learning_rate": 1.0516438356164383e-05,
      "loss": 0.0255,
      "step": 14230
    },
    {
      "epoch": 0.6502283105022831,
      "grad_norm": 0.5365645289421082,
      "learning_rate": 1.0502739726027398e-05,
      "loss": 0.025,
      "step": 14240
    },
    {
      "epoch": 0.6506849315068494,
      "grad_norm": 1.1518425941467285,
      "learning_rate": 1.048904109589041e-05,
      "loss": 0.0171,
      "step": 14250
    },
    {
      "epoch": 0.6511415525114155,
      "grad_norm": 0.5501025915145874,
      "learning_rate": 1.0475342465753425e-05,
      "loss": 0.0214,
      "step": 14260
    },
    {
      "epoch": 0.6515981735159817,
      "grad_norm": 0.5811665654182434,
      "learning_rate": 1.0461643835616438e-05,
      "loss": 0.0187,
      "step": 14270
    },
    {
      "epoch": 0.6520547945205479,
      "grad_norm": 0.8152409195899963,
      "learning_rate": 1.0447945205479453e-05,
      "loss": 0.0275,
      "step": 14280
    },
    {
      "epoch": 0.6525114155251142,
      "grad_norm": 1.1588629484176636,
      "learning_rate": 1.0434246575342466e-05,
      "loss": 0.0212,
      "step": 14290
    },
    {
      "epoch": 0.6529680365296804,
      "grad_norm": 0.37325742840766907,
      "learning_rate": 1.0420547945205479e-05,
      "loss": 0.0184,
      "step": 14300
    },
    {
      "epoch": 0.6534246575342466,
      "grad_norm": 0.7016029357910156,
      "learning_rate": 1.0406849315068495e-05,
      "loss": 0.0225,
      "step": 14310
    },
    {
      "epoch": 0.6538812785388128,
      "grad_norm": 0.3077682852745056,
      "learning_rate": 1.0393150684931508e-05,
      "loss": 0.0173,
      "step": 14320
    },
    {
      "epoch": 0.654337899543379,
      "grad_norm": 0.9326538443565369,
      "learning_rate": 1.037945205479452e-05,
      "loss": 0.022,
      "step": 14330
    },
    {
      "epoch": 0.6547945205479452,
      "grad_norm": 0.8525338172912598,
      "learning_rate": 1.0365753424657535e-05,
      "loss": 0.0213,
      "step": 14340
    },
    {
      "epoch": 0.6552511415525114,
      "grad_norm": 0.7378880381584167,
      "learning_rate": 1.0352054794520548e-05,
      "loss": 0.0198,
      "step": 14350
    },
    {
      "epoch": 0.6557077625570776,
      "grad_norm": 0.3543720841407776,
      "learning_rate": 1.0338356164383561e-05,
      "loss": 0.0173,
      "step": 14360
    },
    {
      "epoch": 0.6561643835616439,
      "grad_norm": 0.39581286907196045,
      "learning_rate": 1.0324657534246575e-05,
      "loss": 0.0207,
      "step": 14370
    },
    {
      "epoch": 0.6566210045662101,
      "grad_norm": 0.8112215995788574,
      "learning_rate": 1.031095890410959e-05,
      "loss": 0.0224,
      "step": 14380
    },
    {
      "epoch": 0.6570776255707762,
      "grad_norm": 1.2367616891860962,
      "learning_rate": 1.0297260273972603e-05,
      "loss": 0.0277,
      "step": 14390
    },
    {
      "epoch": 0.6575342465753424,
      "grad_norm": 0.064546138048172,
      "learning_rate": 1.0283561643835616e-05,
      "loss": 0.021,
      "step": 14400
    },
    {
      "epoch": 0.6579908675799087,
      "grad_norm": 0.31273311376571655,
      "learning_rate": 1.026986301369863e-05,
      "loss": 0.0288,
      "step": 14410
    },
    {
      "epoch": 0.6584474885844749,
      "grad_norm": 0.6702728271484375,
      "learning_rate": 1.0256164383561645e-05,
      "loss": 0.0199,
      "step": 14420
    },
    {
      "epoch": 0.6589041095890411,
      "grad_norm": 0.5032932162284851,
      "learning_rate": 1.0242465753424658e-05,
      "loss": 0.0359,
      "step": 14430
    },
    {
      "epoch": 0.6593607305936073,
      "grad_norm": 0.9562155604362488,
      "learning_rate": 1.0228767123287672e-05,
      "loss": 0.0292,
      "step": 14440
    },
    {
      "epoch": 0.6598173515981736,
      "grad_norm": 2.1125102043151855,
      "learning_rate": 1.0215068493150685e-05,
      "loss": 0.027,
      "step": 14450
    },
    {
      "epoch": 0.6602739726027397,
      "grad_norm": 0.6194930076599121,
      "learning_rate": 1.0201369863013698e-05,
      "loss": 0.04,
      "step": 14460
    },
    {
      "epoch": 0.6607305936073059,
      "grad_norm": 0.48067519068717957,
      "learning_rate": 1.0187671232876713e-05,
      "loss": 0.0181,
      "step": 14470
    },
    {
      "epoch": 0.6611872146118721,
      "grad_norm": 0.7111128568649292,
      "learning_rate": 1.0173972602739727e-05,
      "loss": 0.0275,
      "step": 14480
    },
    {
      "epoch": 0.6616438356164384,
      "grad_norm": 1.0506078004837036,
      "learning_rate": 1.016027397260274e-05,
      "loss": 0.0298,
      "step": 14490
    },
    {
      "epoch": 0.6621004566210046,
      "grad_norm": 1.4544085264205933,
      "learning_rate": 1.0146575342465753e-05,
      "loss": 0.0296,
      "step": 14500
    },
    {
      "epoch": 0.6625570776255708,
      "grad_norm": 1.0138486623764038,
      "learning_rate": 1.0132876712328768e-05,
      "loss": 0.024,
      "step": 14510
    },
    {
      "epoch": 0.663013698630137,
      "grad_norm": 0.1544160693883896,
      "learning_rate": 1.011917808219178e-05,
      "loss": 0.0195,
      "step": 14520
    },
    {
      "epoch": 0.6634703196347032,
      "grad_norm": 0.9541894197463989,
      "learning_rate": 1.0105479452054795e-05,
      "loss": 0.0322,
      "step": 14530
    },
    {
      "epoch": 0.6639269406392694,
      "grad_norm": 1.6240776777267456,
      "learning_rate": 1.009178082191781e-05,
      "loss": 0.0286,
      "step": 14540
    },
    {
      "epoch": 0.6643835616438356,
      "grad_norm": 0.41805946826934814,
      "learning_rate": 1.0078082191780822e-05,
      "loss": 0.0266,
      "step": 14550
    },
    {
      "epoch": 0.6648401826484018,
      "grad_norm": 0.831250786781311,
      "learning_rate": 1.0064383561643835e-05,
      "loss": 0.0208,
      "step": 14560
    },
    {
      "epoch": 0.6652968036529681,
      "grad_norm": 0.5415608882904053,
      "learning_rate": 1.0050684931506848e-05,
      "loss": 0.0204,
      "step": 14570
    },
    {
      "epoch": 0.6657534246575343,
      "grad_norm": 0.6372570991516113,
      "learning_rate": 1.0036986301369865e-05,
      "loss": 0.0345,
      "step": 14580
    },
    {
      "epoch": 0.6662100456621004,
      "grad_norm": 0.29094845056533813,
      "learning_rate": 1.0023287671232877e-05,
      "loss": 0.0198,
      "step": 14590
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.521453320980072,
      "learning_rate": 1.000958904109589e-05,
      "loss": 0.0141,
      "step": 14600
    },
    {
      "epoch": 0.6671232876712329,
      "grad_norm": 0.47197097539901733,
      "learning_rate": 9.995890410958905e-06,
      "loss": 0.0291,
      "step": 14610
    },
    {
      "epoch": 0.6675799086757991,
      "grad_norm": 0.4876622259616852,
      "learning_rate": 9.982191780821918e-06,
      "loss": 0.0296,
      "step": 14620
    },
    {
      "epoch": 0.6680365296803653,
      "grad_norm": 0.29899123311042786,
      "learning_rate": 9.96849315068493e-06,
      "loss": 0.0284,
      "step": 14630
    },
    {
      "epoch": 0.6684931506849315,
      "grad_norm": 0.9191572666168213,
      "learning_rate": 9.954794520547947e-06,
      "loss": 0.0355,
      "step": 14640
    },
    {
      "epoch": 0.6689497716894978,
      "grad_norm": 0.8759658932685852,
      "learning_rate": 9.94109589041096e-06,
      "loss": 0.0242,
      "step": 14650
    },
    {
      "epoch": 0.6694063926940639,
      "grad_norm": 0.8589367270469666,
      "learning_rate": 9.927397260273973e-06,
      "loss": 0.023,
      "step": 14660
    },
    {
      "epoch": 0.6698630136986301,
      "grad_norm": 0.6386731863021851,
      "learning_rate": 9.913698630136986e-06,
      "loss": 0.0248,
      "step": 14670
    },
    {
      "epoch": 0.6703196347031963,
      "grad_norm": 0.6955627202987671,
      "learning_rate": 9.9e-06,
      "loss": 0.0206,
      "step": 14680
    },
    {
      "epoch": 0.6707762557077626,
      "grad_norm": 2.0732805728912354,
      "learning_rate": 9.886301369863015e-06,
      "loss": 0.0185,
      "step": 14690
    },
    {
      "epoch": 0.6712328767123288,
      "grad_norm": 0.6474383473396301,
      "learning_rate": 9.872602739726028e-06,
      "loss": 0.034,
      "step": 14700
    },
    {
      "epoch": 0.671689497716895,
      "grad_norm": 0.6529400944709778,
      "learning_rate": 9.858904109589042e-06,
      "loss": 0.0245,
      "step": 14710
    },
    {
      "epoch": 0.6721461187214612,
      "grad_norm": 0.6248375177383423,
      "learning_rate": 9.845205479452055e-06,
      "loss": 0.0263,
      "step": 14720
    },
    {
      "epoch": 0.6726027397260274,
      "grad_norm": 0.7425717711448669,
      "learning_rate": 9.831506849315068e-06,
      "loss": 0.0337,
      "step": 14730
    },
    {
      "epoch": 0.6730593607305936,
      "grad_norm": 0.3852536380290985,
      "learning_rate": 9.817808219178082e-06,
      "loss": 0.023,
      "step": 14740
    },
    {
      "epoch": 0.6735159817351598,
      "grad_norm": 0.30719950795173645,
      "learning_rate": 9.804109589041097e-06,
      "loss": 0.0192,
      "step": 14750
    },
    {
      "epoch": 0.673972602739726,
      "grad_norm": 0.44015026092529297,
      "learning_rate": 9.79041095890411e-06,
      "loss": 0.0187,
      "step": 14760
    },
    {
      "epoch": 0.6744292237442923,
      "grad_norm": 0.4633845090866089,
      "learning_rate": 9.776712328767123e-06,
      "loss": 0.0263,
      "step": 14770
    },
    {
      "epoch": 0.6748858447488585,
      "grad_norm": 1.3743689060211182,
      "learning_rate": 9.763013698630137e-06,
      "loss": 0.028,
      "step": 14780
    },
    {
      "epoch": 0.6753424657534246,
      "grad_norm": 0.3389835059642792,
      "learning_rate": 9.74931506849315e-06,
      "loss": 0.0246,
      "step": 14790
    },
    {
      "epoch": 0.6757990867579908,
      "grad_norm": 1.176101803779602,
      "learning_rate": 9.735616438356165e-06,
      "loss": 0.021,
      "step": 14800
    },
    {
      "epoch": 0.6762557077625571,
      "grad_norm": 0.459810346364975,
      "learning_rate": 9.72191780821918e-06,
      "loss": 0.0154,
      "step": 14810
    },
    {
      "epoch": 0.6767123287671233,
      "grad_norm": 1.4074101448059082,
      "learning_rate": 9.708219178082192e-06,
      "loss": 0.0211,
      "step": 14820
    },
    {
      "epoch": 0.6771689497716895,
      "grad_norm": 0.5612333416938782,
      "learning_rate": 9.694520547945205e-06,
      "loss": 0.015,
      "step": 14830
    },
    {
      "epoch": 0.6776255707762557,
      "grad_norm": 0.24502521753311157,
      "learning_rate": 9.68082191780822e-06,
      "loss": 0.0275,
      "step": 14840
    },
    {
      "epoch": 0.678082191780822,
      "grad_norm": 0.7528285384178162,
      "learning_rate": 9.667123287671234e-06,
      "loss": 0.0326,
      "step": 14850
    },
    {
      "epoch": 0.6785388127853881,
      "grad_norm": 0.6426411271095276,
      "learning_rate": 9.653424657534247e-06,
      "loss": 0.0249,
      "step": 14860
    },
    {
      "epoch": 0.6789954337899543,
      "grad_norm": 0.2254263311624527,
      "learning_rate": 9.63972602739726e-06,
      "loss": 0.0255,
      "step": 14870
    },
    {
      "epoch": 0.6794520547945205,
      "grad_norm": 0.946602463722229,
      "learning_rate": 9.626027397260275e-06,
      "loss": 0.0147,
      "step": 14880
    },
    {
      "epoch": 0.6799086757990868,
      "grad_norm": 0.4049622118473053,
      "learning_rate": 9.612328767123287e-06,
      "loss": 0.0236,
      "step": 14890
    },
    {
      "epoch": 0.680365296803653,
      "grad_norm": 0.3491150736808777,
      "learning_rate": 9.5986301369863e-06,
      "loss": 0.0221,
      "step": 14900
    },
    {
      "epoch": 0.6808219178082192,
      "grad_norm": 0.15419629216194153,
      "learning_rate": 9.584931506849317e-06,
      "loss": 0.0237,
      "step": 14910
    },
    {
      "epoch": 0.6812785388127854,
      "grad_norm": 0.15875917673110962,
      "learning_rate": 9.57123287671233e-06,
      "loss": 0.0236,
      "step": 14920
    },
    {
      "epoch": 0.6817351598173516,
      "grad_norm": 1.056433081626892,
      "learning_rate": 9.557534246575342e-06,
      "loss": 0.0319,
      "step": 14930
    },
    {
      "epoch": 0.6821917808219178,
      "grad_norm": 0.6581543684005737,
      "learning_rate": 9.543835616438357e-06,
      "loss": 0.0245,
      "step": 14940
    },
    {
      "epoch": 0.682648401826484,
      "grad_norm": 0.9285590052604675,
      "learning_rate": 9.53013698630137e-06,
      "loss": 0.0253,
      "step": 14950
    },
    {
      "epoch": 0.6831050228310502,
      "grad_norm": 0.45992669463157654,
      "learning_rate": 9.516438356164384e-06,
      "loss": 0.0281,
      "step": 14960
    },
    {
      "epoch": 0.6835616438356165,
      "grad_norm": 0.40504753589630127,
      "learning_rate": 9.502739726027397e-06,
      "loss": 0.0267,
      "step": 14970
    },
    {
      "epoch": 0.6840182648401827,
      "grad_norm": 0.4367597699165344,
      "learning_rate": 9.489041095890412e-06,
      "loss": 0.0376,
      "step": 14980
    },
    {
      "epoch": 0.6844748858447488,
      "grad_norm": 1.4909672737121582,
      "learning_rate": 9.475342465753425e-06,
      "loss": 0.0346,
      "step": 14990
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 1.1040453910827637,
      "learning_rate": 9.461643835616438e-06,
      "loss": 0.0264,
      "step": 15000
    },
    {
      "epoch": 0.6853881278538813,
      "grad_norm": 1.3819715976715088,
      "learning_rate": 9.447945205479452e-06,
      "loss": 0.0254,
      "step": 15010
    },
    {
      "epoch": 0.6858447488584475,
      "grad_norm": 0.9337833523750305,
      "learning_rate": 9.434246575342467e-06,
      "loss": 0.0238,
      "step": 15020
    },
    {
      "epoch": 0.6863013698630137,
      "grad_norm": 0.7023641467094421,
      "learning_rate": 9.42054794520548e-06,
      "loss": 0.0221,
      "step": 15030
    },
    {
      "epoch": 0.6867579908675799,
      "grad_norm": 0.6277076005935669,
      "learning_rate": 9.406849315068494e-06,
      "loss": 0.0205,
      "step": 15040
    },
    {
      "epoch": 0.6872146118721462,
      "grad_norm": 0.5947481393814087,
      "learning_rate": 9.393150684931507e-06,
      "loss": 0.0275,
      "step": 15050
    },
    {
      "epoch": 0.6876712328767123,
      "grad_norm": 0.3959766626358032,
      "learning_rate": 9.37945205479452e-06,
      "loss": 0.0165,
      "step": 15060
    },
    {
      "epoch": 0.6881278538812785,
      "grad_norm": 0.6393659710884094,
      "learning_rate": 9.365753424657535e-06,
      "loss": 0.023,
      "step": 15070
    },
    {
      "epoch": 0.6885844748858447,
      "grad_norm": 0.13130588829517365,
      "learning_rate": 9.352054794520549e-06,
      "loss": 0.0295,
      "step": 15080
    },
    {
      "epoch": 0.689041095890411,
      "grad_norm": 0.559079110622406,
      "learning_rate": 9.338356164383562e-06,
      "loss": 0.025,
      "step": 15090
    },
    {
      "epoch": 0.6894977168949772,
      "grad_norm": 0.4399394392967224,
      "learning_rate": 9.324657534246575e-06,
      "loss": 0.0256,
      "step": 15100
    },
    {
      "epoch": 0.6899543378995434,
      "grad_norm": 0.6285720467567444,
      "learning_rate": 9.31095890410959e-06,
      "loss": 0.0237,
      "step": 15110
    },
    {
      "epoch": 0.6904109589041096,
      "grad_norm": 0.7562563419342041,
      "learning_rate": 9.297260273972602e-06,
      "loss": 0.0165,
      "step": 15120
    },
    {
      "epoch": 0.6908675799086758,
      "grad_norm": 0.9407088756561279,
      "learning_rate": 9.283561643835617e-06,
      "loss": 0.0296,
      "step": 15130
    },
    {
      "epoch": 0.691324200913242,
      "grad_norm": 1.2567548751831055,
      "learning_rate": 9.26986301369863e-06,
      "loss": 0.0249,
      "step": 15140
    },
    {
      "epoch": 0.6917808219178082,
      "grad_norm": 0.4868007004261017,
      "learning_rate": 9.256164383561644e-06,
      "loss": 0.0243,
      "step": 15150
    },
    {
      "epoch": 0.6922374429223744,
      "grad_norm": 0.06255145370960236,
      "learning_rate": 9.242465753424657e-06,
      "loss": 0.0263,
      "step": 15160
    },
    {
      "epoch": 0.6926940639269407,
      "grad_norm": 0.7777374982833862,
      "learning_rate": 9.22876712328767e-06,
      "loss": 0.0297,
      "step": 15170
    },
    {
      "epoch": 0.6931506849315069,
      "grad_norm": 1.2534395456314087,
      "learning_rate": 9.215068493150686e-06,
      "loss": 0.0362,
      "step": 15180
    },
    {
      "epoch": 0.693607305936073,
      "grad_norm": 0.7066097855567932,
      "learning_rate": 9.2013698630137e-06,
      "loss": 0.0235,
      "step": 15190
    },
    {
      "epoch": 0.6940639269406392,
      "grad_norm": 0.5862346887588501,
      "learning_rate": 9.187671232876712e-06,
      "loss": 0.0262,
      "step": 15200
    },
    {
      "epoch": 0.6945205479452055,
      "grad_norm": 0.5177844762802124,
      "learning_rate": 9.173972602739727e-06,
      "loss": 0.0157,
      "step": 15210
    },
    {
      "epoch": 0.6949771689497717,
      "grad_norm": 1.358664631843567,
      "learning_rate": 9.16027397260274e-06,
      "loss": 0.0354,
      "step": 15220
    },
    {
      "epoch": 0.6954337899543379,
      "grad_norm": 1.2521069049835205,
      "learning_rate": 9.146575342465754e-06,
      "loss": 0.0173,
      "step": 15230
    },
    {
      "epoch": 0.6958904109589041,
      "grad_norm": 0.5164827704429626,
      "learning_rate": 9.132876712328767e-06,
      "loss": 0.0185,
      "step": 15240
    },
    {
      "epoch": 0.6963470319634704,
      "grad_norm": 0.647412121295929,
      "learning_rate": 9.119178082191782e-06,
      "loss": 0.026,
      "step": 15250
    },
    {
      "epoch": 0.6968036529680365,
      "grad_norm": 0.4580824375152588,
      "learning_rate": 9.105479452054794e-06,
      "loss": 0.0197,
      "step": 15260
    },
    {
      "epoch": 0.6972602739726027,
      "grad_norm": 0.7484040856361389,
      "learning_rate": 9.091780821917807e-06,
      "loss": 0.0264,
      "step": 15270
    },
    {
      "epoch": 0.6977168949771689,
      "grad_norm": 0.4005078673362732,
      "learning_rate": 9.078082191780822e-06,
      "loss": 0.0256,
      "step": 15280
    },
    {
      "epoch": 0.6981735159817352,
      "grad_norm": 0.13838976621627808,
      "learning_rate": 9.064383561643836e-06,
      "loss": 0.0208,
      "step": 15290
    },
    {
      "epoch": 0.6986301369863014,
      "grad_norm": 0.7365982532501221,
      "learning_rate": 9.05068493150685e-06,
      "loss": 0.0277,
      "step": 15300
    },
    {
      "epoch": 0.6990867579908676,
      "grad_norm": 0.46380865573883057,
      "learning_rate": 9.036986301369864e-06,
      "loss": 0.0308,
      "step": 15310
    },
    {
      "epoch": 0.6995433789954338,
      "grad_norm": 0.9552140235900879,
      "learning_rate": 9.023287671232877e-06,
      "loss": 0.0399,
      "step": 15320
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.5740294456481934,
      "learning_rate": 9.00958904109589e-06,
      "loss": 0.0198,
      "step": 15330
    },
    {
      "epoch": 0.7004566210045662,
      "grad_norm": 0.5593771934509277,
      "learning_rate": 8.995890410958904e-06,
      "loss": 0.0206,
      "step": 15340
    },
    {
      "epoch": 0.7009132420091324,
      "grad_norm": 0.8079227209091187,
      "learning_rate": 8.982191780821919e-06,
      "loss": 0.0244,
      "step": 15350
    },
    {
      "epoch": 0.7013698630136986,
      "grad_norm": 0.6001431941986084,
      "learning_rate": 8.968493150684932e-06,
      "loss": 0.0188,
      "step": 15360
    },
    {
      "epoch": 0.7018264840182649,
      "grad_norm": 0.7945672869682312,
      "learning_rate": 8.954794520547945e-06,
      "loss": 0.0224,
      "step": 15370
    },
    {
      "epoch": 0.7022831050228311,
      "grad_norm": 0.48963961005210876,
      "learning_rate": 8.94109589041096e-06,
      "loss": 0.0238,
      "step": 15380
    },
    {
      "epoch": 0.7027397260273973,
      "grad_norm": 0.864486813545227,
      "learning_rate": 8.927397260273972e-06,
      "loss": 0.023,
      "step": 15390
    },
    {
      "epoch": 0.7031963470319634,
      "grad_norm": 0.03653311729431152,
      "learning_rate": 8.913698630136987e-06,
      "loss": 0.0193,
      "step": 15400
    },
    {
      "epoch": 0.7036529680365297,
      "grad_norm": 0.4083620011806488,
      "learning_rate": 8.900000000000001e-06,
      "loss": 0.0284,
      "step": 15410
    },
    {
      "epoch": 0.7041095890410959,
      "grad_norm": 0.5187571048736572,
      "learning_rate": 8.886301369863014e-06,
      "loss": 0.0226,
      "step": 15420
    },
    {
      "epoch": 0.7045662100456621,
      "grad_norm": 0.3099510371685028,
      "learning_rate": 8.872602739726027e-06,
      "loss": 0.0301,
      "step": 15430
    },
    {
      "epoch": 0.7050228310502283,
      "grad_norm": 0.48829978704452515,
      "learning_rate": 8.85890410958904e-06,
      "loss": 0.0229,
      "step": 15440
    },
    {
      "epoch": 0.7054794520547946,
      "grad_norm": 1.2114629745483398,
      "learning_rate": 8.845205479452056e-06,
      "loss": 0.0188,
      "step": 15450
    },
    {
      "epoch": 0.7059360730593607,
      "grad_norm": 0.4140576422214508,
      "learning_rate": 8.831506849315069e-06,
      "loss": 0.0264,
      "step": 15460
    },
    {
      "epoch": 0.7063926940639269,
      "grad_norm": 0.49034175276756287,
      "learning_rate": 8.817808219178082e-06,
      "loss": 0.0287,
      "step": 15470
    },
    {
      "epoch": 0.7068493150684931,
      "grad_norm": 0.463152676820755,
      "learning_rate": 8.804109589041096e-06,
      "loss": 0.02,
      "step": 15480
    },
    {
      "epoch": 0.7073059360730594,
      "grad_norm": 1.0636719465255737,
      "learning_rate": 8.79041095890411e-06,
      "loss": 0.027,
      "step": 15490
    },
    {
      "epoch": 0.7077625570776256,
      "grad_norm": 0.4595049321651459,
      "learning_rate": 8.776712328767124e-06,
      "loss": 0.0233,
      "step": 15500
    },
    {
      "epoch": 0.7082191780821918,
      "grad_norm": 0.21012374758720398,
      "learning_rate": 8.763013698630138e-06,
      "loss": 0.027,
      "step": 15510
    },
    {
      "epoch": 0.708675799086758,
      "grad_norm": 1.485575795173645,
      "learning_rate": 8.749315068493151e-06,
      "loss": 0.0309,
      "step": 15520
    },
    {
      "epoch": 0.7091324200913242,
      "grad_norm": 0.8507908582687378,
      "learning_rate": 8.735616438356164e-06,
      "loss": 0.0272,
      "step": 15530
    },
    {
      "epoch": 0.7095890410958904,
      "grad_norm": 0.564703643321991,
      "learning_rate": 8.721917808219177e-06,
      "loss": 0.0286,
      "step": 15540
    },
    {
      "epoch": 0.7100456621004566,
      "grad_norm": 0.4636666476726532,
      "learning_rate": 8.708219178082192e-06,
      "loss": 0.0209,
      "step": 15550
    },
    {
      "epoch": 0.7105022831050228,
      "grad_norm": 0.8299076557159424,
      "learning_rate": 8.694520547945206e-06,
      "loss": 0.023,
      "step": 15560
    },
    {
      "epoch": 0.7109589041095891,
      "grad_norm": 0.1704750657081604,
      "learning_rate": 8.680821917808219e-06,
      "loss": 0.018,
      "step": 15570
    },
    {
      "epoch": 0.7114155251141553,
      "grad_norm": 1.8149315118789673,
      "learning_rate": 8.667123287671234e-06,
      "loss": 0.0376,
      "step": 15580
    },
    {
      "epoch": 0.7118721461187215,
      "grad_norm": 0.5089711546897888,
      "learning_rate": 8.653424657534247e-06,
      "loss": 0.0233,
      "step": 15590
    },
    {
      "epoch": 0.7123287671232876,
      "grad_norm": 0.34171050786972046,
      "learning_rate": 8.63972602739726e-06,
      "loss": 0.0313,
      "step": 15600
    },
    {
      "epoch": 0.7127853881278539,
      "grad_norm": 0.7045868635177612,
      "learning_rate": 8.626027397260276e-06,
      "loss": 0.0248,
      "step": 15610
    },
    {
      "epoch": 0.7132420091324201,
      "grad_norm": 0.8904518485069275,
      "learning_rate": 8.612328767123289e-06,
      "loss": 0.0221,
      "step": 15620
    },
    {
      "epoch": 0.7136986301369863,
      "grad_norm": 0.8195303082466125,
      "learning_rate": 8.598630136986301e-06,
      "loss": 0.0312,
      "step": 15630
    },
    {
      "epoch": 0.7141552511415525,
      "grad_norm": 0.9178885817527771,
      "learning_rate": 8.584931506849314e-06,
      "loss": 0.024,
      "step": 15640
    },
    {
      "epoch": 0.7146118721461188,
      "grad_norm": 0.45540565252304077,
      "learning_rate": 8.571232876712329e-06,
      "loss": 0.0248,
      "step": 15650
    },
    {
      "epoch": 0.7150684931506849,
      "grad_norm": 0.21707572042942047,
      "learning_rate": 8.557534246575342e-06,
      "loss": 0.022,
      "step": 15660
    },
    {
      "epoch": 0.7155251141552511,
      "grad_norm": 0.6796426177024841,
      "learning_rate": 8.543835616438356e-06,
      "loss": 0.0359,
      "step": 15670
    },
    {
      "epoch": 0.7159817351598173,
      "grad_norm": 0.6719589829444885,
      "learning_rate": 8.530136986301371e-06,
      "loss": 0.0235,
      "step": 15680
    },
    {
      "epoch": 0.7164383561643836,
      "grad_norm": 0.24698898196220398,
      "learning_rate": 8.516438356164384e-06,
      "loss": 0.0166,
      "step": 15690
    },
    {
      "epoch": 0.7168949771689498,
      "grad_norm": 1.1257596015930176,
      "learning_rate": 8.502739726027397e-06,
      "loss": 0.0341,
      "step": 15700
    },
    {
      "epoch": 0.717351598173516,
      "grad_norm": 0.928359866142273,
      "learning_rate": 8.489041095890411e-06,
      "loss": 0.0245,
      "step": 15710
    },
    {
      "epoch": 0.7178082191780822,
      "grad_norm": 0.4908980131149292,
      "learning_rate": 8.475342465753426e-06,
      "loss": 0.0334,
      "step": 15720
    },
    {
      "epoch": 0.7182648401826484,
      "grad_norm": 1.7631722688674927,
      "learning_rate": 8.461643835616439e-06,
      "loss": 0.0207,
      "step": 15730
    },
    {
      "epoch": 0.7187214611872146,
      "grad_norm": 0.8885893821716309,
      "learning_rate": 8.447945205479452e-06,
      "loss": 0.0252,
      "step": 15740
    },
    {
      "epoch": 0.7191780821917808,
      "grad_norm": 0.2400909811258316,
      "learning_rate": 8.434246575342466e-06,
      "loss": 0.0264,
      "step": 15750
    },
    {
      "epoch": 0.719634703196347,
      "grad_norm": 0.6131179928779602,
      "learning_rate": 8.420547945205479e-06,
      "loss": 0.0236,
      "step": 15760
    },
    {
      "epoch": 0.7200913242009133,
      "grad_norm": 0.5237264037132263,
      "learning_rate": 8.406849315068492e-06,
      "loss": 0.027,
      "step": 15770
    },
    {
      "epoch": 0.7205479452054795,
      "grad_norm": 0.21750062704086304,
      "learning_rate": 8.393150684931508e-06,
      "loss": 0.0259,
      "step": 15780
    },
    {
      "epoch": 0.7210045662100457,
      "grad_norm": 0.8849585056304932,
      "learning_rate": 8.379452054794521e-06,
      "loss": 0.0318,
      "step": 15790
    },
    {
      "epoch": 0.7214611872146118,
      "grad_norm": 0.5293562412261963,
      "learning_rate": 8.365753424657534e-06,
      "loss": 0.0263,
      "step": 15800
    },
    {
      "epoch": 0.7219178082191781,
      "grad_norm": 1.024534821510315,
      "learning_rate": 8.352054794520549e-06,
      "loss": 0.0326,
      "step": 15810
    },
    {
      "epoch": 0.7223744292237443,
      "grad_norm": 0.32061538100242615,
      "learning_rate": 8.338356164383561e-06,
      "loss": 0.027,
      "step": 15820
    },
    {
      "epoch": 0.7228310502283105,
      "grad_norm": 0.36080148816108704,
      "learning_rate": 8.324657534246576e-06,
      "loss": 0.0192,
      "step": 15830
    },
    {
      "epoch": 0.7232876712328767,
      "grad_norm": 2.618713855743408,
      "learning_rate": 8.310958904109589e-06,
      "loss": 0.0234,
      "step": 15840
    },
    {
      "epoch": 0.723744292237443,
      "grad_norm": 0.6398808360099792,
      "learning_rate": 8.297260273972603e-06,
      "loss": 0.0189,
      "step": 15850
    },
    {
      "epoch": 0.7242009132420091,
      "grad_norm": 0.6619907021522522,
      "learning_rate": 8.283561643835616e-06,
      "loss": 0.0298,
      "step": 15860
    },
    {
      "epoch": 0.7246575342465753,
      "grad_norm": 1.722286581993103,
      "learning_rate": 8.26986301369863e-06,
      "loss": 0.0293,
      "step": 15870
    },
    {
      "epoch": 0.7251141552511415,
      "grad_norm": 0.9653599262237549,
      "learning_rate": 8.256164383561645e-06,
      "loss": 0.0268,
      "step": 15880
    },
    {
      "epoch": 0.7255707762557078,
      "grad_norm": 0.594550371170044,
      "learning_rate": 8.242465753424658e-06,
      "loss": 0.0272,
      "step": 15890
    },
    {
      "epoch": 0.726027397260274,
      "grad_norm": 0.9056357741355896,
      "learning_rate": 8.228767123287671e-06,
      "loss": 0.0274,
      "step": 15900
    },
    {
      "epoch": 0.7264840182648402,
      "grad_norm": 0.6153044104576111,
      "learning_rate": 8.215068493150686e-06,
      "loss": 0.0307,
      "step": 15910
    },
    {
      "epoch": 0.7269406392694064,
      "grad_norm": 0.659288227558136,
      "learning_rate": 8.201369863013699e-06,
      "loss": 0.0196,
      "step": 15920
    },
    {
      "epoch": 0.7273972602739726,
      "grad_norm": 0.8901428580284119,
      "learning_rate": 8.187671232876712e-06,
      "loss": 0.0282,
      "step": 15930
    },
    {
      "epoch": 0.7278538812785388,
      "grad_norm": 0.424124538898468,
      "learning_rate": 8.173972602739726e-06,
      "loss": 0.0213,
      "step": 15940
    },
    {
      "epoch": 0.728310502283105,
      "grad_norm": 0.8069025278091431,
      "learning_rate": 8.16027397260274e-06,
      "loss": 0.0259,
      "step": 15950
    },
    {
      "epoch": 0.7287671232876712,
      "grad_norm": 0.7399476170539856,
      "learning_rate": 8.146575342465754e-06,
      "loss": 0.0339,
      "step": 15960
    },
    {
      "epoch": 0.7292237442922375,
      "grad_norm": 1.0458872318267822,
      "learning_rate": 8.132876712328766e-06,
      "loss": 0.0262,
      "step": 15970
    },
    {
      "epoch": 0.7296803652968037,
      "grad_norm": 0.6834430694580078,
      "learning_rate": 8.119178082191781e-06,
      "loss": 0.0262,
      "step": 15980
    },
    {
      "epoch": 0.7301369863013699,
      "grad_norm": 0.44756633043289185,
      "learning_rate": 8.105479452054796e-06,
      "loss": 0.0153,
      "step": 15990
    },
    {
      "epoch": 0.730593607305936,
      "grad_norm": 0.6909480690956116,
      "learning_rate": 8.091780821917808e-06,
      "loss": 0.0177,
      "step": 16000
    },
    {
      "epoch": 0.7310502283105023,
      "grad_norm": 0.42306309938430786,
      "learning_rate": 8.078082191780823e-06,
      "loss": 0.0217,
      "step": 16010
    },
    {
      "epoch": 0.7315068493150685,
      "grad_norm": 0.36374127864837646,
      "learning_rate": 8.064383561643836e-06,
      "loss": 0.0287,
      "step": 16020
    },
    {
      "epoch": 0.7319634703196347,
      "grad_norm": 0.4567129909992218,
      "learning_rate": 8.050684931506849e-06,
      "loss": 0.0288,
      "step": 16030
    },
    {
      "epoch": 0.7324200913242009,
      "grad_norm": 0.6965654492378235,
      "learning_rate": 8.036986301369862e-06,
      "loss": 0.0245,
      "step": 16040
    },
    {
      "epoch": 0.7328767123287672,
      "grad_norm": 0.6395736932754517,
      "learning_rate": 8.023287671232878e-06,
      "loss": 0.0224,
      "step": 16050
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.354358434677124,
      "learning_rate": 8.00958904109589e-06,
      "loss": 0.0252,
      "step": 16060
    },
    {
      "epoch": 0.7337899543378995,
      "grad_norm": 0.5500860214233398,
      "learning_rate": 7.995890410958904e-06,
      "loss": 0.0256,
      "step": 16070
    },
    {
      "epoch": 0.7342465753424657,
      "grad_norm": 0.8173680901527405,
      "learning_rate": 7.982191780821918e-06,
      "loss": 0.0353,
      "step": 16080
    },
    {
      "epoch": 0.734703196347032,
      "grad_norm": 0.6744921803474426,
      "learning_rate": 7.968493150684931e-06,
      "loss": 0.0297,
      "step": 16090
    },
    {
      "epoch": 0.7351598173515982,
      "grad_norm": 1.4878010749816895,
      "learning_rate": 7.954794520547946e-06,
      "loss": 0.0399,
      "step": 16100
    },
    {
      "epoch": 0.7356164383561644,
      "grad_norm": 0.5043936967849731,
      "learning_rate": 7.94109589041096e-06,
      "loss": 0.0227,
      "step": 16110
    },
    {
      "epoch": 0.7360730593607306,
      "grad_norm": 0.4959428608417511,
      "learning_rate": 7.927397260273973e-06,
      "loss": 0.0218,
      "step": 16120
    },
    {
      "epoch": 0.7365296803652968,
      "grad_norm": 0.6744772791862488,
      "learning_rate": 7.913698630136986e-06,
      "loss": 0.0269,
      "step": 16130
    },
    {
      "epoch": 0.736986301369863,
      "grad_norm": 1.0543979406356812,
      "learning_rate": 7.899999999999999e-06,
      "loss": 0.033,
      "step": 16140
    },
    {
      "epoch": 0.7374429223744292,
      "grad_norm": 0.5541080832481384,
      "learning_rate": 7.886301369863013e-06,
      "loss": 0.0263,
      "step": 16150
    },
    {
      "epoch": 0.7378995433789954,
      "grad_norm": 0.6108269095420837,
      "learning_rate": 7.872602739726028e-06,
      "loss": 0.0264,
      "step": 16160
    },
    {
      "epoch": 0.7383561643835617,
      "grad_norm": 0.8577277064323425,
      "learning_rate": 7.858904109589041e-06,
      "loss": 0.0212,
      "step": 16170
    },
    {
      "epoch": 0.7388127853881279,
      "grad_norm": 1.4778027534484863,
      "learning_rate": 7.845205479452055e-06,
      "loss": 0.0298,
      "step": 16180
    },
    {
      "epoch": 0.7392694063926941,
      "grad_norm": 0.25999715924263,
      "learning_rate": 7.831506849315068e-06,
      "loss": 0.0369,
      "step": 16190
    },
    {
      "epoch": 0.7397260273972602,
      "grad_norm": 0.5964335203170776,
      "learning_rate": 7.817808219178081e-06,
      "loss": 0.0262,
      "step": 16200
    },
    {
      "epoch": 0.7401826484018265,
      "grad_norm": 1.0063045024871826,
      "learning_rate": 7.804109589041098e-06,
      "loss": 0.0167,
      "step": 16210
    },
    {
      "epoch": 0.7406392694063927,
      "grad_norm": 0.6839851140975952,
      "learning_rate": 7.79041095890411e-06,
      "loss": 0.0342,
      "step": 16220
    },
    {
      "epoch": 0.7410958904109589,
      "grad_norm": 0.9958752393722534,
      "learning_rate": 7.776712328767123e-06,
      "loss": 0.021,
      "step": 16230
    },
    {
      "epoch": 0.7415525114155251,
      "grad_norm": 0.6080510020256042,
      "learning_rate": 7.763013698630136e-06,
      "loss": 0.024,
      "step": 16240
    },
    {
      "epoch": 0.7420091324200914,
      "grad_norm": 0.5936075448989868,
      "learning_rate": 7.74931506849315e-06,
      "loss": 0.0274,
      "step": 16250
    },
    {
      "epoch": 0.7424657534246575,
      "grad_norm": 0.6505873799324036,
      "learning_rate": 7.735616438356165e-06,
      "loss": 0.0284,
      "step": 16260
    },
    {
      "epoch": 0.7429223744292237,
      "grad_norm": 0.788381814956665,
      "learning_rate": 7.721917808219178e-06,
      "loss": 0.0321,
      "step": 16270
    },
    {
      "epoch": 0.7433789954337899,
      "grad_norm": 1.108529806137085,
      "learning_rate": 7.708219178082193e-06,
      "loss": 0.0248,
      "step": 16280
    },
    {
      "epoch": 0.7438356164383562,
      "grad_norm": 2.445340871810913,
      "learning_rate": 7.694520547945206e-06,
      "loss": 0.0235,
      "step": 16290
    },
    {
      "epoch": 0.7442922374429224,
      "grad_norm": 0.6202954053878784,
      "learning_rate": 7.680821917808219e-06,
      "loss": 0.0283,
      "step": 16300
    },
    {
      "epoch": 0.7447488584474886,
      "grad_norm": 0.5203611254692078,
      "learning_rate": 7.667123287671233e-06,
      "loss": 0.0236,
      "step": 16310
    },
    {
      "epoch": 0.7452054794520548,
      "grad_norm": 0.8047798275947571,
      "learning_rate": 7.653424657534248e-06,
      "loss": 0.0254,
      "step": 16320
    },
    {
      "epoch": 0.745662100456621,
      "grad_norm": 0.4731796681880951,
      "learning_rate": 7.63972602739726e-06,
      "loss": 0.0238,
      "step": 16330
    },
    {
      "epoch": 0.7461187214611872,
      "grad_norm": 0.6735702157020569,
      "learning_rate": 7.626027397260273e-06,
      "loss": 0.0261,
      "step": 16340
    },
    {
      "epoch": 0.7465753424657534,
      "grad_norm": 0.5993340611457825,
      "learning_rate": 7.612328767123288e-06,
      "loss": 0.0341,
      "step": 16350
    },
    {
      "epoch": 0.7470319634703196,
      "grad_norm": 0.6336392760276794,
      "learning_rate": 7.598630136986302e-06,
      "loss": 0.0208,
      "step": 16360
    },
    {
      "epoch": 0.7474885844748859,
      "grad_norm": 0.5932365655899048,
      "learning_rate": 7.584931506849315e-06,
      "loss": 0.0244,
      "step": 16370
    },
    {
      "epoch": 0.7479452054794521,
      "grad_norm": 0.8708791732788086,
      "learning_rate": 7.571232876712329e-06,
      "loss": 0.0193,
      "step": 16380
    },
    {
      "epoch": 0.7484018264840183,
      "grad_norm": 0.5110594034194946,
      "learning_rate": 7.557534246575343e-06,
      "loss": 0.0366,
      "step": 16390
    },
    {
      "epoch": 0.7488584474885844,
      "grad_norm": 0.42441326379776,
      "learning_rate": 7.543835616438356e-06,
      "loss": 0.0283,
      "step": 16400
    },
    {
      "epoch": 0.7493150684931507,
      "grad_norm": 0.7251980900764465,
      "learning_rate": 7.530136986301371e-06,
      "loss": 0.025,
      "step": 16410
    },
    {
      "epoch": 0.7497716894977169,
      "grad_norm": 0.663906455039978,
      "learning_rate": 7.516438356164384e-06,
      "loss": 0.0197,
      "step": 16420
    },
    {
      "epoch": 0.7502283105022831,
      "grad_norm": 0.2966521978378296,
      "learning_rate": 7.502739726027397e-06,
      "loss": 0.0207,
      "step": 16430
    },
    {
      "epoch": 0.7506849315068493,
      "grad_norm": 0.853744626045227,
      "learning_rate": 7.4890410958904115e-06,
      "loss": 0.0151,
      "step": 16440
    },
    {
      "epoch": 0.7511415525114156,
      "grad_norm": 1.3281041383743286,
      "learning_rate": 7.475342465753424e-06,
      "loss": 0.0266,
      "step": 16450
    },
    {
      "epoch": 0.7515981735159817,
      "grad_norm": 0.58173668384552,
      "learning_rate": 7.461643835616438e-06,
      "loss": 0.0285,
      "step": 16460
    },
    {
      "epoch": 0.7520547945205479,
      "grad_norm": 0.493660032749176,
      "learning_rate": 7.447945205479453e-06,
      "loss": 0.0263,
      "step": 16470
    },
    {
      "epoch": 0.7525114155251141,
      "grad_norm": 1.252503514289856,
      "learning_rate": 7.4342465753424656e-06,
      "loss": 0.0259,
      "step": 16480
    },
    {
      "epoch": 0.7529680365296804,
      "grad_norm": 0.6306846737861633,
      "learning_rate": 7.42054794520548e-06,
      "loss": 0.035,
      "step": 16490
    },
    {
      "epoch": 0.7534246575342466,
      "grad_norm": 0.4695625305175781,
      "learning_rate": 7.406849315068493e-06,
      "loss": 0.0202,
      "step": 16500
    },
    {
      "epoch": 0.7538812785388128,
      "grad_norm": 0.8321065902709961,
      "learning_rate": 7.393150684931507e-06,
      "loss": 0.0235,
      "step": 16510
    },
    {
      "epoch": 0.754337899543379,
      "grad_norm": 0.214586541056633,
      "learning_rate": 7.379452054794521e-06,
      "loss": 0.0248,
      "step": 16520
    },
    {
      "epoch": 0.7547945205479452,
      "grad_norm": 0.3339592516422272,
      "learning_rate": 7.365753424657534e-06,
      "loss": 0.0247,
      "step": 16530
    },
    {
      "epoch": 0.7552511415525114,
      "grad_norm": 0.7254947423934937,
      "learning_rate": 7.352054794520548e-06,
      "loss": 0.0192,
      "step": 16540
    },
    {
      "epoch": 0.7557077625570776,
      "grad_norm": 1.331186056137085,
      "learning_rate": 7.338356164383562e-06,
      "loss": 0.0388,
      "step": 16550
    },
    {
      "epoch": 0.7561643835616438,
      "grad_norm": 0.879470944404602,
      "learning_rate": 7.324657534246575e-06,
      "loss": 0.0215,
      "step": 16560
    },
    {
      "epoch": 0.7566210045662101,
      "grad_norm": 0.9602764248847961,
      "learning_rate": 7.310958904109589e-06,
      "loss": 0.0149,
      "step": 16570
    },
    {
      "epoch": 0.7570776255707763,
      "grad_norm": 0.3142746388912201,
      "learning_rate": 7.297260273972603e-06,
      "loss": 0.0238,
      "step": 16580
    },
    {
      "epoch": 0.7575342465753425,
      "grad_norm": 0.7696230411529541,
      "learning_rate": 7.2835616438356165e-06,
      "loss": 0.0237,
      "step": 16590
    },
    {
      "epoch": 0.7579908675799086,
      "grad_norm": 0.5044689178466797,
      "learning_rate": 7.26986301369863e-06,
      "loss": 0.0213,
      "step": 16600
    },
    {
      "epoch": 0.7584474885844749,
      "grad_norm": 0.8099607825279236,
      "learning_rate": 7.256164383561644e-06,
      "loss": 0.0215,
      "step": 16610
    },
    {
      "epoch": 0.7589041095890411,
      "grad_norm": 0.8764986395835876,
      "learning_rate": 7.242465753424658e-06,
      "loss": 0.0234,
      "step": 16620
    },
    {
      "epoch": 0.7593607305936073,
      "grad_norm": 0.6982179284095764,
      "learning_rate": 7.2287671232876714e-06,
      "loss": 0.0201,
      "step": 16630
    },
    {
      "epoch": 0.7598173515981735,
      "grad_norm": 1.2422171831130981,
      "learning_rate": 7.215068493150685e-06,
      "loss": 0.0273,
      "step": 16640
    },
    {
      "epoch": 0.7602739726027398,
      "grad_norm": 0.39863142371177673,
      "learning_rate": 7.201369863013698e-06,
      "loss": 0.0215,
      "step": 16650
    },
    {
      "epoch": 0.7607305936073059,
      "grad_norm": 0.5467363595962524,
      "learning_rate": 7.187671232876713e-06,
      "loss": 0.0283,
      "step": 16660
    },
    {
      "epoch": 0.7611872146118721,
      "grad_norm": 1.1982392072677612,
      "learning_rate": 7.173972602739726e-06,
      "loss": 0.0299,
      "step": 16670
    },
    {
      "epoch": 0.7616438356164383,
      "grad_norm": 0.49717646837234497,
      "learning_rate": 7.16027397260274e-06,
      "loss": 0.0289,
      "step": 16680
    },
    {
      "epoch": 0.7621004566210046,
      "grad_norm": 0.7101370692253113,
      "learning_rate": 7.146575342465754e-06,
      "loss": 0.0256,
      "step": 16690
    },
    {
      "epoch": 0.7625570776255708,
      "grad_norm": 0.8489019274711609,
      "learning_rate": 7.132876712328767e-06,
      "loss": 0.0175,
      "step": 16700
    },
    {
      "epoch": 0.763013698630137,
      "grad_norm": 0.4363303482532501,
      "learning_rate": 7.119178082191781e-06,
      "loss": 0.0188,
      "step": 16710
    },
    {
      "epoch": 0.7634703196347032,
      "grad_norm": 1.2204667329788208,
      "learning_rate": 7.105479452054795e-06,
      "loss": 0.0291,
      "step": 16720
    },
    {
      "epoch": 0.7639269406392694,
      "grad_norm": 0.5098122358322144,
      "learning_rate": 7.091780821917808e-06,
      "loss": 0.0204,
      "step": 16730
    },
    {
      "epoch": 0.7643835616438356,
      "grad_norm": 0.5923327803611755,
      "learning_rate": 7.078082191780822e-06,
      "loss": 0.0283,
      "step": 16740
    },
    {
      "epoch": 0.7648401826484018,
      "grad_norm": 0.22875848412513733,
      "learning_rate": 7.064383561643835e-06,
      "loss": 0.0227,
      "step": 16750
    },
    {
      "epoch": 0.765296803652968,
      "grad_norm": 1.0387370586395264,
      "learning_rate": 7.05068493150685e-06,
      "loss": 0.0207,
      "step": 16760
    },
    {
      "epoch": 0.7657534246575343,
      "grad_norm": 0.6274595260620117,
      "learning_rate": 7.036986301369864e-06,
      "loss": 0.0311,
      "step": 16770
    },
    {
      "epoch": 0.7662100456621005,
      "grad_norm": 0.7450882792472839,
      "learning_rate": 7.0232876712328765e-06,
      "loss": 0.0189,
      "step": 16780
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 1.0295854806900024,
      "learning_rate": 7.009589041095891e-06,
      "loss": 0.0242,
      "step": 16790
    },
    {
      "epoch": 0.7671232876712328,
      "grad_norm": 0.2943810224533081,
      "learning_rate": 6.995890410958904e-06,
      "loss": 0.0282,
      "step": 16800
    },
    {
      "epoch": 0.7675799086757991,
      "grad_norm": 0.2789947986602783,
      "learning_rate": 6.982191780821918e-06,
      "loss": 0.0263,
      "step": 16810
    },
    {
      "epoch": 0.7680365296803653,
      "grad_norm": 0.8049736618995667,
      "learning_rate": 6.968493150684932e-06,
      "loss": 0.0293,
      "step": 16820
    },
    {
      "epoch": 0.7684931506849315,
      "grad_norm": 0.7406426668167114,
      "learning_rate": 6.954794520547945e-06,
      "loss": 0.0288,
      "step": 16830
    },
    {
      "epoch": 0.7689497716894977,
      "grad_norm": 1.2691693305969238,
      "learning_rate": 6.941095890410959e-06,
      "loss": 0.0277,
      "step": 16840
    },
    {
      "epoch": 0.769406392694064,
      "grad_norm": 0.6391613483428955,
      "learning_rate": 6.9273972602739726e-06,
      "loss": 0.0216,
      "step": 16850
    },
    {
      "epoch": 0.7698630136986301,
      "grad_norm": 0.8996132612228394,
      "learning_rate": 6.913698630136986e-06,
      "loss": 0.0208,
      "step": 16860
    },
    {
      "epoch": 0.7703196347031963,
      "grad_norm": 0.4977364242076874,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.0242,
      "step": 16870
    },
    {
      "epoch": 0.7707762557077625,
      "grad_norm": 0.3418982923030853,
      "learning_rate": 6.886301369863014e-06,
      "loss": 0.0151,
      "step": 16880
    },
    {
      "epoch": 0.7712328767123288,
      "grad_norm": 1.2750916481018066,
      "learning_rate": 6.8726027397260275e-06,
      "loss": 0.028,
      "step": 16890
    },
    {
      "epoch": 0.771689497716895,
      "grad_norm": 0.9898911714553833,
      "learning_rate": 6.858904109589041e-06,
      "loss": 0.0223,
      "step": 16900
    },
    {
      "epoch": 0.7721461187214612,
      "grad_norm": 0.3229161202907562,
      "learning_rate": 6.845205479452055e-06,
      "loss": 0.0253,
      "step": 16910
    },
    {
      "epoch": 0.7726027397260274,
      "grad_norm": 0.38687750697135925,
      "learning_rate": 6.831506849315069e-06,
      "loss": 0.0219,
      "step": 16920
    },
    {
      "epoch": 0.7730593607305936,
      "grad_norm": 0.5825086236000061,
      "learning_rate": 6.817808219178082e-06,
      "loss": 0.0247,
      "step": 16930
    },
    {
      "epoch": 0.7735159817351598,
      "grad_norm": 0.8526811003684998,
      "learning_rate": 6.804109589041096e-06,
      "loss": 0.0183,
      "step": 16940
    },
    {
      "epoch": 0.773972602739726,
      "grad_norm": 1.2319730520248413,
      "learning_rate": 6.79041095890411e-06,
      "loss": 0.0233,
      "step": 16950
    },
    {
      "epoch": 0.7744292237442922,
      "grad_norm": 1.7306568622589111,
      "learning_rate": 6.7767123287671235e-06,
      "loss": 0.0276,
      "step": 16960
    },
    {
      "epoch": 0.7748858447488585,
      "grad_norm": 0.8919475078582764,
      "learning_rate": 6.763013698630137e-06,
      "loss": 0.0212,
      "step": 16970
    },
    {
      "epoch": 0.7753424657534247,
      "grad_norm": 0.46758076548576355,
      "learning_rate": 6.749315068493151e-06,
      "loss": 0.0276,
      "step": 16980
    },
    {
      "epoch": 0.7757990867579909,
      "grad_norm": 0.3748443126678467,
      "learning_rate": 6.735616438356165e-06,
      "loss": 0.0195,
      "step": 16990
    },
    {
      "epoch": 0.776255707762557,
      "grad_norm": 0.7217444181442261,
      "learning_rate": 6.721917808219178e-06,
      "loss": 0.031,
      "step": 17000
    },
    {
      "epoch": 0.7767123287671233,
      "grad_norm": 0.6124155521392822,
      "learning_rate": 6.708219178082192e-06,
      "loss": 0.0191,
      "step": 17010
    },
    {
      "epoch": 0.7771689497716895,
      "grad_norm": 1.526476502418518,
      "learning_rate": 6.694520547945206e-06,
      "loss": 0.0346,
      "step": 17020
    },
    {
      "epoch": 0.7776255707762557,
      "grad_norm": 0.6425842642784119,
      "learning_rate": 6.680821917808219e-06,
      "loss": 0.0239,
      "step": 17030
    },
    {
      "epoch": 0.7780821917808219,
      "grad_norm": 0.8182483315467834,
      "learning_rate": 6.667123287671233e-06,
      "loss": 0.0381,
      "step": 17040
    },
    {
      "epoch": 0.7785388127853882,
      "grad_norm": 0.7515584826469421,
      "learning_rate": 6.653424657534246e-06,
      "loss": 0.0268,
      "step": 17050
    },
    {
      "epoch": 0.7789954337899543,
      "grad_norm": 0.4257029592990875,
      "learning_rate": 6.639726027397261e-06,
      "loss": 0.0187,
      "step": 17060
    },
    {
      "epoch": 0.7794520547945205,
      "grad_norm": 0.2844412326812744,
      "learning_rate": 6.6260273972602745e-06,
      "loss": 0.0223,
      "step": 17070
    },
    {
      "epoch": 0.7799086757990867,
      "grad_norm": 0.7284010648727417,
      "learning_rate": 6.612328767123287e-06,
      "loss": 0.0315,
      "step": 17080
    },
    {
      "epoch": 0.780365296803653,
      "grad_norm": 0.7615714073181152,
      "learning_rate": 6.598630136986302e-06,
      "loss": 0.0226,
      "step": 17090
    },
    {
      "epoch": 0.7808219178082192,
      "grad_norm": 1.0573080778121948,
      "learning_rate": 6.584931506849315e-06,
      "loss": 0.0182,
      "step": 17100
    },
    {
      "epoch": 0.7812785388127854,
      "grad_norm": 0.8986353874206543,
      "learning_rate": 6.5712328767123286e-06,
      "loss": 0.026,
      "step": 17110
    },
    {
      "epoch": 0.7817351598173516,
      "grad_norm": 0.34222641587257385,
      "learning_rate": 6.557534246575343e-06,
      "loss": 0.0197,
      "step": 17120
    },
    {
      "epoch": 0.7821917808219178,
      "grad_norm": 0.5344856381416321,
      "learning_rate": 6.543835616438356e-06,
      "loss": 0.0319,
      "step": 17130
    },
    {
      "epoch": 0.782648401826484,
      "grad_norm": 0.47066089510917664,
      "learning_rate": 6.530136986301371e-06,
      "loss": 0.0229,
      "step": 17140
    },
    {
      "epoch": 0.7831050228310502,
      "grad_norm": 0.7327766418457031,
      "learning_rate": 6.5164383561643835e-06,
      "loss": 0.0258,
      "step": 17150
    },
    {
      "epoch": 0.7835616438356164,
      "grad_norm": 0.5841618180274963,
      "learning_rate": 6.502739726027397e-06,
      "loss": 0.0241,
      "step": 17160
    },
    {
      "epoch": 0.7840182648401827,
      "grad_norm": 1.1662375926971436,
      "learning_rate": 6.489041095890412e-06,
      "loss": 0.0297,
      "step": 17170
    },
    {
      "epoch": 0.7844748858447489,
      "grad_norm": 0.47671595215797424,
      "learning_rate": 6.475342465753425e-06,
      "loss": 0.0244,
      "step": 17180
    },
    {
      "epoch": 0.7849315068493151,
      "grad_norm": 1.2980492115020752,
      "learning_rate": 6.461643835616438e-06,
      "loss": 0.0303,
      "step": 17190
    },
    {
      "epoch": 0.7853881278538812,
      "grad_norm": 0.9506463408470154,
      "learning_rate": 6.447945205479452e-06,
      "loss": 0.0254,
      "step": 17200
    },
    {
      "epoch": 0.7858447488584475,
      "grad_norm": 0.7276167869567871,
      "learning_rate": 6.434246575342466e-06,
      "loss": 0.0263,
      "step": 17210
    },
    {
      "epoch": 0.7863013698630137,
      "grad_norm": 0.4733934998512268,
      "learning_rate": 6.4205479452054795e-06,
      "loss": 0.0261,
      "step": 17220
    },
    {
      "epoch": 0.7867579908675799,
      "grad_norm": 0.39351701736450195,
      "learning_rate": 6.406849315068493e-06,
      "loss": 0.02,
      "step": 17230
    },
    {
      "epoch": 0.7872146118721461,
      "grad_norm": 0.11638502776622772,
      "learning_rate": 6.393150684931507e-06,
      "loss": 0.0245,
      "step": 17240
    },
    {
      "epoch": 0.7876712328767124,
      "grad_norm": 0.6896195411682129,
      "learning_rate": 6.379452054794521e-06,
      "loss": 0.027,
      "step": 17250
    },
    {
      "epoch": 0.7881278538812785,
      "grad_norm": 1.1658928394317627,
      "learning_rate": 6.3657534246575344e-06,
      "loss": 0.0242,
      "step": 17260
    },
    {
      "epoch": 0.7885844748858447,
      "grad_norm": 0.4003690779209137,
      "learning_rate": 6.352054794520548e-06,
      "loss": 0.0262,
      "step": 17270
    },
    {
      "epoch": 0.7890410958904109,
      "grad_norm": 0.6718550324440002,
      "learning_rate": 6.338356164383562e-06,
      "loss": 0.0169,
      "step": 17280
    },
    {
      "epoch": 0.7894977168949772,
      "grad_norm": 0.8786395788192749,
      "learning_rate": 6.324657534246576e-06,
      "loss": 0.0315,
      "step": 17290
    },
    {
      "epoch": 0.7899543378995434,
      "grad_norm": 0.06595700979232788,
      "learning_rate": 6.3109589041095885e-06,
      "loss": 0.0199,
      "step": 17300
    },
    {
      "epoch": 0.7904109589041096,
      "grad_norm": 0.556070864200592,
      "learning_rate": 6.297260273972603e-06,
      "loss": 0.0242,
      "step": 17310
    },
    {
      "epoch": 0.7908675799086758,
      "grad_norm": 0.20711980760097504,
      "learning_rate": 6.283561643835617e-06,
      "loss": 0.0224,
      "step": 17320
    },
    {
      "epoch": 0.791324200913242,
      "grad_norm": 0.6736580729484558,
      "learning_rate": 6.2698630136986305e-06,
      "loss": 0.0282,
      "step": 17330
    },
    {
      "epoch": 0.7917808219178082,
      "grad_norm": 0.600264847278595,
      "learning_rate": 6.256164383561644e-06,
      "loss": 0.0287,
      "step": 17340
    },
    {
      "epoch": 0.7922374429223744,
      "grad_norm": 0.41142794489860535,
      "learning_rate": 6.242465753424657e-06,
      "loss": 0.0366,
      "step": 17350
    },
    {
      "epoch": 0.7926940639269406,
      "grad_norm": 0.4602248966693878,
      "learning_rate": 6.228767123287672e-06,
      "loss": 0.0234,
      "step": 17360
    },
    {
      "epoch": 0.7931506849315069,
      "grad_norm": 0.9869236946105957,
      "learning_rate": 6.2150684931506854e-06,
      "loss": 0.0229,
      "step": 17370
    },
    {
      "epoch": 0.7936073059360731,
      "grad_norm": 0.6464300751686096,
      "learning_rate": 6.201369863013698e-06,
      "loss": 0.023,
      "step": 17380
    },
    {
      "epoch": 0.7940639269406393,
      "grad_norm": 0.354460209608078,
      "learning_rate": 6.187671232876713e-06,
      "loss": 0.0165,
      "step": 17390
    },
    {
      "epoch": 0.7945205479452054,
      "grad_norm": 0.391618549823761,
      "learning_rate": 6.173972602739726e-06,
      "loss": 0.026,
      "step": 17400
    },
    {
      "epoch": 0.7949771689497717,
      "grad_norm": 0.6205141544342041,
      "learning_rate": 6.1602739726027395e-06,
      "loss": 0.0292,
      "step": 17410
    },
    {
      "epoch": 0.7954337899543379,
      "grad_norm": 0.2831304967403412,
      "learning_rate": 6.146575342465754e-06,
      "loss": 0.0271,
      "step": 17420
    },
    {
      "epoch": 0.7958904109589041,
      "grad_norm": 0.2554854452610016,
      "learning_rate": 6.132876712328767e-06,
      "loss": 0.0292,
      "step": 17430
    },
    {
      "epoch": 0.7963470319634703,
      "grad_norm": 0.4482092559337616,
      "learning_rate": 6.1191780821917815e-06,
      "loss": 0.0221,
      "step": 17440
    },
    {
      "epoch": 0.7968036529680366,
      "grad_norm": 0.8485084176063538,
      "learning_rate": 6.105479452054794e-06,
      "loss": 0.0209,
      "step": 17450
    },
    {
      "epoch": 0.7972602739726027,
      "grad_norm": 0.9425216913223267,
      "learning_rate": 6.091780821917808e-06,
      "loss": 0.0206,
      "step": 17460
    },
    {
      "epoch": 0.7977168949771689,
      "grad_norm": 0.6549673676490784,
      "learning_rate": 6.078082191780823e-06,
      "loss": 0.0221,
      "step": 17470
    },
    {
      "epoch": 0.7981735159817351,
      "grad_norm": 0.31911519169807434,
      "learning_rate": 6.0643835616438356e-06,
      "loss": 0.024,
      "step": 17480
    },
    {
      "epoch": 0.7986301369863014,
      "grad_norm": 0.7655519247055054,
      "learning_rate": 6.050684931506849e-06,
      "loss": 0.0267,
      "step": 17490
    },
    {
      "epoch": 0.7990867579908676,
      "grad_norm": 0.16400115191936493,
      "learning_rate": 6.036986301369863e-06,
      "loss": 0.0265,
      "step": 17500
    },
    {
      "epoch": 0.7995433789954338,
      "grad_norm": 0.06163165718317032,
      "learning_rate": 6.023287671232877e-06,
      "loss": 0.0159,
      "step": 17510
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.388481467962265,
      "learning_rate": 6.009589041095891e-06,
      "loss": 0.0189,
      "step": 17520
    },
    {
      "epoch": 0.8004566210045662,
      "grad_norm": 0.9885727167129517,
      "learning_rate": 5.995890410958904e-06,
      "loss": 0.0221,
      "step": 17530
    },
    {
      "epoch": 0.8009132420091324,
      "grad_norm": 0.8914745450019836,
      "learning_rate": 5.982191780821918e-06,
      "loss": 0.0342,
      "step": 17540
    },
    {
      "epoch": 0.8013698630136986,
      "grad_norm": 0.5618124604225159,
      "learning_rate": 5.968493150684932e-06,
      "loss": 0.0301,
      "step": 17550
    },
    {
      "epoch": 0.8018264840182648,
      "grad_norm": 0.1943349838256836,
      "learning_rate": 5.954794520547945e-06,
      "loss": 0.0225,
      "step": 17560
    },
    {
      "epoch": 0.8022831050228311,
      "grad_norm": 0.5620220899581909,
      "learning_rate": 5.941095890410959e-06,
      "loss": 0.0237,
      "step": 17570
    },
    {
      "epoch": 0.8027397260273973,
      "grad_norm": 1.160046100616455,
      "learning_rate": 5.927397260273973e-06,
      "loss": 0.0267,
      "step": 17580
    },
    {
      "epoch": 0.8031963470319635,
      "grad_norm": 0.9176075458526611,
      "learning_rate": 5.9136986301369865e-06,
      "loss": 0.0372,
      "step": 17590
    },
    {
      "epoch": 0.8036529680365296,
      "grad_norm": 0.5018401741981506,
      "learning_rate": 5.899999999999999e-06,
      "loss": 0.0314,
      "step": 17600
    },
    {
      "epoch": 0.8041095890410959,
      "grad_norm": 0.7567120790481567,
      "learning_rate": 5.886301369863014e-06,
      "loss": 0.0187,
      "step": 17610
    },
    {
      "epoch": 0.8045662100456621,
      "grad_norm": 0.6116340160369873,
      "learning_rate": 5.872602739726028e-06,
      "loss": 0.0249,
      "step": 17620
    },
    {
      "epoch": 0.8050228310502283,
      "grad_norm": 0.7646195292472839,
      "learning_rate": 5.8589041095890414e-06,
      "loss": 0.0288,
      "step": 17630
    },
    {
      "epoch": 0.8054794520547945,
      "grad_norm": 0.5070538520812988,
      "learning_rate": 5.845205479452055e-06,
      "loss": 0.0154,
      "step": 17640
    },
    {
      "epoch": 0.8059360730593608,
      "grad_norm": 0.5593487620353699,
      "learning_rate": 5.831506849315068e-06,
      "loss": 0.0273,
      "step": 17650
    },
    {
      "epoch": 0.806392694063927,
      "grad_norm": 0.5548263788223267,
      "learning_rate": 5.817808219178083e-06,
      "loss": 0.0134,
      "step": 17660
    },
    {
      "epoch": 0.8068493150684931,
      "grad_norm": 1.0630440711975098,
      "learning_rate": 5.804109589041096e-06,
      "loss": 0.0174,
      "step": 17670
    },
    {
      "epoch": 0.8073059360730593,
      "grad_norm": 0.8661158084869385,
      "learning_rate": 5.790410958904109e-06,
      "loss": 0.0205,
      "step": 17680
    },
    {
      "epoch": 0.8077625570776256,
      "grad_norm": 0.5962100028991699,
      "learning_rate": 5.776712328767124e-06,
      "loss": 0.0182,
      "step": 17690
    },
    {
      "epoch": 0.8082191780821918,
      "grad_norm": 0.3604704439640045,
      "learning_rate": 5.763013698630137e-06,
      "loss": 0.0292,
      "step": 17700
    },
    {
      "epoch": 0.808675799086758,
      "grad_norm": 0.8206785917282104,
      "learning_rate": 5.749315068493151e-06,
      "loss": 0.0189,
      "step": 17710
    },
    {
      "epoch": 0.8091324200913242,
      "grad_norm": 0.8287326693534851,
      "learning_rate": 5.735616438356165e-06,
      "loss": 0.0261,
      "step": 17720
    },
    {
      "epoch": 0.8095890410958904,
      "grad_norm": 0.6849368810653687,
      "learning_rate": 5.721917808219178e-06,
      "loss": 0.026,
      "step": 17730
    },
    {
      "epoch": 0.8100456621004566,
      "grad_norm": 0.20079928636550903,
      "learning_rate": 5.708219178082192e-06,
      "loss": 0.0161,
      "step": 17740
    },
    {
      "epoch": 0.8105022831050228,
      "grad_norm": 0.5735673308372498,
      "learning_rate": 5.694520547945205e-06,
      "loss": 0.0194,
      "step": 17750
    },
    {
      "epoch": 0.810958904109589,
      "grad_norm": 1.0153831243515015,
      "learning_rate": 5.680821917808219e-06,
      "loss": 0.0228,
      "step": 17760
    },
    {
      "epoch": 0.8114155251141553,
      "grad_norm": 1.4011799097061157,
      "learning_rate": 5.667123287671234e-06,
      "loss": 0.0288,
      "step": 17770
    },
    {
      "epoch": 0.8118721461187215,
      "grad_norm": 0.5663310885429382,
      "learning_rate": 5.6534246575342465e-06,
      "loss": 0.0199,
      "step": 17780
    },
    {
      "epoch": 0.8123287671232877,
      "grad_norm": 0.28842833638191223,
      "learning_rate": 5.63972602739726e-06,
      "loss": 0.0273,
      "step": 17790
    },
    {
      "epoch": 0.8127853881278538,
      "grad_norm": 0.5732077360153198,
      "learning_rate": 5.626027397260274e-06,
      "loss": 0.0186,
      "step": 17800
    },
    {
      "epoch": 0.8132420091324201,
      "grad_norm": 1.0482938289642334,
      "learning_rate": 5.612328767123288e-06,
      "loss": 0.0225,
      "step": 17810
    },
    {
      "epoch": 0.8136986301369863,
      "grad_norm": 0.3925830125808716,
      "learning_rate": 5.598630136986302e-06,
      "loss": 0.0217,
      "step": 17820
    },
    {
      "epoch": 0.8141552511415525,
      "grad_norm": 0.3764375150203705,
      "learning_rate": 5.584931506849315e-06,
      "loss": 0.0177,
      "step": 17830
    },
    {
      "epoch": 0.8146118721461187,
      "grad_norm": 0.7331236600875854,
      "learning_rate": 5.571232876712329e-06,
      "loss": 0.0235,
      "step": 17840
    },
    {
      "epoch": 0.815068493150685,
      "grad_norm": 0.15542027354240417,
      "learning_rate": 5.5575342465753425e-06,
      "loss": 0.019,
      "step": 17850
    },
    {
      "epoch": 0.8155251141552512,
      "grad_norm": 0.6209703087806702,
      "learning_rate": 5.543835616438356e-06,
      "loss": 0.0219,
      "step": 17860
    },
    {
      "epoch": 0.8159817351598173,
      "grad_norm": 0.4551151692867279,
      "learning_rate": 5.53013698630137e-06,
      "loss": 0.0246,
      "step": 17870
    },
    {
      "epoch": 0.8164383561643835,
      "grad_norm": 0.6467505693435669,
      "learning_rate": 5.516438356164384e-06,
      "loss": 0.0278,
      "step": 17880
    },
    {
      "epoch": 0.8168949771689498,
      "grad_norm": 1.462253212928772,
      "learning_rate": 5.5027397260273975e-06,
      "loss": 0.0261,
      "step": 17890
    },
    {
      "epoch": 0.817351598173516,
      "grad_norm": 0.5491830706596375,
      "learning_rate": 5.489041095890411e-06,
      "loss": 0.027,
      "step": 17900
    },
    {
      "epoch": 0.8178082191780822,
      "grad_norm": 1.0760332345962524,
      "learning_rate": 5.475342465753425e-06,
      "loss": 0.0259,
      "step": 17910
    },
    {
      "epoch": 0.8182648401826484,
      "grad_norm": 0.5486295819282532,
      "learning_rate": 5.461643835616439e-06,
      "loss": 0.0168,
      "step": 17920
    },
    {
      "epoch": 0.8187214611872146,
      "grad_norm": 0.3167664706707001,
      "learning_rate": 5.447945205479452e-06,
      "loss": 0.0299,
      "step": 17930
    },
    {
      "epoch": 0.8191780821917808,
      "grad_norm": 0.592397153377533,
      "learning_rate": 5.434246575342466e-06,
      "loss": 0.0243,
      "step": 17940
    },
    {
      "epoch": 0.819634703196347,
      "grad_norm": 0.38012126088142395,
      "learning_rate": 5.420547945205479e-06,
      "loss": 0.0294,
      "step": 17950
    },
    {
      "epoch": 0.8200913242009132,
      "grad_norm": 0.251263290643692,
      "learning_rate": 5.4068493150684935e-06,
      "loss": 0.0169,
      "step": 17960
    },
    {
      "epoch": 0.8205479452054795,
      "grad_norm": 0.7800202369689941,
      "learning_rate": 5.393150684931507e-06,
      "loss": 0.0287,
      "step": 17970
    },
    {
      "epoch": 0.8210045662100457,
      "grad_norm": 0.18309065699577332,
      "learning_rate": 5.379452054794521e-06,
      "loss": 0.0144,
      "step": 17980
    },
    {
      "epoch": 0.8214611872146119,
      "grad_norm": 0.5236823558807373,
      "learning_rate": 5.365753424657535e-06,
      "loss": 0.0244,
      "step": 17990
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 1.399910807609558,
      "learning_rate": 5.352054794520548e-06,
      "loss": 0.0218,
      "step": 18000
    },
    {
      "epoch": 0.8223744292237443,
      "grad_norm": 0.4742036759853363,
      "learning_rate": 5.338356164383562e-06,
      "loss": 0.0279,
      "step": 18010
    },
    {
      "epoch": 0.8228310502283105,
      "grad_norm": 0.49714311957359314,
      "learning_rate": 5.324657534246576e-06,
      "loss": 0.017,
      "step": 18020
    },
    {
      "epoch": 0.8232876712328767,
      "grad_norm": 1.7733087539672852,
      "learning_rate": 5.310958904109589e-06,
      "loss": 0.0215,
      "step": 18030
    },
    {
      "epoch": 0.8237442922374429,
      "grad_norm": 0.5137836933135986,
      "learning_rate": 5.297260273972603e-06,
      "loss": 0.0273,
      "step": 18040
    },
    {
      "epoch": 0.8242009132420092,
      "grad_norm": 0.6408032178878784,
      "learning_rate": 5.283561643835616e-06,
      "loss": 0.0284,
      "step": 18050
    },
    {
      "epoch": 0.8246575342465754,
      "grad_norm": 0.6324028968811035,
      "learning_rate": 5.26986301369863e-06,
      "loss": 0.0218,
      "step": 18060
    },
    {
      "epoch": 0.8251141552511415,
      "grad_norm": 0.5020859837532043,
      "learning_rate": 5.2561643835616445e-06,
      "loss": 0.0277,
      "step": 18070
    },
    {
      "epoch": 0.8255707762557077,
      "grad_norm": 0.3561488389968872,
      "learning_rate": 5.242465753424657e-06,
      "loss": 0.0178,
      "step": 18080
    },
    {
      "epoch": 0.826027397260274,
      "grad_norm": 1.4210765361785889,
      "learning_rate": 5.228767123287672e-06,
      "loss": 0.0299,
      "step": 18090
    },
    {
      "epoch": 0.8264840182648402,
      "grad_norm": 0.48317497968673706,
      "learning_rate": 5.215068493150685e-06,
      "loss": 0.0272,
      "step": 18100
    },
    {
      "epoch": 0.8269406392694064,
      "grad_norm": 0.2290092557668686,
      "learning_rate": 5.2013698630136986e-06,
      "loss": 0.028,
      "step": 18110
    },
    {
      "epoch": 0.8273972602739726,
      "grad_norm": 0.42808330059051514,
      "learning_rate": 5.187671232876713e-06,
      "loss": 0.0125,
      "step": 18120
    },
    {
      "epoch": 0.8278538812785388,
      "grad_norm": 1.0300650596618652,
      "learning_rate": 5.173972602739726e-06,
      "loss": 0.0242,
      "step": 18130
    },
    {
      "epoch": 0.828310502283105,
      "grad_norm": 0.5737107396125793,
      "learning_rate": 5.16027397260274e-06,
      "loss": 0.0236,
      "step": 18140
    },
    {
      "epoch": 0.8287671232876712,
      "grad_norm": 0.883123517036438,
      "learning_rate": 5.1465753424657535e-06,
      "loss": 0.0285,
      "step": 18150
    },
    {
      "epoch": 0.8292237442922374,
      "grad_norm": 0.0729188546538353,
      "learning_rate": 5.132876712328767e-06,
      "loss": 0.0131,
      "step": 18160
    },
    {
      "epoch": 0.8296803652968037,
      "grad_norm": 0.8108103275299072,
      "learning_rate": 5.119178082191782e-06,
      "loss": 0.0226,
      "step": 18170
    },
    {
      "epoch": 0.8301369863013699,
      "grad_norm": 0.8367457389831543,
      "learning_rate": 5.105479452054795e-06,
      "loss": 0.0352,
      "step": 18180
    },
    {
      "epoch": 0.8305936073059361,
      "grad_norm": 0.7653394937515259,
      "learning_rate": 5.091780821917808e-06,
      "loss": 0.0252,
      "step": 18190
    },
    {
      "epoch": 0.8310502283105022,
      "grad_norm": 0.4412447512149811,
      "learning_rate": 5.078082191780822e-06,
      "loss": 0.0213,
      "step": 18200
    },
    {
      "epoch": 0.8315068493150685,
      "grad_norm": 0.5257456302642822,
      "learning_rate": 5.064383561643836e-06,
      "loss": 0.0264,
      "step": 18210
    },
    {
      "epoch": 0.8319634703196347,
      "grad_norm": 0.9902570843696594,
      "learning_rate": 5.0506849315068495e-06,
      "loss": 0.0207,
      "step": 18220
    },
    {
      "epoch": 0.8324200913242009,
      "grad_norm": 0.8805831074714661,
      "learning_rate": 5.036986301369863e-06,
      "loss": 0.0155,
      "step": 18230
    },
    {
      "epoch": 0.8328767123287671,
      "grad_norm": 0.8354770541191101,
      "learning_rate": 5.023287671232877e-06,
      "loss": 0.0202,
      "step": 18240
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.9549419283866882,
      "learning_rate": 5.00958904109589e-06,
      "loss": 0.0321,
      "step": 18250
    },
    {
      "epoch": 0.8337899543378996,
      "grad_norm": 0.6528536081314087,
      "learning_rate": 4.9958904109589044e-06,
      "loss": 0.0213,
      "step": 18260
    },
    {
      "epoch": 0.8342465753424657,
      "grad_norm": 0.6771441102027893,
      "learning_rate": 4.982191780821918e-06,
      "loss": 0.019,
      "step": 18270
    },
    {
      "epoch": 0.8347031963470319,
      "grad_norm": 0.5193824768066406,
      "learning_rate": 4.968493150684932e-06,
      "loss": 0.0218,
      "step": 18280
    },
    {
      "epoch": 0.8351598173515982,
      "grad_norm": 0.5247556567192078,
      "learning_rate": 4.954794520547946e-06,
      "loss": 0.0264,
      "step": 18290
    },
    {
      "epoch": 0.8356164383561644,
      "grad_norm": 0.41886550188064575,
      "learning_rate": 4.9410958904109585e-06,
      "loss": 0.0207,
      "step": 18300
    },
    {
      "epoch": 0.8360730593607306,
      "grad_norm": 0.6234474778175354,
      "learning_rate": 4.927397260273973e-06,
      "loss": 0.021,
      "step": 18310
    },
    {
      "epoch": 0.8365296803652968,
      "grad_norm": 1.130730152130127,
      "learning_rate": 4.913698630136987e-06,
      "loss": 0.0322,
      "step": 18320
    },
    {
      "epoch": 0.836986301369863,
      "grad_norm": 0.5262491106987,
      "learning_rate": 4.9e-06,
      "loss": 0.0169,
      "step": 18330
    },
    {
      "epoch": 0.8374429223744292,
      "grad_norm": 0.7232025265693665,
      "learning_rate": 4.886301369863014e-06,
      "loss": 0.0259,
      "step": 18340
    },
    {
      "epoch": 0.8378995433789954,
      "grad_norm": 0.4152979552745819,
      "learning_rate": 4.872602739726027e-06,
      "loss": 0.0362,
      "step": 18350
    },
    {
      "epoch": 0.8383561643835616,
      "grad_norm": 1.7494763135910034,
      "learning_rate": 4.858904109589042e-06,
      "loss": 0.0282,
      "step": 18360
    },
    {
      "epoch": 0.8388127853881279,
      "grad_norm": 0.8501620888710022,
      "learning_rate": 4.845205479452055e-06,
      "loss": 0.0219,
      "step": 18370
    },
    {
      "epoch": 0.8392694063926941,
      "grad_norm": 0.8275811076164246,
      "learning_rate": 4.831506849315068e-06,
      "loss": 0.0305,
      "step": 18380
    },
    {
      "epoch": 0.8397260273972603,
      "grad_norm": 0.8508495092391968,
      "learning_rate": 4.817808219178083e-06,
      "loss": 0.0323,
      "step": 18390
    },
    {
      "epoch": 0.8401826484018264,
      "grad_norm": 0.6150197982788086,
      "learning_rate": 4.804109589041096e-06,
      "loss": 0.0282,
      "step": 18400
    },
    {
      "epoch": 0.8406392694063927,
      "grad_norm": 0.5533710718154907,
      "learning_rate": 4.7904109589041095e-06,
      "loss": 0.0288,
      "step": 18410
    },
    {
      "epoch": 0.8410958904109589,
      "grad_norm": 0.7792576551437378,
      "learning_rate": 4.776712328767123e-06,
      "loss": 0.0158,
      "step": 18420
    },
    {
      "epoch": 0.8415525114155251,
      "grad_norm": 0.6463973522186279,
      "learning_rate": 4.763013698630137e-06,
      "loss": 0.0188,
      "step": 18430
    },
    {
      "epoch": 0.8420091324200913,
      "grad_norm": 0.5396019816398621,
      "learning_rate": 4.749315068493151e-06,
      "loss": 0.0167,
      "step": 18440
    },
    {
      "epoch": 0.8424657534246576,
      "grad_norm": 1.0988428592681885,
      "learning_rate": 4.735616438356164e-06,
      "loss": 0.0219,
      "step": 18450
    },
    {
      "epoch": 0.8429223744292238,
      "grad_norm": 0.5439891815185547,
      "learning_rate": 4.721917808219178e-06,
      "loss": 0.0285,
      "step": 18460
    },
    {
      "epoch": 0.8433789954337899,
      "grad_norm": 0.5456348657608032,
      "learning_rate": 4.708219178082192e-06,
      "loss": 0.0268,
      "step": 18470
    },
    {
      "epoch": 0.8438356164383561,
      "grad_norm": 0.3431539833545685,
      "learning_rate": 4.6945205479452056e-06,
      "loss": 0.0197,
      "step": 18480
    },
    {
      "epoch": 0.8442922374429224,
      "grad_norm": 0.6829326748847961,
      "learning_rate": 4.680821917808219e-06,
      "loss": 0.0293,
      "step": 18490
    },
    {
      "epoch": 0.8447488584474886,
      "grad_norm": 1.1008923053741455,
      "learning_rate": 4.667123287671233e-06,
      "loss": 0.0181,
      "step": 18500
    },
    {
      "epoch": 0.8452054794520548,
      "grad_norm": 0.1610931158065796,
      "learning_rate": 4.653424657534247e-06,
      "loss": 0.0175,
      "step": 18510
    },
    {
      "epoch": 0.845662100456621,
      "grad_norm": 0.6871063113212585,
      "learning_rate": 4.63972602739726e-06,
      "loss": 0.0318,
      "step": 18520
    },
    {
      "epoch": 0.8461187214611872,
      "grad_norm": 0.7757631540298462,
      "learning_rate": 4.626027397260274e-06,
      "loss": 0.022,
      "step": 18530
    },
    {
      "epoch": 0.8465753424657534,
      "grad_norm": 0.38277629017829895,
      "learning_rate": 4.612328767123288e-06,
      "loss": 0.023,
      "step": 18540
    },
    {
      "epoch": 0.8470319634703196,
      "grad_norm": 0.6345208883285522,
      "learning_rate": 4.598630136986302e-06,
      "loss": 0.0264,
      "step": 18550
    },
    {
      "epoch": 0.8474885844748858,
      "grad_norm": 0.7461292147636414,
      "learning_rate": 4.584931506849315e-06,
      "loss": 0.0234,
      "step": 18560
    },
    {
      "epoch": 0.8479452054794521,
      "grad_norm": 0.5504282712936401,
      "learning_rate": 4.571232876712328e-06,
      "loss": 0.0209,
      "step": 18570
    },
    {
      "epoch": 0.8484018264840183,
      "grad_norm": 0.6044120192527771,
      "learning_rate": 4.557534246575343e-06,
      "loss": 0.0218,
      "step": 18580
    },
    {
      "epoch": 0.8488584474885845,
      "grad_norm": 1.078608512878418,
      "learning_rate": 4.5438356164383565e-06,
      "loss": 0.0188,
      "step": 18590
    },
    {
      "epoch": 0.8493150684931506,
      "grad_norm": 0.6891079545021057,
      "learning_rate": 4.530136986301369e-06,
      "loss": 0.022,
      "step": 18600
    },
    {
      "epoch": 0.8497716894977169,
      "grad_norm": 0.6889792084693909,
      "learning_rate": 4.516438356164384e-06,
      "loss": 0.0263,
      "step": 18610
    },
    {
      "epoch": 0.8502283105022831,
      "grad_norm": 1.1375395059585571,
      "learning_rate": 4.502739726027397e-06,
      "loss": 0.0215,
      "step": 18620
    },
    {
      "epoch": 0.8506849315068493,
      "grad_norm": 0.4555596113204956,
      "learning_rate": 4.489041095890411e-06,
      "loss": 0.0205,
      "step": 18630
    },
    {
      "epoch": 0.8511415525114155,
      "grad_norm": 0.35057029128074646,
      "learning_rate": 4.475342465753425e-06,
      "loss": 0.016,
      "step": 18640
    },
    {
      "epoch": 0.8515981735159818,
      "grad_norm": 0.6673441529273987,
      "learning_rate": 4.461643835616438e-06,
      "loss": 0.0242,
      "step": 18650
    },
    {
      "epoch": 0.852054794520548,
      "grad_norm": 0.594258189201355,
      "learning_rate": 4.447945205479453e-06,
      "loss": 0.021,
      "step": 18660
    },
    {
      "epoch": 0.8525114155251141,
      "grad_norm": 0.5413128137588501,
      "learning_rate": 4.4342465753424655e-06,
      "loss": 0.0224,
      "step": 18670
    },
    {
      "epoch": 0.8529680365296803,
      "grad_norm": 0.7135615348815918,
      "learning_rate": 4.420547945205479e-06,
      "loss": 0.0266,
      "step": 18680
    },
    {
      "epoch": 0.8534246575342466,
      "grad_norm": 0.4163840711116791,
      "learning_rate": 4.406849315068494e-06,
      "loss": 0.03,
      "step": 18690
    },
    {
      "epoch": 0.8538812785388128,
      "grad_norm": 0.24420493841171265,
      "learning_rate": 4.393150684931507e-06,
      "loss": 0.012,
      "step": 18700
    },
    {
      "epoch": 0.854337899543379,
      "grad_norm": 0.45470893383026123,
      "learning_rate": 4.37945205479452e-06,
      "loss": 0.0231,
      "step": 18710
    },
    {
      "epoch": 0.8547945205479452,
      "grad_norm": 0.953275740146637,
      "learning_rate": 4.365753424657534e-06,
      "loss": 0.0164,
      "step": 18720
    },
    {
      "epoch": 0.8552511415525114,
      "grad_norm": 0.3627294600009918,
      "learning_rate": 4.352054794520548e-06,
      "loss": 0.0147,
      "step": 18730
    },
    {
      "epoch": 0.8557077625570776,
      "grad_norm": 0.8409762978553772,
      "learning_rate": 4.338356164383562e-06,
      "loss": 0.0188,
      "step": 18740
    },
    {
      "epoch": 0.8561643835616438,
      "grad_norm": 0.45942816138267517,
      "learning_rate": 4.324657534246575e-06,
      "loss": 0.0302,
      "step": 18750
    },
    {
      "epoch": 0.85662100456621,
      "grad_norm": 0.6320294737815857,
      "learning_rate": 4.310958904109589e-06,
      "loss": 0.0267,
      "step": 18760
    },
    {
      "epoch": 0.8570776255707763,
      "grad_norm": 0.5173050761222839,
      "learning_rate": 4.297260273972603e-06,
      "loss": 0.0238,
      "step": 18770
    },
    {
      "epoch": 0.8575342465753425,
      "grad_norm": 0.9004427790641785,
      "learning_rate": 4.2835616438356165e-06,
      "loss": 0.0197,
      "step": 18780
    },
    {
      "epoch": 0.8579908675799087,
      "grad_norm": 0.19796673953533173,
      "learning_rate": 4.26986301369863e-06,
      "loss": 0.0175,
      "step": 18790
    },
    {
      "epoch": 0.8584474885844748,
      "grad_norm": 0.5569977164268494,
      "learning_rate": 4.256164383561644e-06,
      "loss": 0.027,
      "step": 18800
    },
    {
      "epoch": 0.8589041095890411,
      "grad_norm": 0.7881246209144592,
      "learning_rate": 4.242465753424658e-06,
      "loss": 0.0203,
      "step": 18810
    },
    {
      "epoch": 0.8593607305936073,
      "grad_norm": 0.5678671598434448,
      "learning_rate": 4.228767123287671e-06,
      "loss": 0.0194,
      "step": 18820
    },
    {
      "epoch": 0.8598173515981735,
      "grad_norm": 1.013529658317566,
      "learning_rate": 4.215068493150685e-06,
      "loss": 0.0129,
      "step": 18830
    },
    {
      "epoch": 0.8602739726027397,
      "grad_norm": 1.5427478551864624,
      "learning_rate": 4.201369863013699e-06,
      "loss": 0.0276,
      "step": 18840
    },
    {
      "epoch": 0.860730593607306,
      "grad_norm": 0.2177124321460724,
      "learning_rate": 4.1876712328767125e-06,
      "loss": 0.0183,
      "step": 18850
    },
    {
      "epoch": 0.8611872146118722,
      "grad_norm": 0.7511858940124512,
      "learning_rate": 4.173972602739726e-06,
      "loss": 0.0203,
      "step": 18860
    },
    {
      "epoch": 0.8616438356164383,
      "grad_norm": 0.8812867403030396,
      "learning_rate": 4.160273972602739e-06,
      "loss": 0.035,
      "step": 18870
    },
    {
      "epoch": 0.8621004566210045,
      "grad_norm": 0.566563606262207,
      "learning_rate": 4.146575342465754e-06,
      "loss": 0.0272,
      "step": 18880
    },
    {
      "epoch": 0.8625570776255708,
      "grad_norm": 0.8137831687927246,
      "learning_rate": 4.1328767123287674e-06,
      "loss": 0.0172,
      "step": 18890
    },
    {
      "epoch": 0.863013698630137,
      "grad_norm": 0.44965770840644836,
      "learning_rate": 4.11917808219178e-06,
      "loss": 0.0173,
      "step": 18900
    },
    {
      "epoch": 0.8634703196347032,
      "grad_norm": 0.3301747143268585,
      "learning_rate": 4.105479452054795e-06,
      "loss": 0.0175,
      "step": 18910
    },
    {
      "epoch": 0.8639269406392694,
      "grad_norm": 0.11059793829917908,
      "learning_rate": 4.091780821917808e-06,
      "loss": 0.0227,
      "step": 18920
    },
    {
      "epoch": 0.8643835616438356,
      "grad_norm": 0.3787897825241089,
      "learning_rate": 4.078082191780822e-06,
      "loss": 0.0257,
      "step": 18930
    },
    {
      "epoch": 0.8648401826484018,
      "grad_norm": 0.5563650131225586,
      "learning_rate": 4.064383561643836e-06,
      "loss": 0.0308,
      "step": 18940
    },
    {
      "epoch": 0.865296803652968,
      "grad_norm": 0.4326106011867523,
      "learning_rate": 4.050684931506849e-06,
      "loss": 0.0235,
      "step": 18950
    },
    {
      "epoch": 0.8657534246575342,
      "grad_norm": 0.5951550602912903,
      "learning_rate": 4.0369863013698635e-06,
      "loss": 0.0217,
      "step": 18960
    },
    {
      "epoch": 0.8662100456621005,
      "grad_norm": 0.42782968282699585,
      "learning_rate": 4.023287671232876e-06,
      "loss": 0.0185,
      "step": 18970
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.3982679545879364,
      "learning_rate": 4.00958904109589e-06,
      "loss": 0.0236,
      "step": 18980
    },
    {
      "epoch": 0.8671232876712329,
      "grad_norm": 2.094736099243164,
      "learning_rate": 3.995890410958905e-06,
      "loss": 0.0359,
      "step": 18990
    },
    {
      "epoch": 0.867579908675799,
      "grad_norm": 0.6754417419433594,
      "learning_rate": 3.982191780821918e-06,
      "loss": 0.031,
      "step": 19000
    },
    {
      "epoch": 0.8680365296803653,
      "grad_norm": 0.5316932201385498,
      "learning_rate": 3.968493150684932e-06,
      "loss": 0.0212,
      "step": 19010
    },
    {
      "epoch": 0.8684931506849315,
      "grad_norm": 0.1692752242088318,
      "learning_rate": 3.954794520547945e-06,
      "loss": 0.0187,
      "step": 19020
    },
    {
      "epoch": 0.8689497716894977,
      "grad_norm": 1.351185917854309,
      "learning_rate": 3.941095890410959e-06,
      "loss": 0.032,
      "step": 19030
    },
    {
      "epoch": 0.869406392694064,
      "grad_norm": 1.1626219749450684,
      "learning_rate": 3.927397260273973e-06,
      "loss": 0.021,
      "step": 19040
    },
    {
      "epoch": 0.8698630136986302,
      "grad_norm": 0.5052536129951477,
      "learning_rate": 3.913698630136986e-06,
      "loss": 0.0265,
      "step": 19050
    },
    {
      "epoch": 0.8703196347031964,
      "grad_norm": 0.5992619395256042,
      "learning_rate": 3.9e-06,
      "loss": 0.0218,
      "step": 19060
    },
    {
      "epoch": 0.8707762557077625,
      "grad_norm": 0.5750054121017456,
      "learning_rate": 3.886301369863014e-06,
      "loss": 0.0208,
      "step": 19070
    },
    {
      "epoch": 0.8712328767123287,
      "grad_norm": 0.7174871563911438,
      "learning_rate": 3.872602739726027e-06,
      "loss": 0.0173,
      "step": 19080
    },
    {
      "epoch": 0.871689497716895,
      "grad_norm": 1.1401543617248535,
      "learning_rate": 3.858904109589041e-06,
      "loss": 0.0182,
      "step": 19090
    },
    {
      "epoch": 0.8721461187214612,
      "grad_norm": 0.5244148969650269,
      "learning_rate": 3.845205479452055e-06,
      "loss": 0.0275,
      "step": 19100
    },
    {
      "epoch": 0.8726027397260274,
      "grad_norm": 0.7610889077186584,
      "learning_rate": 3.8315068493150686e-06,
      "loss": 0.0218,
      "step": 19110
    },
    {
      "epoch": 0.8730593607305936,
      "grad_norm": 1.1853952407836914,
      "learning_rate": 3.817808219178082e-06,
      "loss": 0.0306,
      "step": 19120
    },
    {
      "epoch": 0.8735159817351598,
      "grad_norm": 0.42836320400238037,
      "learning_rate": 3.804109589041096e-06,
      "loss": 0.021,
      "step": 19130
    },
    {
      "epoch": 0.873972602739726,
      "grad_norm": 0.6257415413856506,
      "learning_rate": 3.79041095890411e-06,
      "loss": 0.0225,
      "step": 19140
    },
    {
      "epoch": 0.8744292237442922,
      "grad_norm": 0.5643991231918335,
      "learning_rate": 3.776712328767123e-06,
      "loss": 0.0201,
      "step": 19150
    },
    {
      "epoch": 0.8748858447488584,
      "grad_norm": 0.8107609152793884,
      "learning_rate": 3.763013698630137e-06,
      "loss": 0.0135,
      "step": 19160
    },
    {
      "epoch": 0.8753424657534247,
      "grad_norm": 0.27862027287483215,
      "learning_rate": 3.749315068493151e-06,
      "loss": 0.0171,
      "step": 19170
    },
    {
      "epoch": 0.8757990867579909,
      "grad_norm": 0.3997372090816498,
      "learning_rate": 3.7356164383561646e-06,
      "loss": 0.0209,
      "step": 19180
    },
    {
      "epoch": 0.8762557077625571,
      "grad_norm": 1.9995322227478027,
      "learning_rate": 3.721917808219178e-06,
      "loss": 0.0225,
      "step": 19190
    },
    {
      "epoch": 0.8767123287671232,
      "grad_norm": 0.4769447445869446,
      "learning_rate": 3.7082191780821917e-06,
      "loss": 0.0294,
      "step": 19200
    },
    {
      "epoch": 0.8771689497716895,
      "grad_norm": 0.7982714772224426,
      "learning_rate": 3.694520547945206e-06,
      "loss": 0.0361,
      "step": 19210
    },
    {
      "epoch": 0.8776255707762557,
      "grad_norm": 0.8744418025016785,
      "learning_rate": 3.6808219178082195e-06,
      "loss": 0.0268,
      "step": 19220
    },
    {
      "epoch": 0.8780821917808219,
      "grad_norm": 0.28093042969703674,
      "learning_rate": 3.667123287671233e-06,
      "loss": 0.018,
      "step": 19230
    },
    {
      "epoch": 0.8785388127853881,
      "grad_norm": 2.406925916671753,
      "learning_rate": 3.6534246575342466e-06,
      "loss": 0.019,
      "step": 19240
    },
    {
      "epoch": 0.8789954337899544,
      "grad_norm": 0.6828247308731079,
      "learning_rate": 3.6397260273972603e-06,
      "loss": 0.0193,
      "step": 19250
    },
    {
      "epoch": 0.8794520547945206,
      "grad_norm": 0.5597646236419678,
      "learning_rate": 3.626027397260274e-06,
      "loss": 0.026,
      "step": 19260
    },
    {
      "epoch": 0.8799086757990867,
      "grad_norm": 0.4895915389060974,
      "learning_rate": 3.6123287671232877e-06,
      "loss": 0.0221,
      "step": 19270
    },
    {
      "epoch": 0.8803652968036529,
      "grad_norm": 0.4976966977119446,
      "learning_rate": 3.5986301369863015e-06,
      "loss": 0.0246,
      "step": 19280
    },
    {
      "epoch": 0.8808219178082192,
      "grad_norm": 0.8486946821212769,
      "learning_rate": 3.584931506849315e-06,
      "loss": 0.022,
      "step": 19290
    },
    {
      "epoch": 0.8812785388127854,
      "grad_norm": 0.43089741468429565,
      "learning_rate": 3.5712328767123285e-06,
      "loss": 0.014,
      "step": 19300
    },
    {
      "epoch": 0.8817351598173516,
      "grad_norm": 0.8542549014091492,
      "learning_rate": 3.5575342465753426e-06,
      "loss": 0.0287,
      "step": 19310
    },
    {
      "epoch": 0.8821917808219178,
      "grad_norm": 0.7656369209289551,
      "learning_rate": 3.5438356164383564e-06,
      "loss": 0.0306,
      "step": 19320
    },
    {
      "epoch": 0.882648401826484,
      "grad_norm": 0.3699270784854889,
      "learning_rate": 3.53013698630137e-06,
      "loss": 0.0274,
      "step": 19330
    },
    {
      "epoch": 0.8831050228310502,
      "grad_norm": 0.6058011651039124,
      "learning_rate": 3.5164383561643834e-06,
      "loss": 0.0218,
      "step": 19340
    },
    {
      "epoch": 0.8835616438356164,
      "grad_norm": 1.0878461599349976,
      "learning_rate": 3.502739726027397e-06,
      "loss": 0.0265,
      "step": 19350
    },
    {
      "epoch": 0.8840182648401826,
      "grad_norm": 0.47568291425704956,
      "learning_rate": 3.4890410958904113e-06,
      "loss": 0.0298,
      "step": 19360
    },
    {
      "epoch": 0.8844748858447489,
      "grad_norm": 0.40159666538238525,
      "learning_rate": 3.475342465753425e-06,
      "loss": 0.0185,
      "step": 19370
    },
    {
      "epoch": 0.8849315068493151,
      "grad_norm": 0.2810528576374054,
      "learning_rate": 3.4616438356164383e-06,
      "loss": 0.0245,
      "step": 19380
    },
    {
      "epoch": 0.8853881278538813,
      "grad_norm": 3.2393996715545654,
      "learning_rate": 3.447945205479452e-06,
      "loss": 0.0262,
      "step": 19390
    },
    {
      "epoch": 0.8858447488584474,
      "grad_norm": 0.7745272517204285,
      "learning_rate": 3.4342465753424657e-06,
      "loss": 0.0217,
      "step": 19400
    },
    {
      "epoch": 0.8863013698630137,
      "grad_norm": 1.0801748037338257,
      "learning_rate": 3.42054794520548e-06,
      "loss": 0.0273,
      "step": 19410
    },
    {
      "epoch": 0.8867579908675799,
      "grad_norm": 0.4735306203365326,
      "learning_rate": 3.406849315068493e-06,
      "loss": 0.018,
      "step": 19420
    },
    {
      "epoch": 0.8872146118721461,
      "grad_norm": 0.6070054769515991,
      "learning_rate": 3.393150684931507e-06,
      "loss": 0.0298,
      "step": 19430
    },
    {
      "epoch": 0.8876712328767123,
      "grad_norm": 0.5528061985969543,
      "learning_rate": 3.3794520547945206e-06,
      "loss": 0.0208,
      "step": 19440
    },
    {
      "epoch": 0.8881278538812786,
      "grad_norm": 0.4596828818321228,
      "learning_rate": 3.3657534246575344e-06,
      "loss": 0.0184,
      "step": 19450
    },
    {
      "epoch": 0.8885844748858448,
      "grad_norm": 0.6649020314216614,
      "learning_rate": 3.352054794520548e-06,
      "loss": 0.0222,
      "step": 19460
    },
    {
      "epoch": 0.8890410958904109,
      "grad_norm": 0.26165372133255005,
      "learning_rate": 3.338356164383562e-06,
      "loss": 0.0275,
      "step": 19470
    },
    {
      "epoch": 0.8894977168949771,
      "grad_norm": 0.3551785349845886,
      "learning_rate": 3.3246575342465755e-06,
      "loss": 0.0209,
      "step": 19480
    },
    {
      "epoch": 0.8899543378995434,
      "grad_norm": 0.979840874671936,
      "learning_rate": 3.3123287671232876e-06,
      "loss": 0.0214,
      "step": 19490
    },
    {
      "epoch": 0.8904109589041096,
      "grad_norm": 0.1148199588060379,
      "learning_rate": 3.298630136986302e-06,
      "loss": 0.0196,
      "step": 19500
    },
    {
      "epoch": 0.8908675799086758,
      "grad_norm": 0.45033273100852966,
      "learning_rate": 3.284931506849315e-06,
      "loss": 0.019,
      "step": 19510
    },
    {
      "epoch": 0.891324200913242,
      "grad_norm": 0.2582690119743347,
      "learning_rate": 3.271232876712329e-06,
      "loss": 0.0223,
      "step": 19520
    },
    {
      "epoch": 0.8917808219178082,
      "grad_norm": 0.3601329028606415,
      "learning_rate": 3.2575342465753425e-06,
      "loss": 0.0246,
      "step": 19530
    },
    {
      "epoch": 0.8922374429223744,
      "grad_norm": 0.5488972663879395,
      "learning_rate": 3.243835616438356e-06,
      "loss": 0.0223,
      "step": 19540
    },
    {
      "epoch": 0.8926940639269406,
      "grad_norm": 0.37234970927238464,
      "learning_rate": 3.23013698630137e-06,
      "loss": 0.0206,
      "step": 19550
    },
    {
      "epoch": 0.8931506849315068,
      "grad_norm": 1.1653488874435425,
      "learning_rate": 3.2164383561643837e-06,
      "loss": 0.0322,
      "step": 19560
    },
    {
      "epoch": 0.8936073059360731,
      "grad_norm": 0.632815957069397,
      "learning_rate": 3.2027397260273974e-06,
      "loss": 0.0245,
      "step": 19570
    },
    {
      "epoch": 0.8940639269406393,
      "grad_norm": 0.6228833794593811,
      "learning_rate": 3.1890410958904108e-06,
      "loss": 0.0224,
      "step": 19580
    },
    {
      "epoch": 0.8945205479452055,
      "grad_norm": 0.9378028512001038,
      "learning_rate": 3.1753424657534245e-06,
      "loss": 0.0267,
      "step": 19590
    },
    {
      "epoch": 0.8949771689497716,
      "grad_norm": 0.647310733795166,
      "learning_rate": 3.1616438356164386e-06,
      "loss": 0.0312,
      "step": 19600
    },
    {
      "epoch": 0.8954337899543379,
      "grad_norm": 1.0755749940872192,
      "learning_rate": 3.1479452054794524e-06,
      "loss": 0.0203,
      "step": 19610
    },
    {
      "epoch": 0.8958904109589041,
      "grad_norm": 0.6152238845825195,
      "learning_rate": 3.1342465753424657e-06,
      "loss": 0.0271,
      "step": 19620
    },
    {
      "epoch": 0.8963470319634703,
      "grad_norm": 0.87257319688797,
      "learning_rate": 3.1205479452054794e-06,
      "loss": 0.0276,
      "step": 19630
    },
    {
      "epoch": 0.8968036529680365,
      "grad_norm": 0.6646199226379395,
      "learning_rate": 3.106849315068493e-06,
      "loss": 0.0224,
      "step": 19640
    },
    {
      "epoch": 0.8972602739726028,
      "grad_norm": 0.4287113845348358,
      "learning_rate": 3.0931506849315073e-06,
      "loss": 0.0285,
      "step": 19650
    },
    {
      "epoch": 0.897716894977169,
      "grad_norm": 0.4287286698818207,
      "learning_rate": 3.0794520547945206e-06,
      "loss": 0.0202,
      "step": 19660
    },
    {
      "epoch": 0.8981735159817351,
      "grad_norm": 0.5248835682868958,
      "learning_rate": 3.0657534246575343e-06,
      "loss": 0.027,
      "step": 19670
    },
    {
      "epoch": 0.8986301369863013,
      "grad_norm": 0.8751994967460632,
      "learning_rate": 3.052054794520548e-06,
      "loss": 0.0228,
      "step": 19680
    },
    {
      "epoch": 0.8990867579908676,
      "grad_norm": 0.9280123710632324,
      "learning_rate": 3.0383561643835617e-06,
      "loss": 0.027,
      "step": 19690
    },
    {
      "epoch": 0.8995433789954338,
      "grad_norm": 0.7210422158241272,
      "learning_rate": 3.0246575342465755e-06,
      "loss": 0.03,
      "step": 19700
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.43353402614593506,
      "learning_rate": 3.010958904109589e-06,
      "loss": 0.0271,
      "step": 19710
    },
    {
      "epoch": 0.9004566210045662,
      "grad_norm": 0.4024425148963928,
      "learning_rate": 2.997260273972603e-06,
      "loss": 0.0241,
      "step": 19720
    },
    {
      "epoch": 0.9009132420091325,
      "grad_norm": 2.3928370475769043,
      "learning_rate": 2.983561643835616e-06,
      "loss": 0.0217,
      "step": 19730
    },
    {
      "epoch": 0.9013698630136986,
      "grad_norm": 0.9507681727409363,
      "learning_rate": 2.96986301369863e-06,
      "loss": 0.0292,
      "step": 19740
    },
    {
      "epoch": 0.9018264840182648,
      "grad_norm": 0.7142447829246521,
      "learning_rate": 2.956164383561644e-06,
      "loss": 0.0206,
      "step": 19750
    },
    {
      "epoch": 0.902283105022831,
      "grad_norm": 0.0863836258649826,
      "learning_rate": 2.942465753424658e-06,
      "loss": 0.0281,
      "step": 19760
    },
    {
      "epoch": 0.9027397260273973,
      "grad_norm": 0.41359731554985046,
      "learning_rate": 2.928767123287671e-06,
      "loss": 0.0181,
      "step": 19770
    },
    {
      "epoch": 0.9031963470319635,
      "grad_norm": 0.2234451025724411,
      "learning_rate": 2.915068493150685e-06,
      "loss": 0.0177,
      "step": 19780
    },
    {
      "epoch": 0.9036529680365297,
      "grad_norm": 1.054512619972229,
      "learning_rate": 2.9013698630136986e-06,
      "loss": 0.0245,
      "step": 19790
    },
    {
      "epoch": 0.9041095890410958,
      "grad_norm": 0.5855626463890076,
      "learning_rate": 2.8876712328767127e-06,
      "loss": 0.0274,
      "step": 19800
    },
    {
      "epoch": 0.9045662100456621,
      "grad_norm": 0.4350617825984955,
      "learning_rate": 2.873972602739726e-06,
      "loss": 0.0188,
      "step": 19810
    },
    {
      "epoch": 0.9050228310502283,
      "grad_norm": 0.47473031282424927,
      "learning_rate": 2.8602739726027397e-06,
      "loss": 0.0218,
      "step": 19820
    },
    {
      "epoch": 0.9054794520547945,
      "grad_norm": 0.8862059116363525,
      "learning_rate": 2.8465753424657535e-06,
      "loss": 0.026,
      "step": 19830
    },
    {
      "epoch": 0.9059360730593607,
      "grad_norm": 0.5772098302841187,
      "learning_rate": 2.832876712328767e-06,
      "loss": 0.0312,
      "step": 19840
    },
    {
      "epoch": 0.906392694063927,
      "grad_norm": 0.22638079524040222,
      "learning_rate": 2.819178082191781e-06,
      "loss": 0.0208,
      "step": 19850
    },
    {
      "epoch": 0.9068493150684932,
      "grad_norm": 0.5195489525794983,
      "learning_rate": 2.8054794520547946e-06,
      "loss": 0.022,
      "step": 19860
    },
    {
      "epoch": 0.9073059360730593,
      "grad_norm": 0.8625880479812622,
      "learning_rate": 2.7917808219178084e-06,
      "loss": 0.0215,
      "step": 19870
    },
    {
      "epoch": 0.9077625570776255,
      "grad_norm": 0.15627701580524445,
      "learning_rate": 2.778082191780822e-06,
      "loss": 0.0212,
      "step": 19880
    },
    {
      "epoch": 0.9082191780821918,
      "grad_norm": 1.098599910736084,
      "learning_rate": 2.7643835616438354e-06,
      "loss": 0.0252,
      "step": 19890
    },
    {
      "epoch": 0.908675799086758,
      "grad_norm": 0.31202009320259094,
      "learning_rate": 2.7506849315068495e-06,
      "loss": 0.0154,
      "step": 19900
    },
    {
      "epoch": 0.9091324200913242,
      "grad_norm": 1.1556715965270996,
      "learning_rate": 2.7369863013698633e-06,
      "loss": 0.0352,
      "step": 19910
    },
    {
      "epoch": 0.9095890410958904,
      "grad_norm": 0.789956271648407,
      "learning_rate": 2.723287671232877e-06,
      "loss": 0.0294,
      "step": 19920
    },
    {
      "epoch": 0.9100456621004567,
      "grad_norm": 0.7299273610115051,
      "learning_rate": 2.7095890410958903e-06,
      "loss": 0.0347,
      "step": 19930
    },
    {
      "epoch": 0.9105022831050228,
      "grad_norm": 0.6021791696548462,
      "learning_rate": 2.695890410958904e-06,
      "loss": 0.0256,
      "step": 19940
    },
    {
      "epoch": 0.910958904109589,
      "grad_norm": 0.3128826320171356,
      "learning_rate": 2.682191780821918e-06,
      "loss": 0.0197,
      "step": 19950
    },
    {
      "epoch": 0.9114155251141552,
      "grad_norm": 0.7363039255142212,
      "learning_rate": 2.6684931506849315e-06,
      "loss": 0.0271,
      "step": 19960
    },
    {
      "epoch": 0.9118721461187215,
      "grad_norm": 0.466982364654541,
      "learning_rate": 2.654794520547945e-06,
      "loss": 0.0269,
      "step": 19970
    },
    {
      "epoch": 0.9123287671232877,
      "grad_norm": 0.825042188167572,
      "learning_rate": 2.641095890410959e-06,
      "loss": 0.0245,
      "step": 19980
    },
    {
      "epoch": 0.9127853881278539,
      "grad_norm": 0.876756489276886,
      "learning_rate": 2.6273972602739726e-06,
      "loss": 0.0188,
      "step": 19990
    },
    {
      "epoch": 0.91324200913242,
      "grad_norm": 0.5343672633171082,
      "learning_rate": 2.6136986301369864e-06,
      "loss": 0.0253,
      "step": 20000
    },
    {
      "epoch": 0.9136986301369863,
      "grad_norm": 0.3650079667568207,
      "learning_rate": 2.6e-06,
      "loss": 0.0174,
      "step": 20010
    },
    {
      "epoch": 0.9141552511415525,
      "grad_norm": 0.7514462471008301,
      "learning_rate": 2.586301369863014e-06,
      "loss": 0.0208,
      "step": 20020
    },
    {
      "epoch": 0.9146118721461187,
      "grad_norm": 0.36934971809387207,
      "learning_rate": 2.5726027397260275e-06,
      "loss": 0.0209,
      "step": 20030
    },
    {
      "epoch": 0.915068493150685,
      "grad_norm": 1.1053868532180786,
      "learning_rate": 2.558904109589041e-06,
      "loss": 0.0307,
      "step": 20040
    },
    {
      "epoch": 0.9155251141552512,
      "grad_norm": 0.44927576184272766,
      "learning_rate": 2.545205479452055e-06,
      "loss": 0.0152,
      "step": 20050
    },
    {
      "epoch": 0.9159817351598174,
      "grad_norm": 0.36324259638786316,
      "learning_rate": 2.5315068493150687e-06,
      "loss": 0.0254,
      "step": 20060
    },
    {
      "epoch": 0.9164383561643835,
      "grad_norm": 0.47377949953079224,
      "learning_rate": 2.5178082191780824e-06,
      "loss": 0.0197,
      "step": 20070
    },
    {
      "epoch": 0.9168949771689497,
      "grad_norm": 0.6635068655014038,
      "learning_rate": 2.5041095890410957e-06,
      "loss": 0.0203,
      "step": 20080
    },
    {
      "epoch": 0.917351598173516,
      "grad_norm": 0.4941372573375702,
      "learning_rate": 2.4904109589041095e-06,
      "loss": 0.0201,
      "step": 20090
    },
    {
      "epoch": 0.9178082191780822,
      "grad_norm": 0.48034146428108215,
      "learning_rate": 2.4767123287671236e-06,
      "loss": 0.0183,
      "step": 20100
    },
    {
      "epoch": 0.9182648401826484,
      "grad_norm": 2.2856075763702393,
      "learning_rate": 2.4630136986301373e-06,
      "loss": 0.0209,
      "step": 20110
    },
    {
      "epoch": 0.9187214611872146,
      "grad_norm": 0.49644139409065247,
      "learning_rate": 2.4493150684931506e-06,
      "loss": 0.0171,
      "step": 20120
    },
    {
      "epoch": 0.9191780821917809,
      "grad_norm": 0.5628954172134399,
      "learning_rate": 2.4356164383561644e-06,
      "loss": 0.017,
      "step": 20130
    },
    {
      "epoch": 0.919634703196347,
      "grad_norm": 0.8702030777931213,
      "learning_rate": 2.421917808219178e-06,
      "loss": 0.0217,
      "step": 20140
    },
    {
      "epoch": 0.9200913242009132,
      "grad_norm": 0.4220215976238251,
      "learning_rate": 2.408219178082192e-06,
      "loss": 0.0183,
      "step": 20150
    },
    {
      "epoch": 0.9205479452054794,
      "grad_norm": 1.0106067657470703,
      "learning_rate": 2.3945205479452056e-06,
      "loss": 0.0236,
      "step": 20160
    },
    {
      "epoch": 0.9210045662100457,
      "grad_norm": 0.4257809817790985,
      "learning_rate": 2.3808219178082193e-06,
      "loss": 0.0256,
      "step": 20170
    },
    {
      "epoch": 0.9214611872146119,
      "grad_norm": 0.5130228996276855,
      "learning_rate": 2.367123287671233e-06,
      "loss": 0.0338,
      "step": 20180
    },
    {
      "epoch": 0.9219178082191781,
      "grad_norm": 0.6619027256965637,
      "learning_rate": 2.3534246575342463e-06,
      "loss": 0.0228,
      "step": 20190
    },
    {
      "epoch": 0.9223744292237442,
      "grad_norm": 0.6210156083106995,
      "learning_rate": 2.3397260273972605e-06,
      "loss": 0.0316,
      "step": 20200
    },
    {
      "epoch": 0.9228310502283105,
      "grad_norm": 0.42652052640914917,
      "learning_rate": 2.326027397260274e-06,
      "loss": 0.0132,
      "step": 20210
    },
    {
      "epoch": 0.9232876712328767,
      "grad_norm": 0.10864794254302979,
      "learning_rate": 2.312328767123288e-06,
      "loss": 0.0161,
      "step": 20220
    },
    {
      "epoch": 0.9237442922374429,
      "grad_norm": 1.0103888511657715,
      "learning_rate": 2.298630136986301e-06,
      "loss": 0.0266,
      "step": 20230
    },
    {
      "epoch": 0.9242009132420091,
      "grad_norm": 0.7301708459854126,
      "learning_rate": 2.284931506849315e-06,
      "loss": 0.0258,
      "step": 20240
    },
    {
      "epoch": 0.9246575342465754,
      "grad_norm": 0.4881691634654999,
      "learning_rate": 2.271232876712329e-06,
      "loss": 0.0219,
      "step": 20250
    },
    {
      "epoch": 0.9251141552511416,
      "grad_norm": 0.49669626355171204,
      "learning_rate": 2.257534246575343e-06,
      "loss": 0.0228,
      "step": 20260
    },
    {
      "epoch": 0.9255707762557077,
      "grad_norm": 1.0067468881607056,
      "learning_rate": 2.243835616438356e-06,
      "loss": 0.0179,
      "step": 20270
    },
    {
      "epoch": 0.9260273972602739,
      "grad_norm": 0.45379766821861267,
      "learning_rate": 2.23013698630137e-06,
      "loss": 0.0247,
      "step": 20280
    },
    {
      "epoch": 0.9264840182648402,
      "grad_norm": 0.2993355393409729,
      "learning_rate": 2.2164383561643836e-06,
      "loss": 0.0179,
      "step": 20290
    },
    {
      "epoch": 0.9269406392694064,
      "grad_norm": 2.0401923656463623,
      "learning_rate": 2.2027397260273977e-06,
      "loss": 0.0259,
      "step": 20300
    },
    {
      "epoch": 0.9273972602739726,
      "grad_norm": 0.7073390483856201,
      "learning_rate": 2.189041095890411e-06,
      "loss": 0.0155,
      "step": 20310
    },
    {
      "epoch": 0.9278538812785389,
      "grad_norm": 0.9529210329055786,
      "learning_rate": 2.1753424657534247e-06,
      "loss": 0.0247,
      "step": 20320
    },
    {
      "epoch": 0.9283105022831051,
      "grad_norm": 0.8644070625305176,
      "learning_rate": 2.1616438356164385e-06,
      "loss": 0.027,
      "step": 20330
    },
    {
      "epoch": 0.9287671232876712,
      "grad_norm": 0.9246038794517517,
      "learning_rate": 2.147945205479452e-06,
      "loss": 0.0176,
      "step": 20340
    },
    {
      "epoch": 0.9292237442922374,
      "grad_norm": 0.3642401695251465,
      "learning_rate": 2.134246575342466e-06,
      "loss": 0.0187,
      "step": 20350
    },
    {
      "epoch": 0.9296803652968036,
      "grad_norm": 0.37298527359962463,
      "learning_rate": 2.1205479452054796e-06,
      "loss": 0.0233,
      "step": 20360
    },
    {
      "epoch": 0.9301369863013699,
      "grad_norm": 0.8540728092193604,
      "learning_rate": 2.1068493150684934e-06,
      "loss": 0.0206,
      "step": 20370
    },
    {
      "epoch": 0.9305936073059361,
      "grad_norm": 0.485025018453598,
      "learning_rate": 2.0931506849315067e-06,
      "loss": 0.0204,
      "step": 20380
    },
    {
      "epoch": 0.9310502283105023,
      "grad_norm": 0.35306239128112793,
      "learning_rate": 2.0794520547945204e-06,
      "loss": 0.0252,
      "step": 20390
    },
    {
      "epoch": 0.9315068493150684,
      "grad_norm": 0.43253499269485474,
      "learning_rate": 2.0657534246575345e-06,
      "loss": 0.02,
      "step": 20400
    },
    {
      "epoch": 0.9319634703196347,
      "grad_norm": 0.8610605597496033,
      "learning_rate": 2.0520547945205483e-06,
      "loss": 0.0267,
      "step": 20410
    },
    {
      "epoch": 0.9324200913242009,
      "grad_norm": 1.1494308710098267,
      "learning_rate": 2.0397260273972604e-06,
      "loss": 0.025,
      "step": 20420
    },
    {
      "epoch": 0.9328767123287671,
      "grad_norm": 0.4744187593460083,
      "learning_rate": 2.0260273972602737e-06,
      "loss": 0.0164,
      "step": 20430
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.160841703414917,
      "learning_rate": 2.012328767123288e-06,
      "loss": 0.0249,
      "step": 20440
    },
    {
      "epoch": 0.9337899543378996,
      "grad_norm": 0.832227885723114,
      "learning_rate": 2e-06,
      "loss": 0.0261,
      "step": 20450
    },
    {
      "epoch": 0.9342465753424658,
      "grad_norm": 0.6511160731315613,
      "learning_rate": 1.9863013698630136e-06,
      "loss": 0.0161,
      "step": 20460
    },
    {
      "epoch": 0.9347031963470319,
      "grad_norm": 0.964021623134613,
      "learning_rate": 1.9726027397260274e-06,
      "loss": 0.024,
      "step": 20470
    },
    {
      "epoch": 0.9351598173515981,
      "grad_norm": 0.6252279877662659,
      "learning_rate": 1.958904109589041e-06,
      "loss": 0.0186,
      "step": 20480
    },
    {
      "epoch": 0.9356164383561644,
      "grad_norm": 0.30829063057899475,
      "learning_rate": 1.945205479452055e-06,
      "loss": 0.0217,
      "step": 20490
    },
    {
      "epoch": 0.9360730593607306,
      "grad_norm": 0.07961542904376984,
      "learning_rate": 1.9315068493150685e-06,
      "loss": 0.0206,
      "step": 20500
    },
    {
      "epoch": 0.9365296803652968,
      "grad_norm": 0.6452302932739258,
      "learning_rate": 1.9178082191780823e-06,
      "loss": 0.0278,
      "step": 20510
    },
    {
      "epoch": 0.936986301369863,
      "grad_norm": 0.6022934317588806,
      "learning_rate": 1.9041095890410958e-06,
      "loss": 0.0295,
      "step": 20520
    },
    {
      "epoch": 0.9374429223744293,
      "grad_norm": 0.3694252371788025,
      "learning_rate": 1.8904109589041097e-06,
      "loss": 0.0199,
      "step": 20530
    },
    {
      "epoch": 0.9378995433789954,
      "grad_norm": 0.48064666986465454,
      "learning_rate": 1.8767123287671234e-06,
      "loss": 0.0296,
      "step": 20540
    },
    {
      "epoch": 0.9383561643835616,
      "grad_norm": 0.4555618166923523,
      "learning_rate": 1.863013698630137e-06,
      "loss": 0.029,
      "step": 20550
    },
    {
      "epoch": 0.9388127853881278,
      "grad_norm": 1.1632397174835205,
      "learning_rate": 1.8493150684931507e-06,
      "loss": 0.0216,
      "step": 20560
    },
    {
      "epoch": 0.9392694063926941,
      "grad_norm": 0.6620786786079407,
      "learning_rate": 1.8356164383561644e-06,
      "loss": 0.0183,
      "step": 20570
    },
    {
      "epoch": 0.9397260273972603,
      "grad_norm": 0.4269220232963562,
      "learning_rate": 1.8219178082191781e-06,
      "loss": 0.0169,
      "step": 20580
    },
    {
      "epoch": 0.9401826484018265,
      "grad_norm": 1.3933807611465454,
      "learning_rate": 1.8082191780821919e-06,
      "loss": 0.0277,
      "step": 20590
    },
    {
      "epoch": 0.9406392694063926,
      "grad_norm": 0.5291165709495544,
      "learning_rate": 1.7945205479452056e-06,
      "loss": 0.0262,
      "step": 20600
    },
    {
      "epoch": 0.9410958904109589,
      "grad_norm": 0.6761700510978699,
      "learning_rate": 1.780821917808219e-06,
      "loss": 0.0234,
      "step": 20610
    },
    {
      "epoch": 0.9415525114155251,
      "grad_norm": 0.33826014399528503,
      "learning_rate": 1.767123287671233e-06,
      "loss": 0.0199,
      "step": 20620
    },
    {
      "epoch": 0.9420091324200913,
      "grad_norm": 0.39581814408302307,
      "learning_rate": 1.7534246575342465e-06,
      "loss": 0.0241,
      "step": 20630
    },
    {
      "epoch": 0.9424657534246575,
      "grad_norm": 0.33488729596138,
      "learning_rate": 1.7397260273972605e-06,
      "loss": 0.0255,
      "step": 20640
    },
    {
      "epoch": 0.9429223744292238,
      "grad_norm": 0.4150344729423523,
      "learning_rate": 1.726027397260274e-06,
      "loss": 0.0139,
      "step": 20650
    },
    {
      "epoch": 0.94337899543379,
      "grad_norm": 1.2112957239151,
      "learning_rate": 1.7123287671232877e-06,
      "loss": 0.0216,
      "step": 20660
    },
    {
      "epoch": 0.9438356164383561,
      "grad_norm": 0.6824030876159668,
      "learning_rate": 1.6986301369863014e-06,
      "loss": 0.0226,
      "step": 20670
    },
    {
      "epoch": 0.9442922374429223,
      "grad_norm": 0.5088287591934204,
      "learning_rate": 1.684931506849315e-06,
      "loss": 0.0272,
      "step": 20680
    },
    {
      "epoch": 0.9447488584474886,
      "grad_norm": 0.2853359878063202,
      "learning_rate": 1.671232876712329e-06,
      "loss": 0.0184,
      "step": 20690
    },
    {
      "epoch": 0.9452054794520548,
      "grad_norm": 0.7581607103347778,
      "learning_rate": 1.6575342465753424e-06,
      "loss": 0.0297,
      "step": 20700
    },
    {
      "epoch": 0.945662100456621,
      "grad_norm": 0.8169159889221191,
      "learning_rate": 1.6438356164383561e-06,
      "loss": 0.022,
      "step": 20710
    },
    {
      "epoch": 0.9461187214611873,
      "grad_norm": 0.4841727614402771,
      "learning_rate": 1.6301369863013699e-06,
      "loss": 0.0209,
      "step": 20720
    },
    {
      "epoch": 0.9465753424657535,
      "grad_norm": 0.3713386058807373,
      "learning_rate": 1.6164383561643836e-06,
      "loss": 0.0257,
      "step": 20730
    },
    {
      "epoch": 0.9470319634703196,
      "grad_norm": 0.641516387462616,
      "learning_rate": 1.6027397260273973e-06,
      "loss": 0.0327,
      "step": 20740
    },
    {
      "epoch": 0.9474885844748858,
      "grad_norm": 0.6592110395431519,
      "learning_rate": 1.589041095890411e-06,
      "loss": 0.0252,
      "step": 20750
    },
    {
      "epoch": 0.947945205479452,
      "grad_norm": 0.01711394265294075,
      "learning_rate": 1.5753424657534245e-06,
      "loss": 0.0201,
      "step": 20760
    },
    {
      "epoch": 0.9484018264840183,
      "grad_norm": 0.4591147303581238,
      "learning_rate": 1.5616438356164385e-06,
      "loss": 0.0207,
      "step": 20770
    },
    {
      "epoch": 0.9488584474885845,
      "grad_norm": 0.580377459526062,
      "learning_rate": 1.547945205479452e-06,
      "loss": 0.022,
      "step": 20780
    },
    {
      "epoch": 0.9493150684931507,
      "grad_norm": 0.20760928094387054,
      "learning_rate": 1.534246575342466e-06,
      "loss": 0.0231,
      "step": 20790
    },
    {
      "epoch": 0.9497716894977168,
      "grad_norm": 0.5650467872619629,
      "learning_rate": 1.5205479452054794e-06,
      "loss": 0.0213,
      "step": 20800
    },
    {
      "epoch": 0.9502283105022831,
      "grad_norm": 0.7850517630577087,
      "learning_rate": 1.5068493150684932e-06,
      "loss": 0.0278,
      "step": 20810
    },
    {
      "epoch": 0.9506849315068493,
      "grad_norm": 1.3104569911956787,
      "learning_rate": 1.493150684931507e-06,
      "loss": 0.0198,
      "step": 20820
    },
    {
      "epoch": 0.9511415525114155,
      "grad_norm": 1.2526092529296875,
      "learning_rate": 1.4794520547945206e-06,
      "loss": 0.0221,
      "step": 20830
    },
    {
      "epoch": 0.9515981735159817,
      "grad_norm": 0.2347041517496109,
      "learning_rate": 1.4657534246575344e-06,
      "loss": 0.0158,
      "step": 20840
    },
    {
      "epoch": 0.952054794520548,
      "grad_norm": 0.9933421611785889,
      "learning_rate": 1.452054794520548e-06,
      "loss": 0.0237,
      "step": 20850
    },
    {
      "epoch": 0.9525114155251142,
      "grad_norm": 0.4978543817996979,
      "learning_rate": 1.4383561643835616e-06,
      "loss": 0.0275,
      "step": 20860
    },
    {
      "epoch": 0.9529680365296803,
      "grad_norm": 0.4658099412918091,
      "learning_rate": 1.4246575342465755e-06,
      "loss": 0.0184,
      "step": 20870
    },
    {
      "epoch": 0.9534246575342465,
      "grad_norm": 0.8321106433868408,
      "learning_rate": 1.410958904109589e-06,
      "loss": 0.0204,
      "step": 20880
    },
    {
      "epoch": 0.9538812785388128,
      "grad_norm": 0.87684565782547,
      "learning_rate": 1.3972602739726028e-06,
      "loss": 0.0244,
      "step": 20890
    },
    {
      "epoch": 0.954337899543379,
      "grad_norm": 0.233599454164505,
      "learning_rate": 1.3835616438356165e-06,
      "loss": 0.0188,
      "step": 20900
    },
    {
      "epoch": 0.9547945205479452,
      "grad_norm": 0.7198092937469482,
      "learning_rate": 1.36986301369863e-06,
      "loss": 0.0214,
      "step": 20910
    },
    {
      "epoch": 0.9552511415525115,
      "grad_norm": 0.0651002824306488,
      "learning_rate": 1.356164383561644e-06,
      "loss": 0.015,
      "step": 20920
    },
    {
      "epoch": 0.9557077625570777,
      "grad_norm": 0.5999169945716858,
      "learning_rate": 1.3424657534246575e-06,
      "loss": 0.032,
      "step": 20930
    },
    {
      "epoch": 0.9561643835616438,
      "grad_norm": 0.6048145294189453,
      "learning_rate": 1.3287671232876714e-06,
      "loss": 0.01,
      "step": 20940
    },
    {
      "epoch": 0.95662100456621,
      "grad_norm": 0.6225705742835999,
      "learning_rate": 1.315068493150685e-06,
      "loss": 0.025,
      "step": 20950
    },
    {
      "epoch": 0.9570776255707762,
      "grad_norm": 0.6818641424179077,
      "learning_rate": 1.3013698630136986e-06,
      "loss": 0.0125,
      "step": 20960
    },
    {
      "epoch": 0.9575342465753425,
      "grad_norm": 0.34018316864967346,
      "learning_rate": 1.2876712328767124e-06,
      "loss": 0.0203,
      "step": 20970
    },
    {
      "epoch": 0.9579908675799087,
      "grad_norm": 1.0045961141586304,
      "learning_rate": 1.273972602739726e-06,
      "loss": 0.0161,
      "step": 20980
    },
    {
      "epoch": 0.9584474885844749,
      "grad_norm": 0.8542181253433228,
      "learning_rate": 1.2602739726027398e-06,
      "loss": 0.024,
      "step": 20990
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 0.4896192252635956,
      "learning_rate": 1.2465753424657535e-06,
      "loss": 0.0215,
      "step": 21000
    },
    {
      "epoch": 0.9593607305936073,
      "grad_norm": 0.7724560499191284,
      "learning_rate": 1.232876712328767e-06,
      "loss": 0.0289,
      "step": 21010
    },
    {
      "epoch": 0.9598173515981735,
      "grad_norm": 0.6219921708106995,
      "learning_rate": 1.219178082191781e-06,
      "loss": 0.0215,
      "step": 21020
    },
    {
      "epoch": 0.9602739726027397,
      "grad_norm": 0.7525526285171509,
      "learning_rate": 1.2054794520547945e-06,
      "loss": 0.0212,
      "step": 21030
    },
    {
      "epoch": 0.960730593607306,
      "grad_norm": 0.35424473881721497,
      "learning_rate": 1.1917808219178084e-06,
      "loss": 0.0206,
      "step": 21040
    },
    {
      "epoch": 0.9611872146118722,
      "grad_norm": 0.34446191787719727,
      "learning_rate": 1.178082191780822e-06,
      "loss": 0.0269,
      "step": 21050
    },
    {
      "epoch": 0.9616438356164384,
      "grad_norm": 0.3637799620628357,
      "learning_rate": 1.1643835616438357e-06,
      "loss": 0.021,
      "step": 21060
    },
    {
      "epoch": 0.9621004566210045,
      "grad_norm": 0.6857439875602722,
      "learning_rate": 1.1506849315068494e-06,
      "loss": 0.0227,
      "step": 21070
    },
    {
      "epoch": 0.9625570776255707,
      "grad_norm": 0.36021238565444946,
      "learning_rate": 1.1369863013698631e-06,
      "loss": 0.0216,
      "step": 21080
    },
    {
      "epoch": 0.963013698630137,
      "grad_norm": 0.656461238861084,
      "learning_rate": 1.1232876712328766e-06,
      "loss": 0.0206,
      "step": 21090
    },
    {
      "epoch": 0.9634703196347032,
      "grad_norm": 1.4309531450271606,
      "learning_rate": 1.1095890410958904e-06,
      "loss": 0.0186,
      "step": 21100
    },
    {
      "epoch": 0.9639269406392694,
      "grad_norm": 0.852100670337677,
      "learning_rate": 1.095890410958904e-06,
      "loss": 0.029,
      "step": 21110
    },
    {
      "epoch": 0.9643835616438357,
      "grad_norm": 0.2674671411514282,
      "learning_rate": 1.0821917808219178e-06,
      "loss": 0.0225,
      "step": 21120
    },
    {
      "epoch": 0.9648401826484019,
      "grad_norm": 0.3685532510280609,
      "learning_rate": 1.0684931506849315e-06,
      "loss": 0.0201,
      "step": 21130
    },
    {
      "epoch": 0.965296803652968,
      "grad_norm": 0.5742667317390442,
      "learning_rate": 1.054794520547945e-06,
      "loss": 0.0243,
      "step": 21140
    },
    {
      "epoch": 0.9657534246575342,
      "grad_norm": 0.2525191307067871,
      "learning_rate": 1.041095890410959e-06,
      "loss": 0.0153,
      "step": 21150
    },
    {
      "epoch": 0.9662100456621004,
      "grad_norm": 0.4919895529747009,
      "learning_rate": 1.0273972602739725e-06,
      "loss": 0.023,
      "step": 21160
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 1.064698576927185,
      "learning_rate": 1.0136986301369864e-06,
      "loss": 0.0235,
      "step": 21170
    },
    {
      "epoch": 0.9671232876712329,
      "grad_norm": 0.555169403553009,
      "learning_rate": 1e-06,
      "loss": 0.0202,
      "step": 21180
    },
    {
      "epoch": 0.9675799086757991,
      "grad_norm": 0.7288471460342407,
      "learning_rate": 9.863013698630137e-07,
      "loss": 0.0221,
      "step": 21190
    },
    {
      "epoch": 0.9680365296803652,
      "grad_norm": 0.3859744668006897,
      "learning_rate": 9.726027397260274e-07,
      "loss": 0.0214,
      "step": 21200
    },
    {
      "epoch": 0.9684931506849315,
      "grad_norm": 0.6904558539390564,
      "learning_rate": 9.589041095890411e-07,
      "loss": 0.0198,
      "step": 21210
    },
    {
      "epoch": 0.9689497716894977,
      "grad_norm": 0.6488252282142639,
      "learning_rate": 9.452054794520549e-07,
      "loss": 0.0262,
      "step": 21220
    },
    {
      "epoch": 0.9694063926940639,
      "grad_norm": 0.5084332227706909,
      "learning_rate": 9.315068493150685e-07,
      "loss": 0.0241,
      "step": 21230
    },
    {
      "epoch": 0.9698630136986301,
      "grad_norm": 0.415808767080307,
      "learning_rate": 9.178082191780822e-07,
      "loss": 0.0244,
      "step": 21240
    },
    {
      "epoch": 0.9703196347031964,
      "grad_norm": 0.5171362161636353,
      "learning_rate": 9.041095890410959e-07,
      "loss": 0.0295,
      "step": 21250
    },
    {
      "epoch": 0.9707762557077626,
      "grad_norm": 0.49746790528297424,
      "learning_rate": 8.904109589041095e-07,
      "loss": 0.0261,
      "step": 21260
    },
    {
      "epoch": 0.9712328767123287,
      "grad_norm": 0.34092357754707336,
      "learning_rate": 8.767123287671233e-07,
      "loss": 0.0204,
      "step": 21270
    },
    {
      "epoch": 0.971689497716895,
      "grad_norm": 1.287355661392212,
      "learning_rate": 8.63013698630137e-07,
      "loss": 0.03,
      "step": 21280
    },
    {
      "epoch": 0.9721461187214612,
      "grad_norm": 0.75604248046875,
      "learning_rate": 8.493150684931507e-07,
      "loss": 0.0257,
      "step": 21290
    },
    {
      "epoch": 0.9726027397260274,
      "grad_norm": 1.1706100702285767,
      "learning_rate": 8.356164383561644e-07,
      "loss": 0.0268,
      "step": 21300
    },
    {
      "epoch": 0.9730593607305936,
      "grad_norm": 0.5121744275093079,
      "learning_rate": 8.219178082191781e-07,
      "loss": 0.0275,
      "step": 21310
    },
    {
      "epoch": 0.9735159817351599,
      "grad_norm": 1.0390055179595947,
      "learning_rate": 8.082191780821918e-07,
      "loss": 0.0153,
      "step": 21320
    },
    {
      "epoch": 0.9739726027397261,
      "grad_norm": 0.39594566822052,
      "learning_rate": 7.945205479452055e-07,
      "loss": 0.0189,
      "step": 21330
    },
    {
      "epoch": 0.9744292237442922,
      "grad_norm": 0.9358177781105042,
      "learning_rate": 7.808219178082192e-07,
      "loss": 0.0254,
      "step": 21340
    },
    {
      "epoch": 0.9748858447488584,
      "grad_norm": 0.8155394792556763,
      "learning_rate": 7.67123287671233e-07,
      "loss": 0.0282,
      "step": 21350
    },
    {
      "epoch": 0.9753424657534246,
      "grad_norm": 0.5351166725158691,
      "learning_rate": 7.534246575342466e-07,
      "loss": 0.0186,
      "step": 21360
    },
    {
      "epoch": 0.9757990867579909,
      "grad_norm": 1.0406826734542847,
      "learning_rate": 7.397260273972603e-07,
      "loss": 0.025,
      "step": 21370
    },
    {
      "epoch": 0.9762557077625571,
      "grad_norm": 0.3366498351097107,
      "learning_rate": 7.26027397260274e-07,
      "loss": 0.0179,
      "step": 21380
    },
    {
      "epoch": 0.9767123287671233,
      "grad_norm": 0.8016663193702698,
      "learning_rate": 7.123287671232878e-07,
      "loss": 0.0286,
      "step": 21390
    },
    {
      "epoch": 0.9771689497716894,
      "grad_norm": 0.7118659615516663,
      "learning_rate": 6.986301369863014e-07,
      "loss": 0.0269,
      "step": 21400
    },
    {
      "epoch": 0.9776255707762557,
      "grad_norm": 0.6200684309005737,
      "learning_rate": 6.84931506849315e-07,
      "loss": 0.0254,
      "step": 21410
    },
    {
      "epoch": 0.9780821917808219,
      "grad_norm": 0.5912351608276367,
      "learning_rate": 6.712328767123287e-07,
      "loss": 0.0338,
      "step": 21420
    },
    {
      "epoch": 0.9785388127853881,
      "grad_norm": 1.3074524402618408,
      "learning_rate": 6.575342465753425e-07,
      "loss": 0.0235,
      "step": 21430
    },
    {
      "epoch": 0.9789954337899544,
      "grad_norm": 0.999195396900177,
      "learning_rate": 6.438356164383562e-07,
      "loss": 0.0287,
      "step": 21440
    },
    {
      "epoch": 0.9794520547945206,
      "grad_norm": 0.3867869973182678,
      "learning_rate": 6.301369863013699e-07,
      "loss": 0.023,
      "step": 21450
    },
    {
      "epoch": 0.9799086757990868,
      "grad_norm": 0.8657093048095703,
      "learning_rate": 6.164383561643835e-07,
      "loss": 0.0246,
      "step": 21460
    },
    {
      "epoch": 0.9803652968036529,
      "grad_norm": 1.1813366413116455,
      "learning_rate": 6.027397260273972e-07,
      "loss": 0.0222,
      "step": 21470
    },
    {
      "epoch": 0.9808219178082191,
      "grad_norm": 0.7291308045387268,
      "learning_rate": 5.89041095890411e-07,
      "loss": 0.0281,
      "step": 21480
    },
    {
      "epoch": 0.9812785388127854,
      "grad_norm": 0.697043776512146,
      "learning_rate": 5.753424657534247e-07,
      "loss": 0.0249,
      "step": 21490
    },
    {
      "epoch": 0.9817351598173516,
      "grad_norm": 0.5941374897956848,
      "learning_rate": 5.616438356164383e-07,
      "loss": 0.0261,
      "step": 21500
    },
    {
      "epoch": 0.9821917808219178,
      "grad_norm": 0.8538964986801147,
      "learning_rate": 5.47945205479452e-07,
      "loss": 0.0161,
      "step": 21510
    },
    {
      "epoch": 0.982648401826484,
      "grad_norm": 0.18240368366241455,
      "learning_rate": 5.342465753424658e-07,
      "loss": 0.0306,
      "step": 21520
    },
    {
      "epoch": 0.9831050228310503,
      "grad_norm": 1.5213719606399536,
      "learning_rate": 5.205479452054795e-07,
      "loss": 0.019,
      "step": 21530
    },
    {
      "epoch": 0.9835616438356164,
      "grad_norm": 0.6520071625709534,
      "learning_rate": 5.068493150684932e-07,
      "loss": 0.0323,
      "step": 21540
    },
    {
      "epoch": 0.9840182648401826,
      "grad_norm": 0.6484745740890503,
      "learning_rate": 4.931506849315068e-07,
      "loss": 0.026,
      "step": 21550
    },
    {
      "epoch": 0.9844748858447488,
      "grad_norm": 1.1057754755020142,
      "learning_rate": 4.794520547945206e-07,
      "loss": 0.0205,
      "step": 21560
    },
    {
      "epoch": 0.9849315068493151,
      "grad_norm": 0.6201976537704468,
      "learning_rate": 4.6575342465753424e-07,
      "loss": 0.0212,
      "step": 21570
    },
    {
      "epoch": 0.9853881278538813,
      "grad_norm": 0.5485007166862488,
      "learning_rate": 4.5205479452054796e-07,
      "loss": 0.0251,
      "step": 21580
    },
    {
      "epoch": 0.9858447488584475,
      "grad_norm": 0.8733893036842346,
      "learning_rate": 4.3835616438356164e-07,
      "loss": 0.0194,
      "step": 21590
    },
    {
      "epoch": 0.9863013698630136,
      "grad_norm": 0.7230252623558044,
      "learning_rate": 4.2465753424657536e-07,
      "loss": 0.0151,
      "step": 21600
    },
    {
      "epoch": 0.9867579908675799,
      "grad_norm": 1.3408838510513306,
      "learning_rate": 4.1095890410958903e-07,
      "loss": 0.0261,
      "step": 21610
    },
    {
      "epoch": 0.9872146118721461,
      "grad_norm": 0.7271303534507751,
      "learning_rate": 3.9726027397260276e-07,
      "loss": 0.0205,
      "step": 21620
    },
    {
      "epoch": 0.9876712328767123,
      "grad_norm": 1.1756044626235962,
      "learning_rate": 3.835616438356165e-07,
      "loss": 0.027,
      "step": 21630
    },
    {
      "epoch": 0.9881278538812786,
      "grad_norm": 0.19082729518413544,
      "learning_rate": 3.6986301369863016e-07,
      "loss": 0.0217,
      "step": 21640
    },
    {
      "epoch": 0.9885844748858448,
      "grad_norm": 0.3842065632343292,
      "learning_rate": 3.561643835616439e-07,
      "loss": 0.0219,
      "step": 21650
    },
    {
      "epoch": 0.989041095890411,
      "grad_norm": 0.8666030764579773,
      "learning_rate": 3.424657534246575e-07,
      "loss": 0.0245,
      "step": 21660
    },
    {
      "epoch": 0.9894977168949771,
      "grad_norm": 0.5490160584449768,
      "learning_rate": 3.2876712328767123e-07,
      "loss": 0.0269,
      "step": 21670
    },
    {
      "epoch": 0.9899543378995433,
      "grad_norm": 0.7138735055923462,
      "learning_rate": 3.1506849315068495e-07,
      "loss": 0.0242,
      "step": 21680
    },
    {
      "epoch": 0.9904109589041096,
      "grad_norm": 0.41769489645957947,
      "learning_rate": 3.013698630136986e-07,
      "loss": 0.0355,
      "step": 21690
    },
    {
      "epoch": 0.9908675799086758,
      "grad_norm": 0.8769559860229492,
      "learning_rate": 2.8767123287671235e-07,
      "loss": 0.0236,
      "step": 21700
    },
    {
      "epoch": 0.991324200913242,
      "grad_norm": 0.4562864899635315,
      "learning_rate": 2.73972602739726e-07,
      "loss": 0.0243,
      "step": 21710
    },
    {
      "epoch": 0.9917808219178083,
      "grad_norm": 0.8571908473968506,
      "learning_rate": 2.6027397260273975e-07,
      "loss": 0.0269,
      "step": 21720
    },
    {
      "epoch": 0.9922374429223745,
      "grad_norm": 0.7041656374931335,
      "learning_rate": 2.465753424657534e-07,
      "loss": 0.0215,
      "step": 21730
    },
    {
      "epoch": 0.9926940639269406,
      "grad_norm": 0.9352306723594666,
      "learning_rate": 2.3287671232876712e-07,
      "loss": 0.0328,
      "step": 21740
    },
    {
      "epoch": 0.9931506849315068,
      "grad_norm": 0.8264881372451782,
      "learning_rate": 2.1917808219178082e-07,
      "loss": 0.0295,
      "step": 21750
    },
    {
      "epoch": 0.993607305936073,
      "grad_norm": 0.928353488445282,
      "learning_rate": 2.0547945205479452e-07,
      "loss": 0.0243,
      "step": 21760
    },
    {
      "epoch": 0.9940639269406393,
      "grad_norm": 0.9412363171577454,
      "learning_rate": 1.9178082191780824e-07,
      "loss": 0.019,
      "step": 21770
    },
    {
      "epoch": 0.9945205479452055,
      "grad_norm": 0.8933408260345459,
      "learning_rate": 1.7808219178082194e-07,
      "loss": 0.0201,
      "step": 21780
    },
    {
      "epoch": 0.9949771689497717,
      "grad_norm": 0.7547826170921326,
      "learning_rate": 1.6438356164383561e-07,
      "loss": 0.025,
      "step": 21790
    },
    {
      "epoch": 0.9954337899543378,
      "grad_norm": 0.39007771015167236,
      "learning_rate": 1.506849315068493e-07,
      "loss": 0.0193,
      "step": 21800
    },
    {
      "epoch": 0.9958904109589041,
      "grad_norm": 0.7674984335899353,
      "learning_rate": 1.36986301369863e-07,
      "loss": 0.0234,
      "step": 21810
    },
    {
      "epoch": 0.9963470319634703,
      "grad_norm": 0.4014040231704712,
      "learning_rate": 1.232876712328767e-07,
      "loss": 0.0247,
      "step": 21820
    },
    {
      "epoch": 0.9968036529680365,
      "grad_norm": 0.8021497130393982,
      "learning_rate": 1.0958904109589041e-07,
      "loss": 0.0239,
      "step": 21830
    },
    {
      "epoch": 0.9972602739726028,
      "grad_norm": 0.4847186207771301,
      "learning_rate": 9.589041095890412e-08,
      "loss": 0.0219,
      "step": 21840
    },
    {
      "epoch": 0.997716894977169,
      "grad_norm": 0.5549882650375366,
      "learning_rate": 8.219178082191781e-08,
      "loss": 0.0263,
      "step": 21850
    },
    {
      "epoch": 0.9981735159817352,
      "grad_norm": 0.253557026386261,
      "learning_rate": 6.84931506849315e-08,
      "loss": 0.0195,
      "step": 21860
    },
    {
      "epoch": 0.9986301369863013,
      "grad_norm": 0.8671102523803711,
      "learning_rate": 5.4794520547945204e-08,
      "loss": 0.0217,
      "step": 21870
    },
    {
      "epoch": 0.9990867579908675,
      "grad_norm": 0.41881293058395386,
      "learning_rate": 4.1095890410958903e-08,
      "loss": 0.0188,
      "step": 21880
    },
    {
      "epoch": 0.9995433789954338,
      "grad_norm": 0.749717116355896,
      "learning_rate": 2.7397260273972602e-08,
      "loss": 0.0238,
      "step": 21890
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.035897135734558,
      "learning_rate": 1.3698630136986301e-08,
      "loss": 0.0207,
      "step": 21900
    }
  ],
  "logging_steps": 10,
  "max_steps": 21900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.670615200268288e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
